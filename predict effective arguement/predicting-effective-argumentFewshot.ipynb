{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":35308,"databundleVersionId":3712025,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:23:47.706534Z","iopub.execute_input":"2025-07-02T09:23:47.706687Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/feedback-prize-effectiveness/sample_submission.csv\n/kaggle/input/feedback-prize-effectiveness/train.csv\n/kaggle/input/feedback-prize-effectiveness/test.csv\n/kaggle/input/feedback-prize-effectiveness/test/D72CB1C11673.txt\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install -U autogluon > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:31:00.447503Z","iopub.execute_input":"2025-06-30T14:31:00.447727Z","iopub.status.idle":"2025-06-30T14:33:13.903637Z","shell.execute_reply.started":"2025-06-30T14:31:00.447702Z","shell.execute_reply":"2025-06-30T14:33:13.902898Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\ntextblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom autogluon.tabular import TabularPredictor\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:50:50.956167Z","iopub.execute_input":"2025-06-30T14:50:50.956460Z","iopub.status.idle":"2025-06-30T14:50:52.278653Z","shell.execute_reply.started":"2025-06-30T14:50:50.956409Z","shell.execute_reply":"2025-06-30T14:50:52.277790Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_path = '/kaggle/input/feedback-prize-effectiveness'\ntrain_csv_path = os.path.join(data_path, 'train.csv')\ntrain_df = pd.read_csv(train_csv_path)\nprint(\"Dữ liệu huấn luyện có\", len(train_df), \"hàng.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T15:39:40.081872Z","iopub.execute_input":"2025-06-30T15:39:40.082548Z","iopub.status.idle":"2025-06-30T15:39:40.234816Z","shell.execute_reply.started":"2025-06-30T15:39:40.082528Z","shell.execute_reply":"2025-06-30T15:39:40.233930Z"}},"outputs":[{"name":"stdout","text":"Dữ liệu huấn luyện có 36765 hàng.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(train_df, test_size = 0.2, random_state = 3, stratify = train_df['discourse_effectiveness'] )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:51:05.288919Z","iopub.execute_input":"2025-06-30T14:51:05.289197Z","iopub.status.idle":"2025-06-30T14:51:05.362679Z","shell.execute_reply.started":"2025-06-30T14:51:05.289174Z","shell.execute_reply":"2025-06-30T14:51:05.361820Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"label = 'discourse_effectiveness'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:51:06.995075Z","iopub.execute_input":"2025-06-30T14:51:06.995897Z","iopub.status.idle":"2025-06-30T14:51:06.999810Z","shell.execute_reply.started":"2025-06-30T14:51:06.995866Z","shell.execute_reply":"2025-06-30T14:51:06.998905Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"predictor = TabularPredictor(\n    label=label,\n    eval_metric='log_loss'\n).fit(\n    train_data = train_data,\n    time_limit = 600\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:51:08.553429Z","iopub.execute_input":"2025-06-30T14:51:08.554332Z","iopub.status.idle":"2025-06-30T15:01:21.971704Z","shell.execute_reply.started":"2025-06-30T14:51:08.554296Z","shell.execute_reply":"2025-06-30T15:01:21.971107Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20250630_145108\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.81 GB / 31.35 GB (95.1%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n\tpresets='high'         : Strong accuracy with fast inference speed.\n\tpresets='good'         : Good accuracy with very fast inference speed.\n\tpresets='medium'       : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ... Time limit = 600s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250630_145108\"\nTrain Data Rows:    29412\nTrain Data Columns: 4\nLabel Column:       discourse_effectiveness\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n\t3 unique label values:  ['Adequate', 'Effective', 'Ineffective']\n\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       multiclass\nPreprocessing data ...\nTrain Data Class Count: 3\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30545.26 MB\n\tTrain Data (Original)  Memory Usage: 14.38 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\t\tFitting TextSpecialFeatureGenerator...\n\t\t\tFitting BinnedFeatureGenerator...\n\t\t\tFitting DropDuplicatesFeatureGenerator...\n\t\tFitting TextNgramFeatureGenerator...\n\t\t\tFitting CountVectorizer for text features: ['discourse_text']\n\t\t\tCountVectorizer fit with vocabulary size = 10000\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tUnused Original Features (Count: 1): ['discourse_id']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('object', []) : 1 | ['discourse_id']\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('object', [])       : 2 | ['essay_id', 'discourse_type']\n\t\t('object', ['text']) : 1 | ['discourse_text']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])                    :    2 | ['essay_id', 'discourse_type']\n\t\t('category', ['text_as_category'])  :    1 | ['discourse_text']\n\t\t('int', ['binned', 'text_special']) :   22 | ['discourse_text.char_count', 'discourse_text.word_count', 'discourse_text.capital_ratio', 'discourse_text.lower_ratio', 'discourse_text.digit_ratio', ...]\n\t\t('int', ['text_ngram'])             : 9030 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', ...]\n\t48.0s = Fit runtime\n\t3 features in original data used to generate 9055 features in processed data.\n\tTrain Data (Processed) Memory Usage: 507.30 MB (1.7% of available memory)\nData preprocessing and feature engineering runtime = 52.77s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.08499932000543996, Train Rows: 26912, Val Rows: 2500\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 547.23s of the 547.23s of remaining time.\n\t-2.3351\t = Validation score   (-log_loss)\n\t10.78s\t = Training   runtime\n\t7.24s\t = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 523.92s of the 523.92s of remaining time.\n\t-2.3524\t = Validation score   (-log_loss)\n\t10.49s\t = Training   runtime\n\t7.21s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 501.01s of the 501.00s of remaining time.\nNo improvement since epoch 1: early stopping\n\t-0.726\t = Validation score   (-log_loss)\n\t33.03s\t = Training   runtime\n\t0.05s\t = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 467.89s of the 467.89s of remaining time.\n\t-0.7427\t = Validation score   (-log_loss)\n\t65.8s\t = Training   runtime\n\t0.57s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 401.39s of the 401.39s of remaining time.\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's multi_logloss: 0.652658\n","output_type":"stream"},{"name":"stderr","text":"\t-0.6438\t = Validation score   (-log_loss)\n\t70.81s\t = Training   runtime\n\t0.98s\t = Validation runtime\nFitting model: RandomForestGini ... Training model for up to 329.32s of the 329.32s of remaining time.\n\t-0.8224\t = Validation score   (-log_loss)\n\t168.01s\t = Training   runtime\n\t0.39s\t = Validation runtime\nFitting model: RandomForestEntr ... Training model for up to 160.16s of the 160.16s of remaining time.\n\t-0.8151\t = Validation score   (-log_loss)\n\t136.99s\t = Training   runtime\n\t0.35s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 21.98s of the 21.97s of remaining time.\n\tMany features detected (9055), dynamically setting 'colsample_bylevel' to 0.11043622308117063 to speed up training (Default = 1).\n\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n\tRan out of time, early stopping on iteration 1.\n\t-1.0825\t = Validation score   (-log_loss)\n\t15.68s\t = Training   runtime\n\t0.51s\t = Validation runtime\nFitting model: ExtraTreesGini ... Training model for up to 5.76s of the 5.76s of remaining time.\n\tWarning: Model is expected to require 247.4s to train, which exceeds the maximum time limit of 1.2s, skipping model...\n\tTime limit exceeded... Skipping ExtraTreesGini.\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -5.48s of remaining time.\n\tEnsemble Weights: {'LightGBM': 0.65, 'NeuralNetFastAI': 0.3, 'RandomForestEntr': 0.05}\n\t-0.6112\t = Validation score   (-log_loss)\n\t0.29s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 608.02s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1818.8 rows/s (2500 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250630_145108\")\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"y_true = test_data[label]\ny_pred = predictor.predict(test_data)\ny_proba = predictor.predict_proba(test_data)\n\n\nprint(\"AutoGluon Evaluation:\")\npredictor.leaderboard(test_data, extra_metrics=['accuracy', 'log_loss', 'f1_macro', 'f1_weighted', 'roc_auc_ovr_macro', 'roc_auc_ovr_weighted'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T15:39:47.090670Z","iopub.execute_input":"2025-06-30T15:39:47.090921Z","iopub.status.idle":"2025-06-30T15:41:06.582956Z","shell.execute_reply.started":"2025-06-30T15:39:47.090903Z","shell.execute_reply":"2025-06-30T15:41:06.582202Z"}},"outputs":[{"name":"stdout","text":"AutoGluon Evaluation:\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                 model  score_test  accuracy  log_loss  f1_macro  f1_weighted  \\\n0  WeightedEnsemble_L2   -0.607523  0.733850 -0.607523  0.639545     0.710914   \n1             LightGBM   -0.630263  0.734938 -0.630263  0.650482     0.716398   \n2           LightGBMXT   -0.726364  0.685299 -0.726364  0.582364     0.659501   \n3      NeuralNetFastAI   -0.731691  0.694546 -0.731691  0.624737     0.684699   \n4     RandomForestEntr   -0.801031  0.645451 -0.801031  0.456616     0.578240   \n5     RandomForestGini   -0.805086  0.645723 -0.805086  0.462493     0.581448   \n6             CatBoost   -1.082272  0.604651 -1.082272  0.357289     0.507299   \n7       KNeighborsUnif   -2.341401  0.595131 -2.341401  0.445950     0.552527   \n8       KNeighborsDist   -2.348156  0.577995 -2.348156  0.456147     0.551428   \n\n   roc_auc_ovr  roc_auc_ovr_weighted  score_val eval_metric  pred_time_test  \\\n0     0.850435              0.835072  -0.611240    log_loss        5.570127   \n1     0.845190              0.829035  -0.643773    log_loss        2.972107   \n2     0.796201              0.773060  -0.742729    log_loss        2.447841   \n3     0.808582              0.794917  -0.725978    log_loss        0.287795   \n4     0.758484              0.733711  -0.815078    log_loss        2.305365   \n5     0.753169              0.729884  -0.822409    log_loss        2.757589   \n6     0.550240              0.558968  -1.082466    log_loss        1.465542   \n7     0.629957              0.624845  -2.335141    log_loss       25.440251   \n8     0.635092              0.627424  -2.352427    log_loss       25.011472   \n\n   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0       1.374568  241.117007                 0.004860                0.001756   \n1       0.975409   70.811756                 2.972107                0.975409   \n2       0.568084   65.796991                 2.447841                0.568084   \n3       0.049090   33.025633                 0.287795                0.049090   \n4       0.348313  136.987265                 2.305365                0.348313   \n5       0.393684  168.012482                 2.757589                0.393684   \n6       0.505817   15.675976                 1.465542                0.505817   \n7       7.240264   10.780032                25.440251                7.240264   \n8       7.205302   10.486506                25.011472                7.205302   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           0.292353            2       True          9  \n1          70.811756            1       True          5  \n2          65.796991            1       True          4  \n3          33.025633            1       True          3  \n4         136.987265            1       True          7  \n5         168.012482            1       True          6  \n6          15.675976            1       True          8  \n7          10.780032            1       True          1  \n8          10.486506            1       True          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>accuracy</th>\n      <th>log_loss</th>\n      <th>f1_macro</th>\n      <th>f1_weighted</th>\n      <th>roc_auc_ovr</th>\n      <th>roc_auc_ovr_weighted</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>-0.607523</td>\n      <td>0.733850</td>\n      <td>-0.607523</td>\n      <td>0.639545</td>\n      <td>0.710914</td>\n      <td>0.850435</td>\n      <td>0.835072</td>\n      <td>-0.611240</td>\n      <td>log_loss</td>\n      <td>5.570127</td>\n      <td>1.374568</td>\n      <td>241.117007</td>\n      <td>0.004860</td>\n      <td>0.001756</td>\n      <td>0.292353</td>\n      <td>2</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LightGBM</td>\n      <td>-0.630263</td>\n      <td>0.734938</td>\n      <td>-0.630263</td>\n      <td>0.650482</td>\n      <td>0.716398</td>\n      <td>0.845190</td>\n      <td>0.829035</td>\n      <td>-0.643773</td>\n      <td>log_loss</td>\n      <td>2.972107</td>\n      <td>0.975409</td>\n      <td>70.811756</td>\n      <td>2.972107</td>\n      <td>0.975409</td>\n      <td>70.811756</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBMXT</td>\n      <td>-0.726364</td>\n      <td>0.685299</td>\n      <td>-0.726364</td>\n      <td>0.582364</td>\n      <td>0.659501</td>\n      <td>0.796201</td>\n      <td>0.773060</td>\n      <td>-0.742729</td>\n      <td>log_loss</td>\n      <td>2.447841</td>\n      <td>0.568084</td>\n      <td>65.796991</td>\n      <td>2.447841</td>\n      <td>0.568084</td>\n      <td>65.796991</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NeuralNetFastAI</td>\n      <td>-0.731691</td>\n      <td>0.694546</td>\n      <td>-0.731691</td>\n      <td>0.624737</td>\n      <td>0.684699</td>\n      <td>0.808582</td>\n      <td>0.794917</td>\n      <td>-0.725978</td>\n      <td>log_loss</td>\n      <td>0.287795</td>\n      <td>0.049090</td>\n      <td>33.025633</td>\n      <td>0.287795</td>\n      <td>0.049090</td>\n      <td>33.025633</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RandomForestEntr</td>\n      <td>-0.801031</td>\n      <td>0.645451</td>\n      <td>-0.801031</td>\n      <td>0.456616</td>\n      <td>0.578240</td>\n      <td>0.758484</td>\n      <td>0.733711</td>\n      <td>-0.815078</td>\n      <td>log_loss</td>\n      <td>2.305365</td>\n      <td>0.348313</td>\n      <td>136.987265</td>\n      <td>2.305365</td>\n      <td>0.348313</td>\n      <td>136.987265</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RandomForestGini</td>\n      <td>-0.805086</td>\n      <td>0.645723</td>\n      <td>-0.805086</td>\n      <td>0.462493</td>\n      <td>0.581448</td>\n      <td>0.753169</td>\n      <td>0.729884</td>\n      <td>-0.822409</td>\n      <td>log_loss</td>\n      <td>2.757589</td>\n      <td>0.393684</td>\n      <td>168.012482</td>\n      <td>2.757589</td>\n      <td>0.393684</td>\n      <td>168.012482</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CatBoost</td>\n      <td>-1.082272</td>\n      <td>0.604651</td>\n      <td>-1.082272</td>\n      <td>0.357289</td>\n      <td>0.507299</td>\n      <td>0.550240</td>\n      <td>0.558968</td>\n      <td>-1.082466</td>\n      <td>log_loss</td>\n      <td>1.465542</td>\n      <td>0.505817</td>\n      <td>15.675976</td>\n      <td>1.465542</td>\n      <td>0.505817</td>\n      <td>15.675976</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>KNeighborsUnif</td>\n      <td>-2.341401</td>\n      <td>0.595131</td>\n      <td>-2.341401</td>\n      <td>0.445950</td>\n      <td>0.552527</td>\n      <td>0.629957</td>\n      <td>0.624845</td>\n      <td>-2.335141</td>\n      <td>log_loss</td>\n      <td>25.440251</td>\n      <td>7.240264</td>\n      <td>10.780032</td>\n      <td>25.440251</td>\n      <td>7.240264</td>\n      <td>10.780032</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>KNeighborsDist</td>\n      <td>-2.348156</td>\n      <td>0.577995</td>\n      <td>-2.348156</td>\n      <td>0.456147</td>\n      <td>0.551428</td>\n      <td>0.635092</td>\n      <td>0.627424</td>\n      <td>-2.352427</td>\n      <td>log_loss</td>\n      <td>25.011472</td>\n      <td>7.205302</td>\n      <td>10.486506</td>\n      <td>25.011472</td>\n      <td>7.205302</td>\n      <td>10.486506</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nfrom autogluon.tabular import TabularPredictor\nimport os\n\n# --- Bước 1: Tải dữ liệu ---\n# Đường dẫn tới dữ liệu trên môi trường Kaggle\ndata_path = '/kaggle/input/feedback-prize-effectiveness'\ntrain_csv_path = os.path.join(data_path, 'train.csv')\ntest_csv_path = os.path.join(data_path, 'test.csv')\nsample_submission_path = os.path.join(data_path, 'sample_submission.csv')\n\nprint(\"Đang tải dữ liệu...\")\n# Tải dữ liệu huấn luyện\ntrain_df = pd.read_csv(train_csv_path)\n\n# Tải dữ liệu kiểm tra\ntest_df = pd.read_csv(test_csv_path)\n\n# Tải file submission mẫu để biết định dạng\nsample_submission_df = pd.read_csv(sample_submission_path)\n\nprint(\"Tải dữ liệu thành công!\")\nprint(\"Dữ liệu huấn luyện có\", len(train_df), \"hàng.\")\nprint(\"Dữ liệu kiểm tra có\", len(test_df), \"hàng.\")\n\n\n# --- Bước 2: Chuẩn bị dữ liệu ---\n# Biến mục tiêu (target) mà chúng ta cần dự đoán\nlabel = 'discourse_effectiveness'\n\n# AutoGluon có thể tự động xử lý các cột không cần thiết,\n# nhưng để rõ ràng, chúng ta có thể chỉ chọn những cột quan trọng.\n# Các feature chính là nội dung văn bản và loại của yếu tố lập luận.\n# AutoGluon sẽ tự động xem 'discourse_text' là văn bản và 'discourse_type' là biến phân loại.\n# Chúng ta không cần xóa các cột khác, AutoGluon sẽ tự bỏ qua chúng nếu không hữu ích.\n\n# Hiển thị 5 dòng đầu của dữ liệu huấn luyện\nprint(\"\\n5 dòng đầu của dữ liệu huấn luyện:\")\nprint(train_df.head())\n\n\n# --- Bước 3: Huấn luyện mô hình với AutoGluon ---\n# Khởi tạo TabularPredictor\n# - label: Tên cột mục tiêu cần dự đoán.\n# - path: Thư mục để lưu lại các mô hình đã huấn luyện.\n# - eval_metric: Thước đo để đánh giá mô hình, dùng 'log_loss' theo yêu cầu cuộc thi.\npredictor = TabularPredictor(\n    label=label,\n    path='autogluon_models',\n    eval_metric='log_loss'\n)\n\n# Bắt đầu huấn luyện\n# - train_data: DataFrame chứa dữ liệu huấn luyện.\n# - presets: Cấu hình có sẵn. 'best_quality' sẽ ưu tiên độ chính xác cao nhất,\n#            nhưng tốn nhiều thời gian. Bạn có thể dùng 'high_quality' hoặc\n#            'medium_quality_faster_train' để chạy nhanh hơn.\n# - time_limit: Thời gian tối đa cho phép huấn luyện (tính bằng giây).\n#               Rất quan trọng trong các cuộc thi có giới hạn thời gian chạy notebook.\n#               Ví dụ: 2 giờ = 7200 giây.\n#               Trên Kaggle, bạn có khoảng 9 tiếng (khoảng 32400 giây).\n#               Chúng ta đặt 8 tiếng để có thời gian dự phòng.\ntime_limit_seconds = 1200\n\nprint(\"\\nBắt đầu huấn luyện mô hình...\")\npredictor.fit(\n    train_data=train_df,\n    presets='best_quality', # Tối ưu cho độ chính xác\n    time_limit=time_limit_seconds\n)\n\nprint(\"Huấn luyện hoàn tất!\")\n\n\n# --- Bước 4: Xem lại kết quả và các mô hình đã huấn luyện ---\nprint(\"\\nBảng xếp hạng các mô hình:\")\nleaderboard = predictor.leaderboard(train_df, silent=True)\nprint(leaderboard)\n\n\n# --- Bước 5: Dự đoán trên tập kiểm tra (Test set) ---\nprint(\"\\nBắt đầu dự đoán trên tập test...\")\n# Sử dụng predict_proba() để lấy xác suất cho mỗi lớp, vì cuộc thi yêu cầu log_loss.\npredictions_proba = predictor.predict_proba(test_df)\n\nprint(\"Dự đoán hoàn tất!\")\nprint(\"5 dòng đầu của kết quả dự đoán (xác suất):\")\nprint(predictions_proba.head())\n\n\n# --- Bước 6: Tạo tệp submission ---\nprint(\"\\nTạo tệp submission...\")\n# Kết quả dự đoán có các cột là 'Ineffective', 'Adequate', 'Effective'.\n# Chúng ta chỉ cần gán các giá trị này vào đúng cột trong file submission.\nsubmission_df = test_df[['discourse_id']].copy()\nsubmission_df[['Ineffective', 'Adequate', 'Effective']] = predictions_proba\n\n# Lưu tệp submission\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nTệp submission.csv đã được tạo thành công!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:21:29.879447Z","iopub.execute_input":"2025-06-30T01:21:29.879725Z","iopub.status.idle":"2025-06-30T01:49:26.667350Z","shell.execute_reply.started":"2025-06-30T01:21:29.879707Z","shell.execute_reply":"2025-06-30T01:49:26.666548Z"}},"outputs":[{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       27.91 GB / 31.35 GB (89.0%)\nDisk Space Avail:   14.48 GB / 19.52 GB (74.2%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 300s of the 1200s of remaining time (25%).\n","output_type":"stream"},{"name":"stdout","text":"Đang tải dữ liệu...\nTải dữ liệu thành công!\nDữ liệu huấn luyện có 36765 hàng.\nDữ liệu kiểm tra có 10 hàng.\n\n5 dòng đầu của dữ liệu huấn luyện:\n   discourse_id      essay_id  \\\n0  0013cc385424  007ACE74B050   \n1  9704a709b505  007ACE74B050   \n2  c22adee811b6  007ACE74B050   \n3  a10d361e54e4  007ACE74B050   \n4  db3e453ec4e2  007ACE74B050   \n\n                                                                                                                                                                                                                                                                                                                                                               discourse_text  \\\n0                                               Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform.    \n1                                                                                                                                                          On my perspective, I think that the face is a natural landform because I dont think that there is any life on Mars. In these next few paragraphs, I'll be talking about how I think that is is a natural landform    \n2                                                                                                                                                                                                                                                                   I think that the face is a natural landform because there is no life on Mars that we have descovered yet    \n3  If life was on Mars, we would know by now. The reason why I think it is a natural landform because, nobody live on Mars in order to create the figure. It says in paragraph 9, \"It's not easy to target Cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. In all that they're saying, its probably a natural landform.    \n4                                                                                                                                                                                                                                                                       People thought that the face was formed by alieans because they thought that there was life on Mars.    \n\n  discourse_type discourse_effectiveness  \n0           Lead                Adequate  \n1       Position                Adequate  \n2          Claim                Adequate  \n3       Evidence                Adequate  \n4   Counterclaim                Adequate  \n\nBắt đầu huấn luyện mô hình...\n","output_type":"stream"},{"name":"stderr","text":"\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2025-06-30 01:21:33,455\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n\t\tContext path: \"/kaggle/working/autogluon_models/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=398)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=398)\u001b[0m Beginning AutoGluon training ... Time limit = 295s\n\u001b[36m(_dystack pid=398)\u001b[0m AutoGluon will save models to \"/kaggle/working/autogluon_models/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=398)\u001b[0m Train Data Rows:    32680\n\u001b[36m(_dystack pid=398)\u001b[0m Train Data Columns: 4\n\u001b[36m(_dystack pid=398)\u001b[0m Label Column:       discourse_effectiveness\n\u001b[36m(_dystack pid=398)\u001b[0m Problem Type:       multiclass\n\u001b[36m(_dystack pid=398)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=398)\u001b[0m Train Data Class Count: 3\n\u001b[36m(_dystack pid=398)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \tAvailable Memory:                    27998.41 MB\n\u001b[36m(_dystack pid=398)\u001b[0m \tTrain Data (Original)  Memory Usage: 15.92 MB (0.1% of available memory)\n\u001b[36m(_dystack pid=398)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=398)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting TextSpecialFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t\tFitting BinnedFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting TextNgramFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t\tFitting CountVectorizer for text features: ['discourse_text']\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t\tCountVectorizer fit with vocabulary size = 10000\n\u001b[36m(_dystack pid=398)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=398)\u001b[0m \tUnused Original Features (Count: 1): ['discourse_id']\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\u001b[36m(_dystack pid=398)\u001b[0m \t\tThese features do not need to be present at inference time.\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('object', []) : 1 | ['discourse_id']\n\u001b[36m(_dystack pid=398)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('object', [])       : 2 | ['essay_id', 'discourse_type']\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('object', ['text']) : 1 | ['discourse_text']\n\u001b[36m(_dystack pid=398)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('category', [])                    :    2 | ['essay_id', 'discourse_type']\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('category', ['text_as_category'])  :    1 | ['discourse_text']\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('int', ['binned', 'text_special']) :   22 | ['discourse_text.char_count', 'discourse_text.word_count', 'discourse_text.capital_ratio', 'discourse_text.lower_ratio', 'discourse_text.digit_ratio', ...]\n\u001b[36m(_dystack pid=398)\u001b[0m \t\t('int', ['text_ngram'])             : 9041 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', ...]\n\u001b[36m(_dystack pid=398)\u001b[0m \t51.7s = Fit runtime\n\u001b[36m(_dystack pid=398)\u001b[0m \t3 features in original data used to generate 9066 features in processed data.\n\u001b[36m(_dystack pid=398)\u001b[0m \tTrain Data (Processed) Memory Usage: 564.36 MB (2.0% of available memory)\n\u001b[36m(_dystack pid=398)\u001b[0m Data preprocessing and feature engineering runtime = 56.85s ...\n\u001b[36m(_dystack pid=398)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n\u001b[36m(_dystack pid=398)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\u001b[36m(_dystack pid=398)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n\u001b[36m(_dystack pid=398)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=398)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=398)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=398)\u001b[0m {\n\u001b[36m(_dystack pid=398)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=398)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=398)\u001b[0m }\n\u001b[36m(_dystack pid=398)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 158.61s of the 237.96s of remaining time.\n\u001b[36m(_dystack pid=398)\u001b[0m \t-2.2942\t = Validation score   (-log_loss)\n\u001b[36m(_dystack pid=398)\u001b[0m \t16.15s\t = Training   runtime\n\u001b[36m(_dystack pid=398)\u001b[0m \t124.85s\t = Validation runtime\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 0.23s of the 79.59s of remaining time.\n\u001b[36m(_dystack pid=398)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -8.9s)\n\u001b[36m(_dystack pid=398)\u001b[0m \tTime limit exceeded... Skipping KNeighborsDist_BAG_L1.\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 237.97s of the 70.06s of remaining time.\n\u001b[36m(_dystack pid=398)\u001b[0m \tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=398)\u001b[0m \t-2.2942\t = Validation score   (-log_loss)\n\u001b[36m(_dystack pid=398)\u001b[0m \t0.02s\t = Training   runtime\n\u001b[36m(_dystack pid=398)\u001b[0m \t0.01s\t = Validation runtime\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 70.01s of the 69.79s of remaining time.\n\u001b[36m(_dystack pid=398)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.34% memory usage per fold, 46.68%/80.00% total).\n\u001b[36m(_dystack pid=398)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=23.34%)\n\u001b[36m(_ray_fit pid=669)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 3)\n\u001b[36m(_ray_fit pid=758)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 3)\n\u001b[36m(_ray_fit pid=849)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\n\u001b[36m(_ray_fit pid=941)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\n\u001b[36m(_ray_fit pid=940)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 3)\n\u001b[36m(_dystack pid=398)\u001b[0m \t-0.6839\t = Validation score   (-log_loss)\n\u001b[36m(_dystack pid=398)\u001b[0m \t69.58s\t = Training   runtime\n\u001b[36m(_dystack pid=398)\u001b[0m \t0.78s\t = Validation runtime\n\u001b[36m(_dystack pid=398)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 237.97s of the -10.63s of remaining time.\n\u001b[36m(_dystack pid=398)\u001b[0m \tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.875, 'KNeighborsUnif_BAG_L1': 0.125}\n\u001b[36m(_dystack pid=398)\u001b[0m \t-0.6635\t = Validation score   (-log_loss)\n\u001b[36m(_dystack pid=398)\u001b[0m \t0.38s\t = Training   runtime\n\u001b[36m(_dystack pid=398)\u001b[0m \t0.01s\t = Validation runtime\n\u001b[36m(_dystack pid=398)\u001b[0m AutoGluon training complete, total runtime = 306.92s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 249.3 rows/s (4085 batch size)\n\u001b[36m(_dystack pid=398)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/autogluon_models/ds_sub_fit/sub_fit_ho\")\n\u001b[36m(_dystack pid=398)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                    model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0  NeuralNetFastAI_BAG_L2      -0.583226  -0.683901    log_loss       24.716759     125.632634  85.723874                 3.451208                0.781327          69.575505            2       True          3\n1     WeightedEnsemble_L3      -0.590345  -0.663522    log_loss       24.719003     125.638769  86.105834                 0.002244                0.006135           0.381960            3       True          4\n2   KNeighborsUnif_BAG_L1      -2.261751  -2.294162    log_loss       21.265551     124.851308  16.148368                21.265551              124.851308          16.148368            1       True          1\n3     WeightedEnsemble_L2      -2.261751  -2.294162    log_loss       21.268176     124.857817  16.166843                 0.002625                0.006509           0.018475            2       True          2\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t341s\t = DyStack   runtime |\t859s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 859s\nAutoGluon will save models to \"/kaggle/working/autogluon_models\"\nTrain Data Rows:    36765\nTrain Data Columns: 4\nLabel Column:       discourse_effectiveness\nProblem Type:       multiclass\nPreprocessing data ...\nTrain Data Class Count: 3\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    27069.91 MB\n\tTrain Data (Original)  Memory Usage: 19.70 MB (0.1% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\t\tFitting TextSpecialFeatureGenerator...\n\t\t\tFitting BinnedFeatureGenerator...\n\t\t\tFitting DropDuplicatesFeatureGenerator...\n\t\tFitting TextNgramFeatureGenerator...\n\t\t\tFitting CountVectorizer for text features: ['discourse_text']\n\t\t\tCountVectorizer fit with vocabulary size = 10000\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tUnused Original Features (Count: 1): ['discourse_id']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('object', []) : 1 | ['discourse_id']\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('object', [])       : 2 | ['essay_id', 'discourse_type']\n\t\t('object', ['text']) : 1 | ['discourse_text']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])                    :    2 | ['essay_id', 'discourse_type']\n\t\t('category', ['text_as_category'])  :    1 | ['discourse_text']\n\t\t('int', ['binned', 'text_special']) :   22 | ['discourse_text.char_count', 'discourse_text.word_count', 'discourse_text.capital_ratio', 'discourse_text.lower_ratio', 'discourse_text.digit_ratio', ...]\n\t\t('int', ['text_ngram'])             : 8980 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.12', '__nlp__.15', ...]\n\t57.4s = Fit runtime\n\t3 features in original data used to generate 9005 features in processed data.\n\tTrain Data (Processed) Memory Usage: 630.62 MB (2.3% of available memory)\nData preprocessing and feature engineering runtime = 63.88s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 110 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 530.14s of the 795.41s of remaining time.\n\tWarning: Potentially not enough memory to safely train model. Estimated to require 4.767 GB out of 28.176 GB available memory (16.920%)... (20.000% of avail memory is the max safe size)\n\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.18 to avoid the warning)\n\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\t-2.2942\t = Validation score   (-log_loss)\n\t15.6s\t = Training   runtime\n\t151.66s\t = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 344.42s of the 609.68s of remaining time.\n\tWarning: Potentially not enough memory to safely train model. Estimated to require 4.767 GB out of 28.171 GB available memory (16.923%)... (20.000% of avail memory is the max safe size)\n\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.18 to avoid the warning)\n\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\t-2.2992\t = Validation score   (-log_loss)\n\t15.07s\t = Training   runtime\n\t149.07s\t = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 160.92s of the 426.19s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.83% memory usage per fold, 51.66%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=25.83%)\n\t-0.7008\t = Validation score   (-log_loss)\n\t126.07s\t = Training   runtime\n\t0.63s\t = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 24.90s of the 290.16s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.02% memory usage per fold, 44.04%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=22.02%)\n\tTime limit exceeded... Skipping LightGBMXT_BAG_L1.\nFitting model: LightGBM_BAG_L1 ... Training model for up to 1.08s of the 266.34s of remaining time.\n2025-06-30 01:37:09,639\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1549, ip=172.19.2.2)\n  File \"/usr/local/lib/python3.11/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n  File \"/usr/local/lib/python3.11/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 1050, in fit\n    raise TimeLimitExceeded\nautogluon.core.utils.exceptions.TimeLimitExceeded\n\tWarning: Model has no time left to train, skipping model... (Time Left = -7.4s)\n\tTime limit exceeded... Skipping LightGBM_BAG_L1.\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 257.10s of remaining time.\n\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.833, 'KNeighborsDist_BAG_L1': 0.167}\n\t-0.6668\t = Validation score   (-log_loss)\n\t0.59s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting 108 L2 models, fit_strategy=\"sequential\" ...\nFitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 256.47s of the 256.22s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.59% memory usage per fold, 53.19%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=26.59%)\n\t-0.6996\t = Validation score   (-log_loss)\n\t197.03s\t = Training   runtime\n\t0.75s\t = Validation runtime\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 50.21s of the 49.96s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.59% memory usage per fold, 45.18%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=22.59%)\n\t-0.9432\t = Validation score   (-log_loss)\n\t130.48s\t = Training   runtime\n\t3.09s\t = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -97.97s of remaining time.\n\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.524, 'NeuralNetFastAI_BAG_L2': 0.381, 'KNeighborsDist_BAG_L1': 0.095}\n\t-0.645\t = Validation score   (-log_loss)\n\t1.03s\t = Training   runtime\n\t0.01s\t = Validation runtime\nAutoGluon training complete, total runtime = 959.55s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 117.9 rows/s (4596 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/autogluon_models\")\n","output_type":"stream"},{"name":"stdout","text":"Huấn luyện hoàn tất!\n\nBảng xếp hạng các mô hình:\n                    model  score_test  score_val eval_metric  pred_time_test  \\\n0   KNeighborsDist_BAG_L1   -0.000831  -2.299245    log_loss      161.872678   \n1     WeightedEnsemble_L2   -0.304060  -0.666804    log_loss      165.793072   \n2     WeightedEnsemble_L3   -0.344510  -0.645014    log_loss      337.348608   \n3  NeuralNetFastAI_BAG_L2   -0.382564  -0.699641    log_loss      337.338576   \n4  NeuralNetFastAI_BAG_L1   -0.401539  -0.700783    log_loss        3.916750   \n5   KNeighborsUnif_BAG_L1   -0.605692  -2.294204    log_loss      161.701364   \n6       LightGBMXT_BAG_L2   -0.934319  -0.943187    log_loss      348.527791   \n\n   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0     149.071196   15.069632               161.872678              149.071196   \n1     149.712203  141.729271                 0.003643                0.007970   \n2     302.120903  354.791245                 0.010033                0.007068   \n3     302.113835  353.765731                 9.847783                0.753316   \n4       0.633037  126.069098                 3.916750                0.633037   \n5     151.656286   15.598710               161.701364              151.656286   \n6     304.453050  287.220800                21.036998                3.092530   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0          15.069632            1       True          2  \n1           0.590541            2       True          4  \n2           1.025514            3       True          7  \n3         197.028291            2       True          5  \n4         126.069098            1       True          3  \n5          15.598710            1       True          1  \n6         130.483361            2       True          6  \n\nBắt đầu dự đoán trên tập test...\nDự đoán hoàn tất!\n5 dòng đầu của kết quả dự đoán (xác suất):\n   Adequate  Effective  Ineffective\n0  0.145668   0.041957     0.812375\n1  0.685682   0.041156     0.273162\n2  0.500406   0.065218     0.434377\n3  0.537729   0.059375     0.402896\n4  0.562772   0.032186     0.405041\n\nTạo tệp submission...\n\nTệp submission.csv đã được tạo thành công!\n   discourse_id  Ineffective  Adequate  Effective\n0  a261b6e14276     0.145668  0.041957   0.812375\n1  5a88900e7dc1     0.685682  0.041156   0.273162\n2  9790d835736b     0.500406  0.065218   0.434377\n3  75ce6d68b67b     0.537729  0.059375   0.402896\n4  93578d946723     0.562772  0.032186   0.405041\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    log_loss,\n    f1_score,\n    roc_auc_score,\n)\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:26:47.022937Z","iopub.execute_input":"2025-07-02T09:26:47.023542Z","iopub.status.idle":"2025-07-02T09:26:47.027724Z","shell.execute_reply.started":"2025-07-02T09:26:47.023511Z","shell.execute_reply":"2025-07-02T09:26:47.027148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"TRAIN_CSV_PATH = '/kaggle/input/feedback-prize-effectiveness/train.csv'\nTRAIN_ESSAYS_PATH = '/kaggle/input/feedback-prize-effectiveness/train/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:26:51.934604Z","iopub.execute_input":"2025-07-02T09:26:51.935275Z","iopub.status.idle":"2025-07-02T09:26:51.938566Z","shell.execute_reply.started":"2025-07-02T09:26:51.935251Z","shell.execute_reply":"2025-07-02T09:26:51.937907Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def create_dummy_data():\n    \"\"\"\n    Creates dummy data files for local testing if they don't exist.\n    This simulates the Kaggle environment.\n    \"\"\"\n    print(\"Creating dummy data for local execution...\")\n\n    # Create dummy train.csv\n    if not os.path.exists(TRAIN_CSV_PATH):\n        train_data = {\n            'discourse_id': [f'd{i}' for i in range(100)],\n            'essay_id': [f'essay_{i % 10}' for i in range(100)],\n            'discourse_text': [\n                'This is a sample lead.', 'The author makes a claim.', 'Here is some evidence.',\n                'This is a concluding statement.', 'Another position is taken here.'\n            ] * 20,\n            'discourse_type': ['Lead', 'Claim', 'Evidence', 'Concluding Statement', 'Position'] * 20,\n            'discourse_effectiveness': ['Adequate', 'Effective', 'Ineffective', 'Adequate', 'Effective'] * 20\n        }\n        train_df = pd.DataFrame(train_data)\n        train_df.to_csv(TRAIN_CSV_PATH, index=False)\n        print(f\"'{TRAIN_CSV_PATH}' created.\")\n\n    # Create dummy essay text files\n    if not os.path.exists(TRAIN_ESSAYS_PATH):\n        os.makedirs(TRAIN_ESSAYS_PATH)\n        for i in range(10):\n            essay_id = f'essay_{i}'\n            essay_text = (\n                f\"This is the full text for essay {essay_id}. \"\n                \"It contains various discourse elements that are analyzed. \"\n                \"The student argues a point, provides evidence, and concludes the argument. \"\n                \"The quality of these elements varies.\"\n            )\n            with open(os.path.join(TRAIN_ESSAYS_PATH, f'{essay_id}.txt'), 'w') as f:\n                f.write(essay_text)\n        print(f\"'{TRAIN_ESSAYS_PATH}' directory and dummy essays created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:26:53.941709Z","iopub.execute_input":"2025-07-02T09:26:53.942000Z","iopub.status.idle":"2025-07-02T09:26:53.948344Z","shell.execute_reply.started":"2025-07-02T09:26:53.941978Z","shell.execute_reply":"2025-07-02T09:26:53.947530Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_data(train_csv_path, essays_path):\n    \"\"\"\n    Loads the training data from the CSV and merges it with the full essay texts.\n\n    Args:\n        train_csv_path (str): Path to the train.csv file.\n        essays_path (str): Path to the directory containing essay .txt files.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with the combined data.\n    \"\"\"\n    print(\"Loading data...\")\n    # Load the main training data\n    try:\n        df = pd.read_csv(train_csv_path)\n    except FileNotFoundError:\n        print(f\"Error: Training CSV not found at '{train_csv_path}'.\")\n        return None\n\n    # Load essay texts into a dictionary\n    essay_texts = {}\n    for filename in os.listdir(essays_path):\n        if filename.endswith('.txt'):\n            essay_id = filename.split('.')[0]\n            with open(os.path.join(essays_path, filename), 'r') as f:\n                essay_texts[essay_id] = f.read()\n\n    # Map the essay texts to the DataFrame\n    df['essay_full_text'] = df['essay_id'].map(essay_texts)\n    print(\"Data loading complete.\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:26:56.029874Z","iopub.execute_input":"2025-07-02T09:26:56.030159Z","iopub.status.idle":"2025-07-02T09:26:56.035616Z","shell.execute_reply.started":"2025-07-02T09:26:56.030139Z","shell.execute_reply":"2025-07-02T09:26:56.034815Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def feature_engineering(df):\n    \"\"\"\n    Creates a combined text feature for the model.\n\n    Args:\n        df (pandas.DataFrame): The input DataFrame.\n\n    Returns:\n        pandas.DataFrame: The DataFrame with the new 'full_context_text' feature.\n    \"\"\"\n    print(\"Performing feature engineering...\")\n    # Combine the discourse text, type, and the full essay for context\n    # Using a separator to give the model a hint about the different parts\n    df['full_context_text'] = (\n        df['discourse_type'] + ' [SEP] ' +\n        df['discourse_text'] + ' [SEP] ' +\n        df['essay_full_text']\n    )\n    print(\"Feature engineering complete.\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:26:58.271630Z","iopub.execute_input":"2025-07-02T09:26:58.271943Z","iopub.status.idle":"2025-07-02T09:26:58.276338Z","shell.execute_reply.started":"2025-07-02T09:26:58.271918Z","shell.execute_reply":"2025-07-02T09:26:58.275423Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Create dummy files if they don't exist (for local runs)\n    if not (os.path.exists(TRAIN_CSV_PATH) and os.path.exists(TRAIN_ESSAYS_PATH)):\n        create_dummy_data()\n\n    # 1. Load and Prepare Data\n    train_df = load_data(TRAIN_CSV_PATH, TRAIN_ESSAYS_PATH)\n\n    if train_df is not None:\n        train_df = feature_engineering(train_df)\n\n        # Define features (X) and target (y)\n        X = train_df['full_context_text']\n        y_raw = train_df['discourse_effectiveness']\n\n        # 2. Encode Target Labels\n        # The labels need to be converted from strings to integers (0, 1, 2)\n        print(\"Encoding target labels...\")\n        label_encoder = LabelEncoder()\n        y = label_encoder.fit_transform(y_raw)\n        # The mapping will be: Adequate -> 0, Effective -> 1, Ineffective -> 2\n        # We can see the classes with `label_encoder.classes_`\n        class_names = label_encoder.classes_\n        print(f\"Labels encoded. Class mapping: {dict(zip(class_names, range(len(class_names))))}\")\n\n\n        # 3. Split Data into Training and Validation Sets\n        print(\"Splitting data into training and validation sets (80/20 split)...\")\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y,\n            test_size=0.2,\n            random_state=42,\n            stratify=y  # Stratify to maintain class distribution in train/test splits\n        )\n        print(f\"Training set size: {len(X_train)}\")\n        print(f\"Validation set size: {len(X_val)}\")\n\n        # 4. Define the Model Pipeline\n        # A pipeline makes it easy to chain preprocessing and modeling steps.\n        # Step 1: TfidfVectorizer - Converts text to a matrix of TF-IDF features.\n        # Step 2: LogisticRegression - A simple, effective baseline model for text classification.\n        print(\"Defining the model pipeline...\")\n        pipeline = Pipeline([\n            ('tfidf', TfidfVectorizer(\n                ngram_range=(1, 3), # Use unigrams, bigrams, and trigrams\n                max_features=10000, # Limit the number of features to the top 10k\n                stop_words='english'\n            )),\n            ('clf', LogisticRegression(\n                solver='liblinear', # Good for smaller datasets\n                random_state=42,\n                C=1.0 # Regularization strength\n            ))\n        ])\n\n        # 5. Train the Model\n        print(\"Training the model...\")\n        pipeline.fit(X_train, y_train)\n        print(\"Model training complete.\")\n\n        # 6. Make Predictions on the Validation Set\n        print(\"Making predictions on the validation set...\")\n        y_pred = pipeline.predict(X_val)\n        y_pred_proba = pipeline.predict_proba(X_val)\n        print(\"Predictions complete.\")\n\n        # 7. Evaluate the Model\n        print(\"\\n--- Model Evaluation Results ---\")\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_val, y_pred)\n        loss = log_loss(y_val, y_pred_proba)\n        f1_macro = f1_score(y_val, y_pred, average='macro')\n        f1_weighted = f1_score(y_val, y_pred, average='weighted')\n        # For multiclass, roc_auc_score needs probabilities and multi_class='ovr'\n        roc_auc_ovr = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='macro')\n        roc_auc_ovr_weighted = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='weighted')\n\n        # Create a DataFrame for a clean display of results\n        results_df = pd.DataFrame({\n            'Metric': [\n                'Accuracy',\n                'Log Loss',\n                'F1 Score (Macro)',\n                'F1 Score (Weighted)',\n                'ROC AUC (OVR Macro)',\n                'ROC AUC (OVR Weighted)'\n            ],\n            'Score': [\n                accuracy,\n                loss,\n                f1_macro,\n                f1_weighted,\n                roc_auc_ovr,\n                roc_auc_ovr_weighted\n            ]\n        })\n\n        print(results_df.to_string(index=False))\n        print(\"--------------------------------\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:27:00.374762Z","iopub.execute_input":"2025-07-02T09:27:00.375340Z","iopub.status.idle":"2025-07-02T09:27:54.604018Z","shell.execute_reply.started":"2025-07-02T09:27:00.375316Z","shell.execute_reply":"2025-07-02T09:27:54.603173Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nData loading complete.\nPerforming feature engineering...\nFeature engineering complete.\nEncoding target labels...\nLabels encoded. Class mapping: {'Adequate': 0, 'Effective': 1, 'Ineffective': 2}\nSplitting data into training and validation sets (80/20 split)...\nTraining set size: 29412\nValidation set size: 7353\nDefining the model pipeline...\nTraining the model...\nModel training complete.\nMaking predictions on the validation set...\nPredictions complete.\n\n--- Model Evaluation Results ---\n                Metric    Score\n              Accuracy 0.714538\n              Log Loss 0.675059\n      F1 Score (Macro) 0.600052\n   F1 Score (Weighted) 0.681464\n   ROC AUC (OVR Macro) 0.828139\nROC AUC (OVR Weighted) 0.809148\n--------------------------------\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    log_loss,\n    f1_score,\n    roc_auc_score,\n)\n\n# Suppress all warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Configuration ---\n# Define paths to the data files and directories.\n# These paths are set up to work in a Kaggle-like environment.\nTRAIN_CSV_PATH = '/kaggle/input/feedback-prize-effectiveness/train.csv'\nTRAIN_DIR_PATH = '/kaggle/input/feedback-prize-effectiveness/train'\n\n# Define the mapping for target labels\nLABEL_MAPPING = {\n    'Ineffective': 0,\n    'Adequate': 1,\n    'Effective': 2\n}\n# Reverse mapping for display if needed\nREVERSE_LABEL_MAPPING = {v: k for k, v in LABEL_MAPPING.items()}\n\n# --- Data Loading and Preparation ---\n\ndef create_dummy_data():\n    \"\"\"\n    Creates dummy data files and directories for local testing if they don't exist.\n    This simulates the Kaggle environment for development purposes.\n    \"\"\"\n    print(\"Creating dummy data for local execution...\")\n\n    # Create parent directory for dummy data if it doesn't exist\n    os.makedirs(os.path.dirname(TRAIN_CSV_PATH), exist_ok=True)\n    os.makedirs(TRAIN_DIR_PATH, exist_ok=True)\n\n    # Create dummy train.csv\n    if not os.path.exists(TRAIN_CSV_PATH):\n        train_data = {\n            'discourse_id': ['d1', 'd2', 'd3', 'd4', 'd5', 'd6'],\n            'essay_id': ['e1', 'e1', 'e2', 'e2', 'e3', 'e3'],\n            'discourse_text': [\n                \"This is a strong claim about the topic.\",\n                \"Evidence shows that this is true.\",\n                \"Introduction to the essay.\",\n                \"A counter-argument presented here.\",\n                \"Concluding remarks.\",\n                \"Another piece of evidence.\"\n            ],\n            'discourse_type': [\n                'Claim', 'Evidence', 'Lead', 'Counterclaim', 'Concluding Statement', 'Evidence'\n            ],\n            'discourse_effectiveness': [\n                'Effective', 'Adequate', 'Ineffective', 'Adequate', 'Effective', 'Adequate'\n            ]\n        }\n        train_df = pd.DataFrame(train_data)\n        train_df.to_csv(TRAIN_CSV_PATH, index=False)\n        print(f\"'{TRAIN_CSV_PATH}' created.\")\n\n    # Create dummy essay text files\n    dummy_essays = {\n        'e1': \"This is the full essay text for essay e1. It contains a strong claim and supporting evidence.\",\n        'e2': \"Essay e2 starts with an introduction and then presents a counter-argument.\",\n        'e3': \"Essay e3 includes concluding remarks and additional evidence.\"\n    }\n    for essay_id, content in dummy_essays.items():\n        essay_file_path = os.path.join(TRAIN_DIR_PATH, f\"{essay_id}.txt\")\n        if not os.path.exists(essay_file_path):\n            with open(essay_file_path, 'w') as f:\n                f.write(content)\n            print(f\"'{essay_file_path}' created.\")\n\ndef load_data(train_csv_path, train_dir_path):\n    \"\"\"\n    Loads the training data from the CSV and the corresponding essay texts.\n\n    Args:\n        train_csv_path (str): Path to the train.csv file.\n        train_dir_path (str): Path to the directory containing essay text files.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with the loaded data, including essay texts.\n    \"\"\"\n    print(\"Loading data...\")\n    try:\n        df = pd.read_csv(train_csv_path)\n    except FileNotFoundError:\n        print(f\"Error: Training CSV not found at '{train_csv_path}'.\")\n        return None\n\n    # Load essay texts\n    essay_texts = {}\n    if os.path.exists(train_dir_path):\n        for filename in os.listdir(train_dir_path):\n            if filename.endswith('.txt'):\n                essay_id = filename.replace('.txt', '')\n                file_path = os.path.join(train_dir_path, filename)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    essay_texts[essay_id] = f.read()\n    else:\n        print(f\"Warning: Essay directory not found at '{train_dir_path}'. \"\n              \"Proceeding without full essay context.\")\n\n    # Map essay texts to the DataFrame\n    df['full_essay_text'] = df['essay_id'].map(essay_texts)\n    df['full_essay_text'] = df['full_essay_text'].fillna('') # Handle cases where essay text might be missing\n\n    print(\"Data loading complete.\")\n    return df\n\n# --- Main Execution Logic ---\nif __name__ == \"__main__\":\n    # Create dummy files if they don't exist (for local runs)\n    if not os.path.exists(TRAIN_CSV_PATH) or not os.path.exists(TRAIN_DIR_PATH):\n        create_dummy_data()\n\n    # 1. Load Data\n    train_df = load_data(TRAIN_CSV_PATH, TRAIN_DIR_PATH)\n\n    if train_df is not None:\n        # 2. Prepare Features and Target\n        print(\"Preparing features and target...\")\n        # Map target labels to numerical values\n        train_df['discourse_effectiveness_encoded'] = train_df['discourse_effectiveness'].map(LABEL_MAPPING)\n\n        # Combine discourse_text and discourse_type for the main text feature\n        # This allows TF-IDF to capture patterns related to both content and type\n        X = train_df['discourse_type'] + \" \" + train_df['discourse_text']\n        y = train_df['discourse_effectiveness_encoded']\n\n        # Ensure all labels are present in the training set for consistent evaluation\n        unique_labels = sorted(y.unique())\n        if len(unique_labels) < len(LABEL_MAPPING):\n            print(f\"Warning: Not all target classes ({len(unique_labels)}/{len(LABEL_MAPPING)}) \"\n                  \"are present in the dataset. This might affect metric calculations.\")\n\n        # 3. Split Data into Training and Validation Sets\n        print(\"Splitting data into training and validation sets (80/20 split)...\")\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y,\n            test_size=0.2,\n            random_state=42,\n            stratify=y  # Stratify to maintain class distribution across splits\n        )\n        print(f\"Training set size: {len(X_train)}\")\n        print(f\"Validation set size: {len(X_val)}\")\n\n        # 4. Define the Model Pipeline\n        print(\"Defining the model pipeline...\")\n        pipeline = Pipeline([\n            ('tfidf', TfidfVectorizer(\n                ngram_range=(1, 2),  # Use unigrams and bigrams\n                max_features=20000,  # Limit the number of features\n                stop_words='english' # Remove common English stop words\n            )),\n            ('clf', LogisticRegression(\n                solver='liblinear',  # Good for smaller datasets and L1/L2 regularization\n                random_state=42,\n                C=0.5,               # Regularization strength\n                multi_class='ovr'    # One-vs-Rest strategy for multi-class classification\n            ))\n        ])\n\n        # 5. Train the Model\n        print(\"Training the model...\")\n        pipeline.fit(X_train, y_train)\n        print(\"Model training complete.\")\n\n        # 6. Make Predictions on the Validation Set\n        print(\"Making predictions on the validation set...\")\n        y_pred = pipeline.predict(X_val)\n        y_pred_proba = pipeline.predict_proba(X_val) # Get probabilities for all classes\n        print(\"Predictions complete.\")\n\n        # 7. Evaluate the Model\n        print(\"\\n--- Model Evaluation Results ---\")\n        # Calculate metrics\n        accuracy = accuracy_score(y_val, y_pred)\n        loss = log_loss(y_val, y_pred_proba, labels=unique_labels) # Ensure labels are passed to log_loss\n\n        f1_macro = f1_score(y_val, y_pred, average='macro')\n        f1_weighted = f1_score(y_val, y_pred, average='weighted')\n\n        # ROC AUC for multi-class (OVR)\n        # roc_auc_score requires probabilities for all classes (y_pred_proba)\n        # and the true labels (y_val).\n        # The 'labels' parameter should be explicitly passed to handle cases\n        # where not all classes might be present in a small validation set,\n        # but the model predicts probabilities for all expected classes.\n        roc_auc_ovr = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='macro', labels=unique_labels)\n        roc_auc_ovr_weighted = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='weighted', labels=unique_labels)\n\n\n        # Create a DataFrame for a clean display of results\n        results_df = pd.DataFrame({\n            'Metric': [\n                'Accuracy',\n                'Log Loss',\n                'F1 Score (Macro)',\n                'F1 Score (Weighted)',\n                'ROC AUC (OVR, Macro)',\n                'ROC AUC (OVR, Weighted)'\n            ],\n            'Score': [\n                accuracy,\n                loss,\n                f1_macro,\n                f1_weighted,\n                roc_auc_ovr,\n                roc_auc_ovr_weighted\n            ]\n        })\n        print(results_df.to_string(index=False))\n        print(\"--------------------------------\\n\")\n\n    else:\n        print(\"Data loading failed. Exiting.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:24:12.766651Z","iopub.execute_input":"2025-07-02T09:24:12.766840Z","iopub.status.idle":"2025-07-02T09:25:38.866052Z","shell.execute_reply.started":"2025-07-02T09:24:12.766822Z","shell.execute_reply":"2025-07-02T09:25:38.865387Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nData loading complete.\nPreparing features and target...\nSplitting data into training and validation sets (80/20 split)...\nTraining set size: 29412\nValidation set size: 7353\nDefining the model pipeline...\nTraining the model...\nModel training complete.\nMaking predictions on the validation set...\nPredictions complete.\n\n--- Model Evaluation Results ---\n                 Metric    Score\n               Accuracy 0.645451\n               Log Loss 0.792616\n       F1 Score (Macro) 0.487924\n    F1 Score (Weighted) 0.593754\n   ROC AUC (OVR, Macro) 0.763501\nROC AUC (OVR, Weighted) 0.738914\n--------------------------------\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}