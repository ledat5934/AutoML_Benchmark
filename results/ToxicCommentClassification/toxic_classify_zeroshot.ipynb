{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":19018,"databundleVersionId":2703900,"sourceType":"competition"}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T15:29:52.787540Z","iopub.execute_input":"2025-07-01T15:29:52.787836Z","iopub.status.idle":"2025-07-01T15:29:53.237981Z","shell.execute_reply.started":"2025-07-01T15:29:52.787810Z","shell.execute_reply":"2025-07-01T15:29:53.233821Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nThis script trains a multilingual model for toxic comment classification.\n\nThe core approach is zero-shot cross-lingual transfer:\n1.  A pre-trained multilingual transformer model (XLM-RoBERTa) is used.\n2.  The model is fine-tuned *only* on the provided English-language training data.\n3.  The model's ability to generalize is tested on a non-English validation set.\n4.  Final predictions are made on the non-English test set.\n\"\"\"\n\n# # 1. SETUP AND IMPORTS\n# =======================\nprint(\"‚úÖ Starting setup...\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, \n    log_loss, \n    f1_score, \n    roc_auc_score\n)\nimport logging\n\n# Suppress verbose logging from transformers\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n# Ensure reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(\"TensorFlow Version:\", tf.__version__)\nprint(\"‚úÖ Setup complete.\")\n\n\n# # 2. CONFIGURATION\n# ====================\n# A configuration class to hold all hyperparameters and settings in one place.\nclass CFG:\n    # Model configuration\n    MODEL_NAME = 'jplu/tf-xlm-roberta-base' # A TF-specific version of xlm-roberta-base\n    \n    # Training parameters\n    MAX_LEN = 192       # Max sequence length for the tokenizer\n    EPOCHS = 3          # Number of training epochs\n    LEARNING_RATE = 1e-5 # AdamW optimizer learning rate\n    \n    # Batching - BATCH_SIZE_PER_REPLICA is the key setting for TPUs\n    # The global batch size will be BATCH_SIZE_PER_REPLICA * number of replicas\n    BATCH_SIZE_PER_REPLICA = 16\n    \n    # Data paths\n    # Kaggle datasets are typically located in /kaggle/input/\n    DATA_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_BIAS_PATH = os.path.join(DATA_PATH, \"jigsaw-unintended-bias-train.csv\")\n    TRAIN_TOXIC_PATH = os.path.join(DATA_PATH, \"jigsaw-toxic-comment-train.csv\")\n    VALIDATION_PATH = os.path.join(DATA_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(DATA_PATH, \"test.csv\")\n    SUBMISSION_PATH = os.path.join(DATA_PATH, \"sample_submission.csv\")\n    \n    # Internal validation split (from English data)\n    VALID_SPLIT_RATIO = 0.1\n\nprint(\"‚úÖ Configuration loaded.\")\n\n\n# # 3. TPU INITIALIZATION\n# =========================\n# This block is standard boilerplate for using TPUs in TensorFlow.\nprint(\"üöÄ Initializing TPU...\")\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(f\"‚úÖ TPU initialized. Running on: {tpu.master()}\")\nexcept ValueError:\n    # If TPU is not available, fall back to the default strategy (CPU/GPU)\n    strategy = tf.distribute.get_strategy()\n    print(\"‚ö†Ô∏è TPU not found. Running on CPU/GPU.\")\n\nprint(f\"‚úîÔ∏è Number of replicas: {strategy.num_replicas_in_sync}\")\n\n# Calculate global batch size\nCFG.GLOBAL_BATCH_SIZE = CFG.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nprint(f\"‚úîÔ∏è Global batch size: {CFG.GLOBAL_BATCH_SIZE}\")\n\n\n# # 4. DATA LOADING AND PREPROCESSING\n# =====================================\nprint(\"üîÑ Loading and preprocessing data...\")\n\n# --- Load Raw Data ---\n# Training data comes from two files. We'll combine them.\ntrain_bias_df = pd.read_csv(CFG.TRAIN_BIAS_PATH)\ntrain_toxic_df = pd.read_csv(CFG.TRAIN_TOXIC_PATH)\n\n# The 'unintended-bias' dataset has a toxicity score from 0.0 to 1.0.\n# We convert it to a binary label (1 if toxic, 0 if not).\ntrain_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n\n# Combine the two training sets, keeping only the necessary columns\ntrain_df = pd.concat([\n    train_bias_df[['comment_text', 'toxic']],\n    train_toxic_df[['comment_text', 'toxic']]\n]).drop_duplicates(subset=['comment_text']).reset_index(drop=True)\n\nprint(f\"Combined English training data shape: {train_df.shape}\")\n\n# Load non-English validation and test data\nvalid_df = pd.read_csv(CFG.VALIDATION_PATH)\ntest_df = pd.read_csv(CFG.TEST_PATH)\n\nprint(f\"Non-English validation data shape: {valid_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n\n# --- Split English data for local evaluation ---\n# This creates an internal validation set to check model performance on English data.\nX_train, X_val_eng, y_train, y_val_eng = train_test_split(\n    train_df['comment_text'].values,\n    train_df['toxic'].values,\n    test_size=CFG.VALID_SPLIT_RATIO,\n    random_state=42,\n    stratify=train_df['toxic'].values # Stratify to maintain class balance\n)\n\nprint(f\"Training samples (English): {len(X_train)}\")\nprint(f\"Internal validation samples (English): {len(X_val_eng)}\")\n\n# The primary validation set is the non-English one\nX_val_multi = valid_df['comment_text'].values\ny_val_multi = valid_df['toxic'].values\n\n# Test data\nX_test = test_df['comment_text'].values\n\nprint(\"‚úÖ Data loading and preprocessing complete.\")\n\n\n# # 5. TOKENIZATION\n# ===================\nprint(\"‚úçÔ∏è Initializing tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n\ndef encode_text(texts, tokenizer, max_len):\n    \"\"\"Encodes a list of texts into token IDs, attention masks, etc.\"\"\"\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False # XLM-R doesn't use token_type_ids\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids), np.array(attention_masks)\n\nprint(\"‚úçÔ∏è Tokenizing all datasets...\")\n# Tokenize all text data splits\nx_train_ids, x_train_mask = encode_text(X_train, tokenizer, CFG.MAX_LEN)\nx_val_eng_ids, x_val_eng_mask = encode_text(X_val_eng, tokenizer, CFG.MAX_LEN)\nx_val_multi_ids, x_val_multi_mask = encode_text(X_val_multi, tokenizer, CFG.MAX_LEN)\nx_test_ids, x_test_mask = encode_text(X_test, tokenizer, CFG.MAX_LEN)\n\nprint(\"‚úÖ Tokenization complete.\")\n\n\n# # 6. CREATE TF.DATA.DATASET\n# =============================\n# Using tf.data.Dataset is highly recommended for performance, especially on TPUs.\ndef create_tf_dataset(ids, masks, labels=None, shuffle=False, is_test=False):\n    \"\"\"Creates a tf.data.Dataset from numpy arrays.\"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': ids, 'attention_mask': masks}, labels) if not is_test else {'input_ids': ids, 'attention_mask': masks})\n    \n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=len(ids))\n        \n    # Batch and prefetch for performance\n    dataset = dataset.batch(CFG.GLOBAL_BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nprint(\"üìä Creating tf.data.Dataset objects...\")\n# Create datasets for training, validation (multilingual), and testing\ntrain_dataset = create_tf_dataset(x_train_ids, x_train_mask, y_train, shuffle=True)\nval_multi_dataset = create_tf_dataset(x_val_multi_ids, x_val_multi_mask, y_val_multi)\ntest_dataset = create_tf_dataset(x_test_ids, x_test_mask, is_test=True)\n\n# Create a separate dataset for evaluating on the English validation split\nval_eng_dataset = create_tf_dataset(x_val_eng_ids, x_val_eng_mask, y_val_eng)\n\nprint(\"‚úÖ TF Datasets created.\")\n\n\n# # 7. MODEL BUILDING\n# =====================\ndef build_model(model_name, max_len):\n    \"\"\"Builds and compiles the Keras model within the TPU strategy scope.\"\"\"\n    with strategy.scope():\n        # Input layers\n        input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n        attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n        # Load the pre-trained transformer model\n        transformer_encoder = TFAutoModel.from_pretrained(model_name)\n        \n        # The output is a dictionary. We want the 'last_hidden_state'.\n        # We take the embedding of the [CLS] token (at index 0) for classification.\n        embedding = transformer_encoder(input_ids, attention_mask=attention_mask)[0][:, 0, :]\n\n        # Classification head\n        output = tf.keras.layers.Dense(1, activation='sigmoid')(embedding)\n        \n        # Define the model\n        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n        # Compile the model with AdamW optimizer, Binary Crossentropy loss, and AUC metric\n        optimizer = tf.keras.optimizers.AdamW(learning_rate=CFG.LEARNING_RATE)\n        model.compile(\n            optimizer=optimizer,\n            loss='binary_crossentropy',\n            metrics=[tf.keras.metrics.AUC(name='auc')]\n        )\n    return model\n\nprint(\"üß† Building the model...\")\nmodel = build_model(CFG.MODEL_NAME, CFG.MAX_LEN)\nmodel.summary()\nprint(\"‚úÖ Model built and compiled.\")\n\n\n# # 8. TRAINING\n# ===============\nprint(\"üöÇ Starting training...\")\n\n# Callbacks\n# Save the best model based on the multilingual validation AUC\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'best_model.h5', \n    monitor='val_auc', \n    mode='max', \n    save_best_only=True,\n    save_weights_only=True\n)\n\n# Early stopping to prevent overfitting\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc',\n    patience=1, # Stop after 1 epoch of no improvement\n    mode='max',\n    restore_best_weights=True # This is redundant if save_best_only=True but good practice\n)\n\nhistory = model.fit(\n    train_dataset,\n    epochs=CFG.EPOCHS,\n    validation_data=val_multi_dataset, # Key: validate on non-English data\n    callbacks=[model_checkpoint, early_stopping]\n)\n\nprint(\"‚úÖ Training complete.\")\n\n# # 9. EVALUATION ON ENGLISH SPLIT\n# ==================================\n# After training, we explicitly evaluate the model's performance on the held-out English data.\nprint(\"\\n\" + \"=\"*50)\nprint(\"üìà Evaluating on the held-out English validation set...\")\n\n# Load the best weights saved during training\nprint(\"Loading best model weights from 'best_model.h5'...\")\nmodel.load_weights('best_model.h5')\n\n# Get predictions (probabilities)\ny_pred_eng_probs = model.predict(val_eng_dataset, verbose=1).flatten()\n# Get class predictions (0 or 1)\ny_pred_eng_class = (y_pred_eng_probs >= 0.5).astype(int)\n\n# Calculate metrics as requested\naccuracy = accuracy_score(y_val_eng, y_pred_eng_class)\nloss = log_loss(y_val_eng, y_pred_eng_probs)\nf1_macro = f1_score(y_val_eng, y_pred_eng_class, average='macro')\nf1_weighted = f1_score(y_val_eng, y_pred_eng_class, average='weighted')\n# For binary classification, 'ovr' and 'weighted' AUC are the same as the standard AUC.\nroc_auc = roc_auc_score(y_val_eng, y_pred_eng_probs)\n\nprint(\"\\n--- English Validation Metrics ---\")\nprint(f\"Accuracy         : {accuracy:.4f}\")\nprint(f\"Log Loss         : {loss:.4f}\")\nprint(f\"F1 Score (Macro) : {f1_macro:.4f}\")\nprint(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\nprint(f\"ROC AUC          : {roc_auc:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\n\n# # 10. INFERENCE AND SUBMISSION\n# ================================\nprint(\"üöÄ Making predictions on the test set...\")\n\n# Predict on the test dataset\ntest_predictions = model.predict(test_dataset, verbose=1).flatten()\n\n# Create submission file\nprint(\"üìù Creating submission file...\")\nsubmission_df = pd.read_csv(CFG.SUBMISSION_PATH)\nsubmission_df['toxic'] = test_predictions\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nüéâ Submission file 'submission.csv' created successfully!\")\nprint(\"Top 5 rows of the submission file:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T15:29:55.419358Z","iopub.execute_input":"2025-07-01T15:29:55.419781Z","execution_failed":"2025-07-01T15:30:04.803Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Starting setup...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1751383795.746842     372 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}