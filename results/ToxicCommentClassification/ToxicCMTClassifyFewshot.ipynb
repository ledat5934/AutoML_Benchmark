{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KByNkUlwr7T",
        "outputId": "87218a83-0192-4ed9-9323-0589d3edc3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data loading complete.\n",
            "Splitting data into training and validation sets (80/20 split)...\n",
            "Training set size: 178839\n",
            "Validation set size: 44710\n",
            "Defining the model pipeline...\n",
            "Training the model...\n",
            "Model training complete.\n",
            "Making predictions on the validation set...\n",
            "Predictions complete.\n",
            "\n",
            "--- Model Evaluation Results ---\n",
            "             Metric    Score\n",
            "           Accuracy 0.949631\n",
            "           Log Loss 0.133818\n",
            "   F1 Score (Macro) 0.822126\n",
            "F1 Score (Weighted) 0.943912\n",
            "            ROC AUC 0.966764\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define paths to the data files.\n",
        "TRAIN_CSV_PATH = '/content/jigsaw-toxic-comment-train.csv'\n",
        "\n",
        "# --- Data Loading and Preparation ---\n",
        "def create_dummy_data():\n",
        "    \"\"\"Creates dummy data file for local testing if it doesn't exist.\n",
        "    This simulates the Kaggle environment.\n",
        "    \"\"\"\n",
        "    print(\"Creating dummy data for local execution...\")\n",
        "    if not os.path.exists(os.path.dirname(TRAIN_CSV_PATH)):\n",
        "        os.makedirs(os.path.dirname(TRAIN_CSV_PATH))\n",
        "\n",
        "    # Create dummy train.csv\n",
        "    if not os.path.exists(TRAIN_CSV_PATH):\n",
        "        train_data = {\n",
        "            'comment_text': [\n",
        "                \"This is a toxic comment.\",\n",
        "                \"I love this product!\",\n",
        "                \"You are an idiot.\",\n",
        "                \"What a beautiful day.\",\n",
        "                \"Go kill yourself.\",\n",
        "                \"Hello world.\",\n",
        "                \"This is terrible.\",\n",
        "                \"Thank you for your help.\",\n",
        "                \"You are the worst.\",\n",
        "                \"Nice work!\"\n",
        "            ],\n",
        "            'toxic': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "        }\n",
        "        train_df = pd.DataFrame(train_data)\n",
        "        train_df.to_csv(TRAIN_CSV_PATH, index=False)\n",
        "        print(f\"'{TRAIN_CSV_PATH}' created.\")\n",
        "\n",
        "def load_data(train_csv_path):\n",
        "    \"\"\"Loads the training data from the CSV.\n",
        "    Args:\n",
        "        train_csv_path (str): Path to the train.csv file.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame with the loaded data.\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(train_csv_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Training CSV not found at '{train_csv_path}'.\")\n",
        "        return None\n",
        "    print(\"Data loading complete.\")\n",
        "    return df\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dummy files if they don't exist (for local runs)\n",
        "    if not os.path.exists(TRAIN_CSV_PATH):\n",
        "        create_dummy_data()\n",
        "\n",
        "    # 1. Load Data\n",
        "    train_df = load_data(TRAIN_CSV_PATH)\n",
        "\n",
        "    if train_df is not None:\n",
        "        # Define features (X) and target (y)\n",
        "        X = train_df['comment_text']\n",
        "        y = train_df['toxic']\n",
        "\n",
        "        # 2. Split Data into Training and Validation Sets\n",
        "        print(\"Splitting data into training and validation sets (80/20 split)...\")\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=y  # Stratify to maintain class distribution in train/test splits\n",
        "        )\n",
        "        print(f\"Training set size: {len(X_train)}\")\n",
        "        print(f\"Validation set size: {len(X_val)}\")\n",
        "\n",
        "        # 3. Define the Model Pipeline\n",
        "        print(\"Defining the model pipeline...\")\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(\n",
        "                ngram_range=(1, 2), # Use unigrams and bigrams\n",
        "                max_features=20000, # Limit the number of features\n",
        "                stop_words='english'\n",
        "            )),\n",
        "            ('clf', LogisticRegression(\n",
        "                solver='liblinear', # Good for smaller datasets and L1/L2 regularization\n",
        "                random_state=42,\n",
        "                C=0.5 # Regularization strength\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # 4. Train the Model\n",
        "        print(\"Training the model...\")\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        print(\"Model training complete.\")\n",
        "\n",
        "        # 5. Make Predictions on the Validation Set\n",
        "        print(\"Making predictions on the validation set...\")\n",
        "        y_pred = pipeline.predict(X_val)\n",
        "        y_pred_proba = pipeline.predict_proba(X_val)[:, 1] # Get probabilities for the positive class (toxic)\n",
        "        print(\"Predictions complete.\")\n",
        "\n",
        "        # 6. Evaluate the Model\n",
        "        print(\"\\n--- Model Evaluation Results ---\")\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        loss = log_loss(y_val, y_pred_proba)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro')\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted')\n",
        "        roc_auc_ovr = roc_auc_score(y_val, y_pred_proba) # For binary classification, roc_auc_score handles it directly\n",
        "\n",
        "        # Create a DataFrame for a clean display of results\n",
        "        results_df = pd.DataFrame({\n",
        "            'Metric': [\n",
        "                'Accuracy',\n",
        "                'Log Loss',\n",
        "                'F1 Score (Macro)',\n",
        "                'F1 Score (Weighted)',\n",
        "                'ROC AUC'\n",
        "            ],\n",
        "            'Score': [\n",
        "                accuracy,\n",
        "                loss,\n",
        "                f1_macro,\n",
        "                f1_weighted,\n",
        "                roc_auc_ovr\n",
        "            ]\n",
        "        })\n",
        "        print(results_df.to_string(index=False))\n",
        "        print(\"--------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZukW1MGx_bR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
