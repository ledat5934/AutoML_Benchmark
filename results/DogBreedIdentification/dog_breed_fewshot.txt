You are an expert programmer with years of experience in writing codes for machine learning (ML) tasks on Kaggle. Your goal is to write an end-to-end executable program that can solve the ML task based on the provided instructions.  The user is trying to create a Python program for a supervised machine learning task. Your goal is to assist the user in creating the program. The user might provide the following information for the task:

1. A description of the input data: The input data is a single color image of a dog.

2. A description of the output data: The output is a probability distribution across 120 possible classes. Each class represents a specific dog breed. For each input image, the model must predict the probability that the dog belongs to each of the 120 breeds.

3. The task objective: For each given image of a dog, create a classifier to determine the dog's breed. This is a multi-class classification problem with 120 classes.

4. The evaluation metrics: The evaluation metric is Multi-Class Log Loss.

5. A description of the available files:
- `train.zip`: A zip file containing the training set of dog images.
- `test.zip`: A zip file containing the test set of dog images.
- `labels.csv`: The main training metadata file. It contains two columns: `id` (which corresponds to the image filename) and `breed` (the correct breed label for that image).
- `sample_submission.csv`: An example file showing the required submission format, which includes an `id` column and a probability column for each of the 120 breeds.
Run on Kaggle, GPU T4x2
split the train to evaluate by these folowing metrics: accuracy log_loss f1_macro f1_weighted roc_auc_ovr roc_auc_ovr_weighted
example: # This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current sessionimport torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as dsets
from torchvision.models import vgg16, resnet18
from torchvision import datasets, transforms
import torchvision.transforms as transforms
import torchvision.utils as utils
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm

import matplotlib.pyplot as plt
import numpy as np 
import time
import os
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from torch.utils.data import random_split

torch.manual_seed(0)# Define the transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to match EfficientNet's input size
    transforms.ToTensor(),         # Convert to tensor
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize for ImageNet
])rom PIL import Image

class TestDataset(Dataset):
    def __init__(self, root, transform = None):
        self.root = root
        self.transform = transform
        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root) if fname.endswith(('.png','.jpg','.jpeg'))]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self,idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)

        return image, img_path 

train_data = datasets.ImageFolder(root='/kaggle/input/deep-learning-practice-image-classification/train', transform=transform)

test_data = TestDataset('/kaggle/input/deep-learning-practice-image-classification/test', transform = transform) len(train_data),type(train_data) num_classes = len(train_data.classes)
num_classes 

train_size = int(0.8*len(train_data))
val_size = len(train_data)-train_size 
train_dataset, val_dataset = random_split(train_data, [train_size, val_size])

len(train_dataset), len(val_dataset) 

label_map = {idx: label for idx, label in enumerate(train_data.classes)}
print("Label map:", label_map)


def inverse_normalize(tensor, mean, std):
    """
    Inverses the normalization of a tensor for visualization.
    """
    mean = torch.tensor(mean).view(3, 1, 1)
    std = torch.tensor(std).view(3, 1, 1)
    return tensor * std + mean

def show_image(data, idx, label_map):
    '''
    Function: Displays a single image from the dataset
    
    Inputs:
    - data: dataset (e.g., ImageFolder or CIFAR-10 dataset)
    - idx: index of the image in the dataset
    - label_map: mapping of label indices to class names
    
    Returns:
    - Displays the original and transformed images side by side
    '''
    # Retrieve image and label
    transformed_image, label = data[idx]  # For datasets like ImageFolder or CIFAR-10
    
    # Inverse normalize the image for display
    transformed_image = inverse_normalize(
        transformed_image, 
        mean=[0.491, 0.482, 0.447],  # CIFAR-10 mean
        std=[0.247, 0.244, 0.262]   # CIFAR-10 std
    )
    
    # Convert the tensor to an image format (H, W, C)
    transformed_image = transformed_image.permute(1, 2, 0).clamp(0, 1)  # Ensure values are in [0, 1]
    
    # Plot the transformed image
    plt.figure(figsize=(5, 5))
    plt.imshow(transformed_image)
    plt.title(f'Transformed: {label_map[label]}')
    plt.axis('off')  # Hide axes
    plt.show()

batch_size = 128

train_loader = DataLoader(train_dataset, batch_size =batch_size, shuffle = True)
val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)

print(f'Classes: {train_data.classes}') 

test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
import timm
# Load pre-trained EfficientNet
model = timm.create_model('efficientnet_b0', pretrained=True)

# Replace the classifier with a custom layer
model.classifier = nn.Linear(model.classifier.in_features, num_classes)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
# Training loop
epochs = 5
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Zero the parameter gradients
        optimizer.zero_grad()
        
        # Forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        # Track loss and accuracy
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)
    
    print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total:.2f}%")


# Validation loop
model.eval()
val_loss = 0.0
correct = 0
total = 0

with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        val_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)

print(f"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total:.2f}%")

#on test data
model.eval()
predictions = []
with torch.no_grad():
    for images, img_names in test_loader:
        images = images.to(device)
        outputs = model(images)
        _,predicted = torch.max(outputs, 1)
        for img_name, label in zip(img_names, predicted.cpu().numpy()):
            # print(img_name.split('/')[5].split('.')[0])
            predictions.append({'Image_ID':img_name.split('/')[5].split('.')[0], 'Label':int(label)})
import pandas as pd
submission_df = pd.DataFrame(predictions)
submission_df.to_csv('submision.csv', index = False)