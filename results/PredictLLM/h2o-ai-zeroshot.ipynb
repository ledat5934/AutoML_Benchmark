{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63021,"databundleVersionId":6870560,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:06:46.904429Z","iopub.execute_input":"2025-06-29T15:06:46.904589Z","iopub.status.idle":"2025-06-29T15:06:48.449817Z","shell.execute_reply.started":"2025-06-29T15:06:46.904574Z","shell.execute_reply":"2025-06-29T15:06:48.449053Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/h2oai-predict-the-llm/sample_submission.csv\n/kaggle/input/h2oai-predict-the-llm/train.csv\n/kaggle/input/h2oai-predict-the-llm/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nH2O.ai - LLM Science Exam: Predict the LLM\n-------------------------------------------\n\nThis script provides an end-to-end solution for the Kaggle competition \"H2O.ai - LLM Science Exam\".\nThe goal is to identify which of 7 Large Language Models (LLMs) generated a given response\nto a question.\n\nApproach:\n1.  **Model Selection**: We use a pre-trained `microsoft/deberta-v3-base` model, a powerful\n    transformer architecture well-suited for understanding nuanced text. We add a classification\n    head on top to predict one of the 7 LLM classes.\n\n2.  **Input Formatting**: The `Question` and `Response` texts are concatenated into a single\n    input sequence for the model, separated by a special `[SEP]` token. This allows the model\n    to learn the relationship between the prompt and the generated answer.\n    Example: `[CLS] What is the capital of France? [SEP] The capital of France is Paris. [SEP]`\n\n3.  **Frameworks**: The solution is built using PyTorch for model training and inference, and the\n    Hugging Face `transformers` library for easy access to the pre-trained model and tokenizer.\n\n4.  **Training**: The script trains the model on the full training dataset for a few epochs.\n    This is a common and effective strategy for competitions, especially when building a baseline.\n    The evaluation metric is Log Loss, so the model is trained to output class probabilities using\n    a standard Cross-Entropy Loss function.\n\n5.  **Inference & Submission**: After training, the script runs inference on the test set,\n    calculates the probabilities for each of the 7 classes using a Softmax function, and\n    formats the results into the required `submission.csv` file.\n\nThis script is self-contained and ready to run in a Kaggle environment with GPU acceleration.\n\"\"\"\n\n# =====================================================================================\n# 1. SETUP AND IMPORTS\n# =====================================================================================\nimport os\nimport gc\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW # <<< CORRECTED IMPORT\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# =====================================================================================\n# 2. CONFIGURATION\n# =====================================================================================\nclass CFG:\n    \"\"\"\n    Configuration class for all hyperparameters and settings.\n    \"\"\"\n    # General\n    seed = 42\n    num_workers = os.cpu_count() or 2\n    \n    # Paths\n    data_dir = \"/kaggle/input/h2oai-predict-the-llm/\"\n    \n    # Model: DeBERTa-v3 is a strong baseline for text classification tasks.\n    model_name = \"microsoft/deberta-v3-base\"\n    \n    # Training Parameters\n    epochs = 3\n    train_batch_size = 8\n    eval_batch_size = 16\n    lr = 2e-5\n    max_length = 512  # Max sequence length for the transformer\n    num_labels = 7    # Corresponds to the 7 different LLM models\n    \n    # Hardware\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =====================================================================================\n# 3. UTILITY FUNCTIONS\n# =====================================================================================\ndef set_seed(seed_value):\n    \"\"\"\n    Sets the seed for reproducibility in numpy and torch.\n    \"\"\"\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"Seed set to {seed_value}\")\n\n# =====================================================================================\n# 4. DATASET PREPARATION\n# =====================================================================================\nclass LLMDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for loading and tokenizing the data.\n    \"\"\"\n    def __init__(self, df, tokenizer, max_length, is_test=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_test = is_test\n        # Combine 'Question' and 'Response' into a single text entry\n        self.texts = (df['Question'] + self.tokenizer.sep_token + df['Response']).tolist()\n        if not self.is_test:\n            self.labels = df['target'].tolist()\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Tokenize the combined text\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        # Squeeze to remove the batch dimension from tokenization\n        ids = inputs['input_ids'].squeeze(0)\n        mask = inputs['attention_mask'].squeeze(0)\n        token_type_ids = inputs['token_type_ids'].squeeze(0)\n        \n        if self.is_test:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids\n            }\n        else:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n            }\n\n# =====================================================================================\n# 5. TRAINING AND INFERENCE LOGIC\n# =====================================================================================\ndef train_one_epoch(model, train_loader, optimizer, scheduler, device):\n    \"\"\"\n    Performs one full epoch of training.\n    \"\"\"\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        \n        # Move batch to device\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n        \n    avg_loss = total_loss / len(train_loader)\n    return avg_loss\n\ndef predict_probabilities(model, test_loader, device):\n    \"\"\"\n    Runs inference on the test set and returns class probabilities.\n    \"\"\"\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc=\"Predicting\", leave=False)\n        for batch in progress_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n            \n            # Get logits and apply softmax to convert to probabilities\n            logits = outputs.logits\n            probs = torch.softmax(logits, dim=1).cpu().numpy()\n            all_preds.append(probs)\n            \n    return np.vstack(all_preds)\n\n# =====================================================================================\n# 6. MAIN EXECUTION SCRIPT\n# =====================================================================================\ndef main():\n    print(\"--- Starting LLM Identification Program ---\")\n    \n    # Set seed for deterministic results\n    set_seed(CFG.seed)\n    \n    print(f\"Using device: {CFG.device}\")\n    \n    # Load data\n    print(\"\\nLoading data...\")\n    train_df = pd.read_csv(os.path.join(CFG.data_dir, \"train.csv\"))\n    test_df = pd.read_csv(os.path.join(CFG.data_dir, \"test.csv\"))\n    submission_df = pd.read_csv(os.path.join(CFG.data_dir, \"sample_submission.csv\"))\n    print(f\"Training data shape: {train_df.shape}\")\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Initialize tokenizer\n    print(f\"\\nInitializing tokenizer: {CFG.model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n    \n    # Create datasets and dataloaders\n    print(\"\\nCreating datasets and dataloaders...\")\n    train_dataset = LLMDataset(train_df, tokenizer, CFG.max_length, is_test=False)\n    test_dataset = LLMDataset(test_df, tokenizer, CFG.max_length, is_test=True)\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CFG.train_batch_size,\n        shuffle=True,\n        num_workers=CFG.num_workers,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=CFG.eval_batch_size,\n        shuffle=False,\n        num_workers=CFG.num_workers,\n        pin_memory=True\n    )\n    \n    # Initialize model\n    print(f\"\\nInitializing model: {CFG.model_name}\")\n    model = AutoModelForSequenceClassification.from_pretrained(\n        CFG.model_name, \n        num_labels=CFG.num_labels\n    )\n    model.to(CFG.device)\n    \n    # Setup optimizer and learning rate scheduler\n    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n    num_training_steps = len(train_loader) * CFG.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0, # No warmup steps for this baseline\n        num_training_steps=num_training_steps\n    )\n    \n    # --- Training Loop ---\n    # For a more robust solution, one would use K-Fold Cross-Validation.\n    # For this script, we train on the full training data.\n    print(\"\\n--- Starting Training ---\")\n    for epoch in range(CFG.epochs):\n        print(f\"\\n--- Epoch {epoch + 1}/{CFG.epochs} ---\")\n        avg_loss = train_one_epoch(model, train_loader, optimizer, scheduler, CFG.device)\n        print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_loss:.4f}\")\n    \n    print(\"\\n--- Training Finished ---\")\n    \n    # --- Inference ---\n    print(\"\\n--- Starting Inference on Test Data ---\")\n    predictions = predict_probabilities(model, test_loader, CFG.device)\n    \n    # --- Create Submission File ---\n    print(\"\\nGenerating submission file...\")\n    target_cols = [f'target_{i}' for i in range(CFG.num_labels)]\n    submission_df[target_cols] = predictions\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission file 'submission.csv' created successfully!\")\n    print(submission_df.head())\n    \n    # Clean up GPU memory\n    del model, train_loader, test_loader, train_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    print(\"\\n--- Program Finished ---\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:09:58.988958Z","iopub.execute_input":"2025-06-29T15:09:58.989329Z","iopub.status.idle":"2025-06-29T15:10:52.329404Z","shell.execute_reply.started":"2025-06-29T15:09:58.989303Z","shell.execute_reply":"2025-06-29T15:10:52.328136Z"}},"outputs":[{"name":"stdout","text":"--- Starting LLM Identification Program ---\nSeed set to 42\nUsing device: cuda\n\nLoading data...\nTraining data shape: (3976, 3)\nTest data shape: (1001, 3)\n\nInitializing tokenizer: microsoft/deberta-v3-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbdc7d74b72f43ebb72f6eb0ead7b32c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"615f6e0cac184f7685c1dc9e7d7097e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2896dd9c03114696bc4b861006ad2b44"}},"metadata":{}},{"name":"stdout","text":"\nCreating datasets and dataloaders...\n\nInitializing model: microsoft/deberta-v3-base\n","output_type":"stream"},{"name":"stderr","text":"2025-06-29 15:10:04.436314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751209804.652546      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751209804.711599      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18459f4f5394da7bde1b84bd5448def"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cee76be38bc4f20b310b87327c5e3ab"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting Training ---\n\n--- Epoch 1/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ddaf12771334fbb9d9af60830328d4b"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4187207570.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/4187207570.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Epoch {epoch + 1}/{CFG.epochs} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1} - Average Training Loss: {avg_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/4187207570.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_35/4187207570.py\", line 126, in __getitem__\n    inputs = self.tokenizer.encode_plus(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3073, in encode_plus\n    return self._encode_plus(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\", line 613, in _encode_plus\n    batched_output = self._batch_encode_plus(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\", line 539, in _batch_encode_plus\n    encodings = self._tokenizer.encode_batch(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"],"ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_35/4187207570.py\", line 126, in __getitem__\n    inputs = self.tokenizer.encode_plus(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3073, in encode_plus\n    return self._encode_plus(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\", line 613, in _encode_plus\n    batched_output = self._batch_encode_plus(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\", line 539, in _batch_encode_plus\n    encodings = self._tokenizer.encode_batch(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}