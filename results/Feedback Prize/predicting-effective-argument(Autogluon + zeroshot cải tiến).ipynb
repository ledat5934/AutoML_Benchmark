{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:31:00.447727Z",
     "iopub.status.busy": "2025-06-30T14:31:00.447503Z",
     "iopub.status.idle": "2025-06-30T14:33:13.903637Z",
     "shell.execute_reply": "2025-06-30T14:33:13.902898Z",
     "shell.execute_reply.started": "2025-06-30T14:31:00.447702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U autogluon > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:50:50.956460Z",
     "iopub.status.busy": "2025-06-30T14:50:50.956167Z",
     "iopub.status.idle": "2025-06-30T14:50:52.278653Z",
     "shell.execute_reply": "2025-06-30T14:50:52.277790Z",
     "shell.execute_reply.started": "2025-06-30T14:50:50.956409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T15:39:40.082548Z",
     "iopub.status.busy": "2025-06-30T15:39:40.081872Z",
     "iopub.status.idle": "2025-06-30T15:39:40.234816Z",
     "shell.execute_reply": "2025-06-30T15:39:40.233930Z",
     "shell.execute_reply.started": "2025-06-30T15:39:40.082528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu huấn luyện có 36765 hàng.\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/input/feedback-prize-effectiveness'\n",
    "train_csv_path = os.path.join(data_path, 'train.csv')\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(\"Dữ liệu huấn luyện có\", len(train_df), \"hàng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:51:05.289197Z",
     "iopub.status.busy": "2025-06-30T14:51:05.288919Z",
     "iopub.status.idle": "2025-06-30T14:51:05.362679Z",
     "shell.execute_reply": "2025-06-30T14:51:05.361820Z",
     "shell.execute_reply.started": "2025-06-30T14:51:05.289174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(train_df, test_size = 0.2, random_state = 3, stratify = train_df['discourse_effectiveness'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:51:06.995897Z",
     "iopub.status.busy": "2025-06-30T14:51:06.995075Z",
     "iopub.status.idle": "2025-06-30T14:51:06.999810Z",
     "shell.execute_reply": "2025-06-30T14:51:06.998905Z",
     "shell.execute_reply.started": "2025-06-30T14:51:06.995866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label = 'discourse_effectiveness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:51:08.554332Z",
     "iopub.status.busy": "2025-06-30T14:51:08.553429Z",
     "iopub.status.idle": "2025-06-30T15:01:21.971704Z",
     "shell.execute_reply": "2025-06-30T15:01:21.971107Z",
     "shell.execute_reply.started": "2025-06-30T14:51:08.554296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250630_145108\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       29.81 GB / 31.35 GB (95.1%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250630_145108\"\n",
      "Train Data Rows:    29412\n",
      "Train Data Columns: 4\n",
      "Label Column:       discourse_effectiveness\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\t3 unique label values:  ['Adequate', 'Effective', 'Ineffective']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30545.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.38 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['discourse_text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['discourse_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['discourse_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])       : 2 | ['essay_id', 'discourse_type']\n",
      "\t\t('object', ['text']) : 1 | ['discourse_text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    2 | ['essay_id', 'discourse_type']\n",
      "\t\t('category', ['text_as_category'])  :    1 | ['discourse_text']\n",
      "\t\t('int', ['binned', 'text_special']) :   22 | ['discourse_text.char_count', 'discourse_text.word_count', 'discourse_text.capital_ratio', 'discourse_text.lower_ratio', 'discourse_text.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 9030 | ['__nlp__.000', '__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', ...]\n",
      "\t48.0s = Fit runtime\n",
      "\t3 features in original data used to generate 9055 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 507.30 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 52.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.08499932000543996, Train Rows: 26912, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 547.23s of the 547.23s of remaining time.\n",
      "\t-2.3351\t = Validation score   (-log_loss)\n",
      "\t10.78s\t = Training   runtime\n",
      "\t7.24s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 523.92s of the 523.92s of remaining time.\n",
      "\t-2.3524\t = Validation score   (-log_loss)\n",
      "\t10.49s\t = Training   runtime\n",
      "\t7.21s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 501.01s of the 501.00s of remaining time.\n",
      "No improvement since epoch 1: early stopping\n",
      "\t-0.726\t = Validation score   (-log_loss)\n",
      "\t33.03s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 467.89s of the 467.89s of remaining time.\n",
      "\t-0.7427\t = Validation score   (-log_loss)\n",
      "\t65.8s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 401.39s of the 401.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.652658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6438\t = Validation score   (-log_loss)\n",
      "\t70.81s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 329.32s of the 329.32s of remaining time.\n",
      "\t-0.8224\t = Validation score   (-log_loss)\n",
      "\t168.01s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 160.16s of the 160.16s of remaining time.\n",
      "\t-0.8151\t = Validation score   (-log_loss)\n",
      "\t136.99s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 21.98s of the 21.97s of remaining time.\n",
      "\tMany features detected (9055), dynamically setting 'colsample_bylevel' to 0.11043622308117063 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tRan out of time, early stopping on iteration 1.\n",
      "\t-1.0825\t = Validation score   (-log_loss)\n",
      "\t15.68s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 5.76s of the 5.76s of remaining time.\n",
      "\tWarning: Model is expected to require 247.4s to train, which exceeds the maximum time limit of 1.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -5.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.65, 'NeuralNetFastAI': 0.3, 'RandomForestEntr': 0.05}\n",
      "\t-0.6112\t = Validation score   (-log_loss)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 608.02s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1818.8 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250630_145108\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=label,\n",
    "    eval_metric='log_loss'\n",
    ").fit(\n",
    "    train_data = train_data,\n",
    "    time_limit = 600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T15:39:47.090921Z",
     "iopub.status.busy": "2025-06-30T15:39:47.090670Z",
     "iopub.status.idle": "2025-06-30T15:41:06.582956Z",
     "shell.execute_reply": "2025-06-30T15:41:06.582202Z",
     "shell.execute_reply.started": "2025-06-30T15:39:47.090903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>roc_auc_ovr</th>\n",
       "      <th>roc_auc_ovr_weighted</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.607523</td>\n",
       "      <td>0.733850</td>\n",
       "      <td>-0.607523</td>\n",
       "      <td>0.639545</td>\n",
       "      <td>0.710914</td>\n",
       "      <td>0.850435</td>\n",
       "      <td>0.835072</td>\n",
       "      <td>-0.611240</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5.570127</td>\n",
       "      <td>1.374568</td>\n",
       "      <td>241.117007</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.292353</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>-0.630263</td>\n",
       "      <td>0.734938</td>\n",
       "      <td>-0.630263</td>\n",
       "      <td>0.650482</td>\n",
       "      <td>0.716398</td>\n",
       "      <td>0.845190</td>\n",
       "      <td>0.829035</td>\n",
       "      <td>-0.643773</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.972107</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>70.811756</td>\n",
       "      <td>2.972107</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>70.811756</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>-0.726364</td>\n",
       "      <td>0.685299</td>\n",
       "      <td>-0.726364</td>\n",
       "      <td>0.582364</td>\n",
       "      <td>0.659501</td>\n",
       "      <td>0.796201</td>\n",
       "      <td>0.773060</td>\n",
       "      <td>-0.742729</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.447841</td>\n",
       "      <td>0.568084</td>\n",
       "      <td>65.796991</td>\n",
       "      <td>2.447841</td>\n",
       "      <td>0.568084</td>\n",
       "      <td>65.796991</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>-0.731691</td>\n",
       "      <td>0.694546</td>\n",
       "      <td>-0.731691</td>\n",
       "      <td>0.624737</td>\n",
       "      <td>0.684699</td>\n",
       "      <td>0.808582</td>\n",
       "      <td>0.794917</td>\n",
       "      <td>-0.725978</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.287795</td>\n",
       "      <td>0.049090</td>\n",
       "      <td>33.025633</td>\n",
       "      <td>0.287795</td>\n",
       "      <td>0.049090</td>\n",
       "      <td>33.025633</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>-0.801031</td>\n",
       "      <td>0.645451</td>\n",
       "      <td>-0.801031</td>\n",
       "      <td>0.456616</td>\n",
       "      <td>0.578240</td>\n",
       "      <td>0.758484</td>\n",
       "      <td>0.733711</td>\n",
       "      <td>-0.815078</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.305365</td>\n",
       "      <td>0.348313</td>\n",
       "      <td>136.987265</td>\n",
       "      <td>2.305365</td>\n",
       "      <td>0.348313</td>\n",
       "      <td>136.987265</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>-0.805086</td>\n",
       "      <td>0.645723</td>\n",
       "      <td>-0.805086</td>\n",
       "      <td>0.462493</td>\n",
       "      <td>0.581448</td>\n",
       "      <td>0.753169</td>\n",
       "      <td>0.729884</td>\n",
       "      <td>-0.822409</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.757589</td>\n",
       "      <td>0.393684</td>\n",
       "      <td>168.012482</td>\n",
       "      <td>2.757589</td>\n",
       "      <td>0.393684</td>\n",
       "      <td>168.012482</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-1.082272</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>-1.082272</td>\n",
       "      <td>0.357289</td>\n",
       "      <td>0.507299</td>\n",
       "      <td>0.550240</td>\n",
       "      <td>0.558968</td>\n",
       "      <td>-1.082466</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>1.465542</td>\n",
       "      <td>0.505817</td>\n",
       "      <td>15.675976</td>\n",
       "      <td>1.465542</td>\n",
       "      <td>0.505817</td>\n",
       "      <td>15.675976</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>-2.341401</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>-2.341401</td>\n",
       "      <td>0.445950</td>\n",
       "      <td>0.552527</td>\n",
       "      <td>0.629957</td>\n",
       "      <td>0.624845</td>\n",
       "      <td>-2.335141</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>25.440251</td>\n",
       "      <td>7.240264</td>\n",
       "      <td>10.780032</td>\n",
       "      <td>25.440251</td>\n",
       "      <td>7.240264</td>\n",
       "      <td>10.780032</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>-2.348156</td>\n",
       "      <td>0.577995</td>\n",
       "      <td>-2.348156</td>\n",
       "      <td>0.456147</td>\n",
       "      <td>0.551428</td>\n",
       "      <td>0.635092</td>\n",
       "      <td>0.627424</td>\n",
       "      <td>-2.352427</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>25.011472</td>\n",
       "      <td>7.205302</td>\n",
       "      <td>10.486506</td>\n",
       "      <td>25.011472</td>\n",
       "      <td>7.205302</td>\n",
       "      <td>10.486506</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  accuracy  log_loss  f1_macro  f1_weighted  \\\n",
       "0  WeightedEnsemble_L2   -0.607523  0.733850 -0.607523  0.639545     0.710914   \n",
       "1             LightGBM   -0.630263  0.734938 -0.630263  0.650482     0.716398   \n",
       "2           LightGBMXT   -0.726364  0.685299 -0.726364  0.582364     0.659501   \n",
       "3      NeuralNetFastAI   -0.731691  0.694546 -0.731691  0.624737     0.684699   \n",
       "4     RandomForestEntr   -0.801031  0.645451 -0.801031  0.456616     0.578240   \n",
       "5     RandomForestGini   -0.805086  0.645723 -0.805086  0.462493     0.581448   \n",
       "6             CatBoost   -1.082272  0.604651 -1.082272  0.357289     0.507299   \n",
       "7       KNeighborsUnif   -2.341401  0.595131 -2.341401  0.445950     0.552527   \n",
       "8       KNeighborsDist   -2.348156  0.577995 -2.348156  0.456147     0.551428   \n",
       "\n",
       "   roc_auc_ovr  roc_auc_ovr_weighted  score_val eval_metric  pred_time_test  \\\n",
       "0     0.850435              0.835072  -0.611240    log_loss        5.570127   \n",
       "1     0.845190              0.829035  -0.643773    log_loss        2.972107   \n",
       "2     0.796201              0.773060  -0.742729    log_loss        2.447841   \n",
       "3     0.808582              0.794917  -0.725978    log_loss        0.287795   \n",
       "4     0.758484              0.733711  -0.815078    log_loss        2.305365   \n",
       "5     0.753169              0.729884  -0.822409    log_loss        2.757589   \n",
       "6     0.550240              0.558968  -1.082466    log_loss        1.465542   \n",
       "7     0.629957              0.624845  -2.335141    log_loss       25.440251   \n",
       "8     0.635092              0.627424  -2.352427    log_loss       25.011472   \n",
       "\n",
       "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       1.374568  241.117007                 0.004860                0.001756   \n",
       "1       0.975409   70.811756                 2.972107                0.975409   \n",
       "2       0.568084   65.796991                 2.447841                0.568084   \n",
       "3       0.049090   33.025633                 0.287795                0.049090   \n",
       "4       0.348313  136.987265                 2.305365                0.348313   \n",
       "5       0.393684  168.012482                 2.757589                0.393684   \n",
       "6       0.505817   15.675976                 1.465542                0.505817   \n",
       "7       7.240264   10.780032                25.440251                7.240264   \n",
       "8       7.205302   10.486506                25.011472                7.205302   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0           0.292353            2       True          9  \n",
       "1          70.811756            1       True          5  \n",
       "2          65.796991            1       True          4  \n",
       "3          33.025633            1       True          3  \n",
       "4         136.987265            1       True          7  \n",
       "5         168.012482            1       True          6  \n",
       "6          15.675976            1       True          8  \n",
       "7          10.780032            1       True          1  \n",
       "8          10.486506            1       True          2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_data[label]\n",
    "y_pred = predictor.predict(test_data)\n",
    "y_proba = predictor.predict_proba(test_data)\n",
    "\n",
    "\n",
    "print(\"AutoGluon Evaluation:\")\n",
    "predictor.leaderboard(test_data, extra_metrics=['accuracy', 'log_loss', 'f1_macro', 'f1_weighted', 'roc_auc_ovr_macro', 'roc_auc_ovr_weighted'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    log_loss,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = '/kaggle/input/feedback-prize-effectiveness/train.csv'\n",
    "TRAIN_ESSAYS_PATH = '/kaggle/input/feedback-prize-effectiveness/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data():\n",
    "    \"\"\"\n",
    "    Creates dummy data files for local testing if they don't exist.\n",
    "    This simulates the Kaggle environment.\n",
    "    \"\"\"\n",
    "    print(\"Creating dummy data for local execution...\")\n",
    "\n",
    "    # Create dummy train.csv\n",
    "    if not os.path.exists(TRAIN_CSV_PATH):\n",
    "        train_data = {\n",
    "            'discourse_id': [f'd{i}' for i in range(100)],\n",
    "            'essay_id': [f'essay_{i % 10}' for i in range(100)],\n",
    "            'discourse_text': [\n",
    "                'This is a sample lead.', 'The author makes a claim.', 'Here is some evidence.',\n",
    "                'This is a concluding statement.', 'Another position is taken here.'\n",
    "            ] * 20,\n",
    "            'discourse_type': ['Lead', 'Claim', 'Evidence', 'Concluding Statement', 'Position'] * 20,\n",
    "            'discourse_effectiveness': ['Adequate', 'Effective', 'Ineffective', 'Adequate', 'Effective'] * 20\n",
    "        }\n",
    "        train_df = pd.DataFrame(train_data)\n",
    "        train_df.to_csv(TRAIN_CSV_PATH, index=False)\n",
    "        print(f\"'{TRAIN_CSV_PATH}' created.\")\n",
    "\n",
    "    # Create dummy essay text files\n",
    "    if not os.path.exists(TRAIN_ESSAYS_PATH):\n",
    "        os.makedirs(TRAIN_ESSAYS_PATH)\n",
    "        for i in range(10):\n",
    "            essay_id = f'essay_{i}'\n",
    "            essay_text = (\n",
    "                f\"This is the full text for essay {essay_id}. \"\n",
    "                \"It contains various discourse elements that are analyzed. \"\n",
    "                \"The student argues a point, provides evidence, and concludes the argument. \"\n",
    "                \"The quality of these elements varies.\"\n",
    "            )\n",
    "            with open(os.path.join(TRAIN_ESSAYS_PATH, f'{essay_id}.txt'), 'w') as f:\n",
    "                f.write(essay_text)\n",
    "        print(f\"'{TRAIN_ESSAYS_PATH}' directory and dummy essays created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_csv_path, essays_path):\n",
    "    \"\"\"\n",
    "    Loads the training data from the CSV and merges it with the full essay texts.\n",
    "\n",
    "    Args:\n",
    "        train_csv_path (str): Path to the train.csv file.\n",
    "        essays_path (str): Path to the directory containing essay .txt files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with the combined data.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # Load the main training data\n",
    "    try:\n",
    "        df = pd.read_csv(train_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Training CSV not found at '{train_csv_path}'.\")\n",
    "        return None\n",
    "\n",
    "    # Load essay texts into a dictionary\n",
    "    essay_texts = {}\n",
    "    for filename in os.listdir(essays_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            essay_id = filename.split('.')[0]\n",
    "            with open(os.path.join(essays_path, filename), 'r') as f:\n",
    "                essay_texts[essay_id] = f.read()\n",
    "\n",
    "    # Map the essay texts to the DataFrame\n",
    "    df['essay_full_text'] = df['essay_id'].map(essay_texts)\n",
    "    print(\"Data loading complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Creates a combined text feature for the model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the new 'full_context_text' feature.\n",
    "    \"\"\"\n",
    "    print(\"Performing feature engineering...\")\n",
    "    # Combine the discourse text, type, and the full essay for context\n",
    "    # Using a separator to give the model a hint about the different parts\n",
    "    df['full_context_text'] = (\n",
    "        df['discourse_type'] + ' [SEP] ' +\n",
    "        df['discourse_text'] + ' [SEP] ' +\n",
    "        df['essay_full_text']\n",
    "    )\n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create dummy files if they don't exist (for local runs)\n",
    "    if not (os.path.exists(TRAIN_CSV_PATH) and os.path.exists(TRAIN_ESSAYS_PATH)):\n",
    "        create_dummy_data()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    train_df = load_data(TRAIN_CSV_PATH, TRAIN_ESSAYS_PATH)\n",
    "\n",
    "    if train_df is not None:\n",
    "        train_df = feature_engineering(train_df)\n",
    "\n",
    "        # Define features (X) and target (y)\n",
    "        X = train_df['full_context_text']\n",
    "        y_raw = train_df['discourse_effectiveness']\n",
    "\n",
    "        # 2. Encode Target Labels\n",
    "        # The labels need to be converted from strings to integers (0, 1, 2)\n",
    "        print(\"Encoding target labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y_raw)\n",
    "        # The mapping will be: Adequate -> 0, Effective -> 1, Ineffective -> 2\n",
    "        # We can see the classes with `label_encoder.classes_`\n",
    "        class_names = label_encoder.classes_\n",
    "        print(f\"Labels encoded. Class mapping: {dict(zip(class_names, range(len(class_names))))}\")\n",
    "\n",
    "\n",
    "        # 3. Split Data into Training and Validation Sets\n",
    "        print(\"Splitting data into training and validation sets (80/20 split)...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y  # Stratify to maintain class distribution in train/test splits\n",
    "        )\n",
    "        print(f\"Training set size: {len(X_train)}\")\n",
    "        print(f\"Validation set size: {len(X_val)}\")\n",
    "\n",
    "        # 4. Define the Model Pipeline\n",
    "        # A pipeline makes it easy to chain preprocessing and modeling steps.\n",
    "        # Step 1: TfidfVectorizer - Converts text to a matrix of TF-IDF features.\n",
    "        # Step 2: LogisticRegression - A simple, effective baseline model for text classification.\n",
    "        print(\"Defining the model pipeline...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                ngram_range=(1, 3), # Use unigrams, bigrams, and trigrams\n",
    "                max_features=10000, # Limit the number of features to the top 10k\n",
    "                stop_words='english'\n",
    "            )),\n",
    "            ('clf', LogisticRegression(\n",
    "                solver='liblinear', # Good for smaller datasets\n",
    "                random_state=42,\n",
    "                C=1.0 # Regularization strength\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        # 5. Train the Model\n",
    "        print(\"Training the model...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        print(\"Model training complete.\")\n",
    "\n",
    "        # 6. Make Predictions on the Validation Set\n",
    "        print(\"Making predictions on the validation set...\")\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_proba = pipeline.predict_proba(X_val)\n",
    "        print(\"Predictions complete.\")\n",
    "\n",
    "        # 7. Evaluate the Model\n",
    "        print(\"\\n--- Model Evaluation Results ---\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        loss = log_loss(y_val, y_pred_proba)\n",
    "        f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y_val, y_pred, average='weighted')\n",
    "        # For multiclass, roc_auc_score needs probabilities and multi_class='ovr'\n",
    "        roc_auc_ovr = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='macro')\n",
    "        roc_auc_ovr_weighted = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "        # Create a DataFrame for a clean display of results\n",
    "        results_df = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'Accuracy',\n",
    "                'Log Loss',\n",
    "                'F1 Score (Macro)',\n",
    "                'F1 Score (Weighted)',\n",
    "                'ROC AUC (OVR Macro)',\n",
    "                'ROC AUC (OVR Weighted)'\n",
    "            ],\n",
    "            'Score': [\n",
    "                accuracy,\n",
    "                loss,\n",
    "                f1_macro,\n",
    "                f1_weighted,\n",
    "                roc_auc_ovr,\n",
    "                roc_auc_ovr_weighted\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        print(results_df.to_string(index=False))\n",
    "        print(\"--------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3712025,
     "sourceId": 35308,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
