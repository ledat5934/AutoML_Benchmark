{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc91721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, preprocessing, and splitting for the\n",
    "    predict_effective_arguments dataset.\n",
    "    \"\"\"\n",
    "    # Determine the project root dynamically\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "\n",
    "    # Define base path for the dataset\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    else:\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # Load dataset metadata (as provided in the problem description)\n",
    "    dataset_metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_effective_arguments\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_effective_arguments\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 29574,\n",
    "          \"n_var\": 5,\n",
    "          \"memory_size\": 1183088,\n",
    "          \"record_size\": 40.00432812605667,\n",
    "          \"n_cells_missing\": 0,\n",
    "          \"p_cells_missing\": 0.0,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 84,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"discourse_id\": {\n",
    "            \"n_distinct\": 29574,\n",
    "            \"p_distinct\": 1.0,\n",
    "            \"is_unique\": True,\n",
    "            \"n_unique\": 29574,\n",
    "            \"p_unique\": 1.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"essay_id\": {\n",
    "            \"n_distinct\": 3352,\n",
    "            \"p_distinct\": 0.1133428011090823,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 59,\n",
    "            \"p_unique\": 0.0019949956042469735,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_text\": {\n",
    "            \"n_distinct\": 29520,\n",
    "            \"p_distinct\": 0.9981740718198417,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 29486,\n",
    "            \"p_unique\": 0.9970244133360384,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 3808,\n",
    "            \"mean_length\": 249.7570839250693,\n",
    "            \"median_length\": 1467,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 108,\n",
    "            \"n_characters\": 7386316,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_type\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.0002366943937242172,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 20,\n",
    "            \"mean_length\": 8.06732264827213,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 23,\n",
    "            \"n_characters\": 238583,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_effectiveness\": {\n",
    "            \"n_distinct\": 3,\n",
    "            \"p_distinct\": 0.00010144045445323595,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 11,\n",
    "            \"mean_length\": 8.780651923987286,\n",
    "            \"median_length\": 8,\n",
    "            \"min_length\": 8,\n",
    "            \"n_characters_distinct\": 14,\n",
    "            \"n_characters\": 259679,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset contains argumentative essays from 6th-12th grade US students, with individual discourse elements (e.g., claims, evidence) annotated by expert raters. The goal is to classify each discourse element's quality as 'effective', 'adequate', or 'ineffective'. The competition has two tracks: one focused on classification accuracy and another on computational efficiency combined with accuracy.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"discourse_effectiveness\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"multi-class logarithmic loss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get file paths from metadata\n",
    "    train_file_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'train')\n",
    "    test_file_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'test')\n",
    "\n",
    "    train_path = BASE_PATH / train_file_info['path']\n",
    "    test_path = BASE_PATH / test_file_info['path']\n",
    "\n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    print(\"\\n--- Original Training Data Info ---\")\n",
    "    train_df.info()\n",
    "    print(\"\\n--- Original Training Data Head ---\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    # Identify column types based on metadata and common sense\n",
    "    target_column = dataset_metadata['task_definition']['target_columns'][0]\n",
    "\n",
    "    # Features are all columns except discourse_id (identifier) and the target\n",
    "    features = [col for col in train_df.columns if col not in ['discourse_id', target_column]]\n",
    "\n",
    "    # Separate features by type\n",
    "    numerical_cols = [] # No explicit numerical columns in this dataset based on metadata\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col in features:\n",
    "        var_info = dataset_metadata['profiling_summary']['variables'].get(col)\n",
    "        if var_info:\n",
    "            if var_info['type'] == 'Text':\n",
    "                if col == 'discourse_text':\n",
    "                    text_cols.append(col)\n",
    "                else: # discourse_type, essay_id are categorical text\n",
    "                    categorical_cols.append(col)\n",
    "            # Add logic for numerical if any were present\n",
    "        else:\n",
    "            # Fallback if column not explicitly in metadata variables (e.g., if added later)\n",
    "            if train_df[col].dtype == 'object':\n",
    "                # Heuristic: if text, check for high cardinality or length\n",
    "                if train_df[col].nunique() > 500 or train_df[col].apply(lambda x: len(str(x))).mean() > 50:\n",
    "                    text_cols.append(col)\n",
    "                else:\n",
    "                    categorical_cols.append(col)\n",
    "            elif pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "                numerical_cols.append(col)\n",
    "\n",
    "    print(f\"\\nIdentified Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Identified Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Identified Text Columns: {text_cols}\")\n",
    "    print(f\"Target Column: {target_column}\")\n",
    "\n",
    "    # Preprocessing Pipelines\n",
    "    preprocessor_steps = []\n",
    "\n",
    "    # Categorical Pipeline\n",
    "    if categorical_cols:\n",
    "        # The error \"Mismatching dimensions along axis 0: {1, 23659}\" was caused by\n",
    "        # `pd.DataFrame.mode` being used as an imputer. `pd.DataFrame.mode` returns a DataFrame,\n",
    "        # which is not compatible with `sklearn.pipeline.Pipeline` steps expecting a transformer.\n",
    "        # The correct way to impute categorical data with the mode is to use `SimpleImputer`\n",
    "        # with `strategy='most_frequent'`.\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), # Corrected: Use SimpleImputer for mode imputation\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "        ])\n",
    "        preprocessor_steps.append(('cat', categorical_transformer, categorical_cols))\n",
    "\n",
    "    # Text Pipeline (TF-IDF)\n",
    "    if text_cols:\n",
    "        # Using TfidfVectorizer for discourse_text\n",
    "        text_transformer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        preprocessor_steps.append(('text_tfidf', text_transformer, text_cols))\n",
    "\n",
    "    # Create the ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=preprocessor_steps,\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Separate target variable\n",
    "    X_train = train_df.drop(columns=[target_column, 'discourse_id'])\n",
    "    y_train = train_df[target_column]\n",
    "    X_test = test_df.drop(columns=['discourse_id'])\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    # The ColumnTransformer will return a sparse matrix if TF-IDF is used.\n",
    "    # For LightGBM, sparse matrices are generally fine.\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    print(f\"\\nShape of processed training data: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of processed test data: {X_test_processed.shape}\")\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete ---\")\n",
    "    print(\"Processed data (X_train_processed, X_test_processed) and target (y_train) are ready.\")\n",
    "    print(\"The preprocessor object is also available for future use (e.g., for new data).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer # Import SimpleImputer for categorical imputation\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "\n",
    "# Define constants for file paths\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "\n",
    "# Option 1: Relative to project root (for local development/testing)\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "# Option 2: Relative to current working directory (for Kaggle or specific environments)\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "\n",
    "# Determine the actual base path\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# Define model and metrics paths\n",
    "MODEL_PATH = Path(\"./models/predict_effective_arguments_model.pkl\").resolve()\n",
    "METRICS_PATH = Path(\"./outputs/metrics.json\").resolve()\n",
    "LABEL_ENCODER_PATH = Path(\"./models/label_encoder.pkl\").resolve() # Path to save LabelEncoder\n",
    "\n",
    "# Ensure output directories exist\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "LABEL_ENCODER_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, preprocessing, splitting, model training,\n",
    "    evaluation, and persistence for the predict_effective_arguments dataset.\n",
    "    \"\"\"\n",
    "    # Load dataset metadata (as provided in the problem description)\n",
    "    dataset_metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_effective_arguments\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_effective_arguments\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 29574,\n",
    "          \"n_var\": 5,\n",
    "          \"memory_size\": 1183088,\n",
    "          \"record_size\": 40.00432812605667,\n",
    "          \"n_cells_missing\": 0,\n",
    "          \"p_cells_missing\": 0.0,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 84,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"discourse_id\": {\n",
    "            \"n_distinct\": 29574,\n",
    "            \"p_distinct\": 1.0,\n",
    "            \"is_unique\": True,\n",
    "            \"n_unique\": 29574,\n",
    "            \"p_unique\": 1.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"essay_id\": {\n",
    "            \"n_distinct\": 3352,\n",
    "            \"p_distinct\": 0.1133428011090823,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 59,\n",
    "            \"p_unique\": 0.0019949956042469735,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_text\": {\n",
    "            \"n_distinct\": 29520,\n",
    "            \"p_distinct\": 0.9981740718198417,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 29486,\n",
    "            \"p_unique\": 0.9970244133360384,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 3808,\n",
    "            \"mean_length\": 249.7570839250693,\n",
    "            \"median_length\": 1467,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 108,\n",
    "            \"n_characters\": 7386316,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_type\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.0002366943937242172,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 20,\n",
    "            \"mean_length\": 8.06732264827213,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 23,\n",
    "            \"n_characters\": 238583,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_effectiveness\": {\n",
    "            \"n_distinct\": 3,\n",
    "            \"p_distinct\": 0.00010144045445323595,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 11,\n",
    "            \"mean_length\": 8.780651923987286,\n",
    "            \"median_length\": 8,\n",
    "            \"min_length\": 8,\n",
    "            \"n_characters_distinct\": 14,\n",
    "            \"n_characters\": 259679,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset contains argumentative essays from 6th-12th grade US students, with individual discourse elements (e.g., claims, evidence) annotated by expert raters. The goal is to classify each discourse element's quality as 'effective', 'adequate', or 'ineffective'. The competition has two tracks: one focused on classification accuracy and another on computational efficiency combined with accuracy.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"discourse_effectiveness\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"multi-class logarithmic loss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Get file paths from metadata\n",
    "    train_file_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'train')\n",
    "    test_file_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'test')\n",
    "\n",
    "    train_path = BASE_PATH / train_file_info['path']\n",
    "    test_path = BASE_PATH / test_file_info['path']\n",
    "\n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    print(\"\\n--- Original Training Data Info ---\")\n",
    "    train_df.info()\n",
    "    print(\"\\n--- Original Training Data Head ---\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    # Identify column types based on metadata and common sense\n",
    "    target_column = dataset_metadata['task_definition']['target_columns'][0]\n",
    "\n",
    "    # Features are all columns except discourse_id (identifier) and the target\n",
    "    features = [col for col in train_df.columns if col not in ['discourse_id', target_column]]\n",
    "\n",
    "    # Separate features by type\n",
    "    numerical_cols = [] # No explicit numerical columns in this dataset based on metadata\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col in features:\n",
    "        var_info = dataset_metadata['profiling_summary']['variables'].get(col)\n",
    "        if var_info:\n",
    "            if var_info['type'] == 'Text':\n",
    "                if col == 'discourse_text':\n",
    "                    text_cols.append(col)\n",
    "                else: # discourse_type, essay_id are categorical text\n",
    "                    categorical_cols.append(col)\n",
    "            # Add logic for numerical if any were present\n",
    "        else:\n",
    "            # Fallback if column not explicitly in metadata variables (e.g., if added later)\n",
    "            if train_df[col].dtype == 'object':\n",
    "                # Heuristic: if text, check for high cardinality or length\n",
    "                if train_df[col].nunique() > 500 or train_df[col].apply(lambda x: len(str(x))).mean() > 50:\n",
    "                    text_cols.append(col)\n",
    "                else:\n",
    "                    categorical_cols.append(col)\n",
    "            elif pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "                numerical_cols.append(col)\n",
    "\n",
    "    print(f\"\\nIdentified Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Identified Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Identified Text Columns: {text_cols}\")\n",
    "    print(f\"Target Column: {target_column}\")\n",
    "\n",
    "    # Preprocessing Pipelines\n",
    "    preprocessor_steps = []\n",
    "\n",
    "    # Categorical Pipeline\n",
    "    if categorical_cols:\n",
    "        # The previous error \"Mismatching dimensions along axis 0: {1, 23659}\"\n",
    "        # was caused by `pd.DataFrame.mode` being used as an imputer.\n",
    "        # `pd.DataFrame.mode` returns a DataFrame, which is not compatible with\n",
    "        # `sklearn.pipeline.Pipeline` steps expecting a transformer.\n",
    "        # The correct way to impute categorical data with the mode is to use `SimpleImputer`\n",
    "        # with `strategy='most_frequent'`.\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), # Corrected: Use SimpleImputer for mode imputation\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "        ])\n",
    "        preprocessor_steps.append(('cat', categorical_transformer, categorical_cols))\n",
    "\n",
    "    # Text Pipeline (TF-IDF)\n",
    "    if text_cols:\n",
    "        # Using TfidfVectorizer for discourse_text\n",
    "        text_transformer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features to manage dimensionality\n",
    "        preprocessor_steps.append(('text_tfidf', text_transformer, text_cols))\n",
    "\n",
    "    # Create the ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=preprocessor_steps,\n",
    "        remainder='passthrough' # Keep other columns (like discourse_id if not dropped)\n",
    "    )\n",
    "\n",
    "    # Separate target variable\n",
    "    X = train_df.drop(columns=[target_column, 'discourse_id']) # discourse_id is an identifier, not a feature\n",
    "    y = train_df[target_column]\n",
    "\n",
    "    # Encode target variable for LightGBM (requires integer labels for multi-class)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Target classes: {label_encoder.classes_}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    # Save the label encoder for consistent inverse transformation in Stage 3\n",
    "    joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "    print(f\"LabelEncoder saved to {LABEL_ENCODER_PATH}\")\n",
    "\n",
    "    # 80/20 stratified split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    print(f\"\\nShape of X_train before preprocessing: {X_train.shape}\")\n",
    "    print(f\"Shape of X_val before preprocessing: {X_val.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    print(f\"Shape of y_val: {y_val.shape}\")\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    # The ColumnTransformer will return a sparse matrix if TF-IDF is used.\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "    print(f\"\\nShape of processed training data: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of processed validation data: {X_val_processed.shape}\")\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete ---\")\n",
    "\n",
    "    # Build and train the model (LightGBM for multi-class classification)\n",
    "    print(\"\\n--- Training LightGBM Model ---\")\n",
    "\n",
    "    # LightGBM parameters for multi-class classification\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': num_classes,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 2000, # Large number, will use early stopping\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1, # Suppress verbose output\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    model.fit(X_train_processed, y_train,\n",
    "              eval_set=[(X_val_processed, y_val)],\n",
    "              eval_metric='multi_logloss',\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)]) # Early stopping after 100 rounds without improvement\n",
    "\n",
    "    print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    print(\"\\n--- Evaluating Model ---\")\n",
    "    y_pred_proba = model.predict_proba(X_val_processed)\n",
    "    y_pred = model.predict(X_val_processed)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_val, y_pred, average='macro') # Use macro for imbalanced classes\n",
    "    metrics['logloss'] = log_loss(y_val, y_pred_proba)\n",
    "\n",
    "    # ROC AUC for multi-class: 'ovr' (One-vs-Rest) or 'ovo' (One-vs-One)\n",
    "    # 'ovr' is generally more common and interpretable for multi-class.\n",
    "    # Requires probabilities.\n",
    "    metrics['roc_auc_ovr'] = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "    print(f\"Validation Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Validation LogLoss: {metrics['logloss']:.4f}\")\n",
    "    print(f\"Validation ROC AUC (OvR): {metrics['roc_auc_ovr']:.4f}\")\n",
    "\n",
    "    # Persist metrics to JSON file\n",
    "    with open(METRICS_PATH, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"\\nMetrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    # Persist the trained model\n",
    "    # Create a pipeline that includes preprocessing and the model for easier deployment\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('classifier', model)])\n",
    "\n",
    "    joblib.dump(full_pipeline, MODEL_PATH)\n",
    "    print(f\"Trained model saved to {MODEL_PATH}\")\n",
    "\n",
    "    return full_pipeline, label_encoder.classes_ # Return trained pipeline and label encoder classes\n",
    "\n",
    "def main():\n",
    "    trained_model_pipeline, label_encoder_classes = train_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
    "\n",
    "# Define constants for file paths\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "\n",
    "# Option 1: Relative to project root (for local development/testing)\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "# Option 2: Relative to current working directory (for Kaggle or specific environments)\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_effective_arguments').resolve()\n",
    "\n",
    "# Determine the actual base path\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# Define model and submission paths\n",
    "MODEL_PATH = Path(\"./models/predict_effective_arguments_model.pkl\").resolve()\n",
    "SUBMISSION_PATH = Path(\"./outputs/submission.csv\").resolve()\n",
    "LABEL_ENCODER_PATH = Path(\"./models/label_encoder.pkl\").resolve() # Path to load LabelEncoder\n",
    "\n",
    "# Ensure output directories exist\n",
    "SUBMISSION_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def generate_predictions(trained_model=None):\n",
    "    \"\"\"\n",
    "    Loads the test data, preprocesses it, generates predictions using the trained model,\n",
    "    and saves the predictions in the specified submission format.\n",
    "\n",
    "    Args:\n",
    "        trained_model: An optional pre-trained model pipeline. If None, the model\n",
    "                       will be loaded from MODEL_PATH.\n",
    "    \"\"\"\n",
    "    # Load dataset metadata (as provided in the problem description)\n",
    "    dataset_metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_effective_arguments\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_effective_arguments\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 29574,\n",
    "          \"n_var\": 5,\n",
    "          \"memory_size\": 1183088,\n",
    "          \"record_size\": 40.00432812605667,\n",
    "          \"n_cells_missing\": 0,\n",
    "          \"p_cells_missing\": 0.0,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 84,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"discourse_id\": {\n",
    "            \"n_distinct\": 29574,\n",
    "            \"p_distinct\": 1.0,\n",
    "            \"is_unique\": True,\n",
    "            \"n_unique\": 29574,\n",
    "            \"p_unique\": 1.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"essay_id\": {\n",
    "            \"n_distinct\": 3352,\n",
    "            \"p_distinct\": 0.1133428011090823,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 59,\n",
    "            \"p_unique\": 0.0019949956042469735,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 12,\n",
    "            \"mean_length\": 12.0,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 12,\n",
    "            \"n_characters_distinct\": 16,\n",
    "            \"n_characters\": 354888,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_text\": {\n",
    "            \"n_distinct\": 29520,\n",
    "            \"p_distinct\": 0.9981740718198417,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 29486,\n",
    "            \"p_unique\": 0.9970244133360384,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 3808,\n",
    "            \"mean_length\": 249.7570839250693,\n",
    "            \"median_length\": 1467,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 108,\n",
    "            \"n_characters\": 7386316,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_type\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.0002366943937242172,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 20,\n",
    "            \"mean_length\": 8.06732264827213,\n",
    "            \"median_length\": 12,\n",
    "            \"min_length\": 4,\n",
    "            \"n_characters_distinct\": 23,\n",
    "            \"n_characters\": 238583,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"discourse_effectiveness\": {\n",
    "            \"n_distinct\": 3,\n",
    "            \"p_distinct\": 0.00010144045445323595,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 29574,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 29574,\n",
    "            \"memory_size\": 236720,\n",
    "            \"max_length\": 11,\n",
    "            \"mean_length\": 8.780651923987286,\n",
    "            \"median_length\": 8,\n",
    "            \"min_length\": 8,\n",
    "            \"n_characters_distinct\": 14,\n",
    "            \"n_characters\": 259679,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset contains argumentative essays from 6th-12th grade US students, with individual discourse elements (e.g., claims, evidence) annotated by expert raters. The goal is to classify each discourse element's quality as 'effective', 'adequate', or 'ineffective'. The competition has two tracks: one focused on classification accuracy and another on computational efficiency combined with accuracy.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"discourse_effectiveness\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"multi-class logarithmic loss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Load the trained model if not provided\n",
    "    if trained_model is None:\n",
    "        print(f\"Loading trained model from {MODEL_PATH}\")\n",
    "        try:\n",
    "            trained_model = joblib.load(MODEL_PATH)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Please ensure the training script has been run.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Using provided trained model.\")\n",
    "\n",
    "    # Load the LabelEncoder used during training\n",
    "    print(f\"Loading LabelEncoder from {LABEL_ENCODER_PATH}\")\n",
    "    try:\n",
    "        label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: LabelEncoder file not found at {LABEL_ENCODER_PATH}. Please ensure the training script has been run and saved the LabelEncoder.\")\n",
    "        return\n",
    "\n",
    "    # Get file paths from metadata\n",
    "    test_file_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'test')\n",
    "    sample_submission_info = next(f for f in dataset_metadata['dataset_info']['files'] if f['role'] == 'sample')\n",
    "\n",
    "    test_path = BASE_PATH / test_file_info['path']\n",
    "    sample_submission_path = BASE_PATH / sample_submission_info['path']\n",
    "\n",
    "    # Load test data and sample submission\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    sample_submission_df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "    print(\"\\n--- Test Data Info ---\")\n",
    "    test_df.info()\n",
    "    print(\"\\n--- Test Data Head ---\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    # Extract discourse_id for submission\n",
    "    discourse_ids = test_df['discourse_id']\n",
    "\n",
    "    # Make predictions\n",
    "    # The full_pipeline from the training script handles preprocessing internally.\n",
    "    # We need to pass the raw test_df (excluding 'discourse_id' as it's not a feature).\n",
    "    X_test_raw = test_df.drop(columns=['discourse_id'])\n",
    "\n",
    "    print(\"\\n--- Generating Predictions ---\")\n",
    "\n",
    "    # Predict probabilities for multi-class classification\n",
    "    predictions_proba = trained_model.predict_proba(X_test_raw)\n",
    "\n",
    "    # Get the original class names from the loaded LabelEncoder\n",
    "    target_classes = label_encoder.classes_\n",
    "    print(f\"Target classes from LabelEncoder: {target_classes}\")\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({'discourse_id': discourse_ids})\n",
    "\n",
    "    # Get the column names for probabilities from the sample submission, excluding 'discourse_id'\n",
    "    proba_cols = [col for col in sample_submission_df.columns if col != 'discourse_id']\n",
    "\n",
    "    # Create a mapping from class name to its index in the LabelEncoder's classes\n",
    "    # This ensures that probabilities are assigned to the correct columns even if\n",
    "    # the order of classes in LabelEncoder is different from the sample submission.\n",
    "    class_to_index = {class_name: i for i, class_name in enumerate(target_classes)}\n",
    "\n",
    "    # Populate the submission_df with probabilities, ensuring correct column mapping\n",
    "    for col_name in proba_cols:\n",
    "        if col_name in class_to_index:\n",
    "            idx = class_to_index[col_name]\n",
    "            submission_df[col_name] = predictions_proba[:, idx]\n",
    "        else:\n",
    "            # This case should ideally not happen if target_classes cover all sample submission columns\n",
    "            # and the sample submission columns are valid target classes.\n",
    "            print(f\"Warning: Column '{col_name}' from sample submission not found in trained model's classes. Setting to 0.0.\")\n",
    "            submission_df[col_name] = 0.0 # Default to 0 if class not found\n",
    "\n",
    "    # Ensure the order of columns matches the sample submission\n",
    "    # This is crucial for Kaggle submissions.\n",
    "    submission_df = submission_df[sample_submission_df.columns]\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(f\"\\nPredictions saved to {SUBMISSION_PATH}\")\n",
    "    print(\"\\n--- Submission File Head ---\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the prediction function.\n",
    "    # If you ran the training script in the same session and have `trained_model_pipeline`\n",
    "    # and `label_encoder_classes` available, you could pass them.\n",
    "    # However, for a standalone Stage 3 script, it's better to load them from disk.\n",
    "    generate_predictions()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
