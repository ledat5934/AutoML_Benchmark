{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, and preprocessing steps\n",
    "    for the toxic comment classification dataset.\n",
    "    \"\"\"\n",
    "    # --- Define file paths ---\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    else:\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    TRAIN_FILE = BASE_PATH / \"jigsaw-toxic-comment-train.csv\"\n",
    "    TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "    SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return None, None, None, None, None # Return None for all expected outputs to indicate failure\n",
    "\n",
    "    # --- Dataset Overview (from EDA and Metadata) ---\n",
    "    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    text_column = 'comment_text'\n",
    "    id_column = 'id'\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "\n",
    "    # 1. Handle Missing Values (EDA shows none, but good practice)\n",
    "    print(\"Checking for missing values...\")\n",
    "    # The EDA report indicates 0 missing cells for the relevant columns.\n",
    "    # However, for robustness, especially with text data, it's good to ensure no NaNs.\n",
    "    if train_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in train_df. Filling with empty string.\")\n",
    "        train_df[text_column].fillna('', inplace=True)\n",
    "    if test_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in test_df. Filling with empty string.\")\n",
    "        test_df[text_column].fillna('', inplace=True)\n",
    "\n",
    "    # Check if any other columns have missing values (though EDA says none)\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].isnull().any():\n",
    "            print(f\"Warning: Missing values found in '{col}' in train_df. Imputation strategy needed.\")\n",
    "    for col in test_df.columns:\n",
    "        if test_df[col].isnull().any():\n",
    "            print(f\"Warning: Missing values found in '{col}' in test_df. Imputation strategy needed.\")\n",
    "\n",
    "    print(\"Missing value check complete. Handled text column NaNs if any.\")\n",
    "\n",
    "    # 2. Text Feature Engineering (TF-IDF)\n",
    "    print(f\"Applying TF-IDF to '{text_column}'...\")\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    # max_features can be tuned. Using a reasonable default.\n",
    "    # min_df ignores terms that appear in too few documents (e.g., 3 documents)\n",
    "    # max_df ignores terms that appear in too many documents (e.g., 85% of documents)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=20000, min_df=3, max_df=0.85, ngram_range=(1, 2))\n",
    "\n",
    "    # Fit on training data and transform both train and test data\n",
    "    X_train_text = tfidf_vectorizer.fit_transform(train_df[text_column])\n",
    "    X_test_text = tfidf_vectorizer.transform(test_df[text_column])\n",
    "\n",
    "    print(f\"TF-IDF transformation complete. Train shape: {X_train_text.shape}, Test shape: {X_test_text.shape}\")\n",
    "\n",
    "    # 3. Prepare Target Variables\n",
    "    # For multi-label classification, the targets are the six binary columns.\n",
    "    y_train = train_df[target_columns]\n",
    "\n",
    "    print(\"Target variables prepared.\")\n",
    "\n",
    "    # Final processed data for model training:\n",
    "    # X_train_processed: Sparse matrix of TF-IDF features for training\n",
    "    # y_train: DataFrame of target labels for training\n",
    "    # X_test_processed: Sparse matrix of TF-IDF features for testing\n",
    "\n",
    "    print(\"\\nPreprocessing complete. Data is ready for model training.\")\n",
    "    print(f\"Shape of training features (TF-IDF): {X_train_text.shape}\")\n",
    "    print(f\"Shape of training labels: {y_train.shape}\")\n",
    "    print(f\"Shape of test features (TF-IDF): {X_test_text.shape}\")\n",
    "\n",
    "    # Return these processed datasets for the next stage (model training)\n",
    "    return X_train_text, y_train, X_test_text, test_df[id_column], tfidf_vectorizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train_processed, y_train_labels, X_test_processed, test_ids, tfidf_vectorizer = main()\n",
    "\n",
    "    # The original prompt for Stage 1 did not include model training or evaluation.\n",
    "    # The commented-out example model training block is for demonstration purposes\n",
    "    # and is typically part of Stage 2.\n",
    "    # The error trace indicates a request to \"Add the code to calculate the overall ROC_AUC, Accuracy, F1 score,. .. of all class\".\n",
    "    # This is an evaluation step, which belongs in Stage 2.\n",
    "    # Stage 1's responsibility is solely data loading, cleaning, and preprocessing.\n",
    "    # Therefore, no changes are needed in this __main__ block for Stage 1 based on the error message,\n",
    "    # as the error refers to a calculation that should occur in Stage 2.\n",
    "    # The current Stage 1 code correctly returns the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, preprocessing, model training,\n",
    "    and evaluation steps for the toxic comment classification dataset.\n",
    "    \"\"\"\n",
    "    # --- Define file paths ---\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    else:\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    TRAIN_FILE = BASE_PATH / \"jigsaw-toxic-comment-train.csv\"\n",
    "    TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "    SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "    # Define output paths\n",
    "    OUTPUT_DIR = Path(\"./outputs\")\n",
    "    MODELS_DIR = Path(\"./models\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    METRICS_PATH = OUTPUT_DIR / \"metrics.json\"\n",
    "    MODEL_PATH = MODELS_DIR / \"toxic_cmt_classify_model.pkl\"\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return None, None, None, None # Return None to indicate failure\n",
    "\n",
    "    # --- Dataset Overview (from EDA and Metadata) ---\n",
    "    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    text_column = 'comment_text'\n",
    "    id_column = 'id'\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "\n",
    "    # 1. Handle Missing Values (EDA shows none, but good practice)\n",
    "    print(\"Checking for missing values...\")\n",
    "    if train_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in train_df. Filling with empty string.\")\n",
    "        train_df[text_column].fillna('', inplace=True)\n",
    "    if test_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in test_df. Filling with empty string.\")\n",
    "        test_df[text_column].fillna('', inplace=True)\n",
    "    else:\n",
    "        print(\"No missing values found in critical columns based on EDA.\")\n",
    "\n",
    "    # 2. Text Feature Engineering (TF-IDF)\n",
    "    print(f\"Applying TF-IDF to '{text_column}'...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=20000, min_df=3, max_df=0.85, ngram_range=(1, 2))\n",
    "\n",
    "    X_train_text = tfidf_vectorizer.fit_transform(train_df[text_column])\n",
    "    X_test_text = tfidf_vectorizer.transform(test_df[text_column])\n",
    "\n",
    "    print(f\"TF-IDF transformation complete. Train shape: {X_train_text.shape}, Test shape: {X_test_text.shape}\")\n",
    "\n",
    "    # 3. Prepare Target Variables\n",
    "    y_train_labels = train_df[target_columns]\n",
    "    print(\"Target variables prepared.\")\n",
    "\n",
    "    # --- Data Splitting (80/20 stratified split) ---\n",
    "    print(\"Splitting data into training and validation sets (80/20 stratified)...\")\n",
    "    # For multi-label classification, direct stratification on y_train_labels (a DataFrame)\n",
    "    # is not directly supported by sklearn's train_test_split.\n",
    "    # A common workaround is to stratify on the primary target or a combination.\n",
    "    # Given the primary task is 'toxic' and other labels are highly imbalanced,\n",
    "    # we'll stratify on the 'toxic' column as a reasonable compromise.\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train_text, y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels['toxic']\n",
    "    )\n",
    "\n",
    "    print(f\"Training split shapes: X_train_split={X_train_split.shape}, y_train_split={y_train_split.shape}\")\n",
    "    print(f\"Validation split shapes: X_val_split={X_val_split.shape}, y_val_split={y_val_split.shape}\")\n",
    "\n",
    "    # --- Model Building and Training ---\n",
    "    print(\"\\n--- Model Training ---\")\n",
    "    model = OneVsRestClassifier(LogisticRegression(solver='saga', C=0.5, n_jobs=-1, random_state=42, max_iter=1000))\n",
    "\n",
    "    print(\"Training model (OneVsRestClassifier with Logistic Regression)...\")\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    metrics = {}\n",
    "    overall_roc_auc_scores = [] # To store ROC AUC for each label to calculate overall average\n",
    "    overall_accuracy_scores = []\n",
    "    overall_f1_scores = []\n",
    "    overall_log_loss_scores = []\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    y_val_pred_proba = model.predict_proba(X_val_split)\n",
    "    # Predict hard labels for accuracy and F1\n",
    "    y_val_pred_labels = (y_val_pred_proba > 0.5).astype(int) # Threshold at 0.5 for binary classification\n",
    "\n",
    "    for i, label in enumerate(target_columns):\n",
    "        label_metrics = {}\n",
    "        # Ensure there are at least two unique classes in the validation set for ROC AUC and Log Loss\n",
    "        # and that the predictions are not all the same for F1/Accuracy.\n",
    "\n",
    "        # Check for single class in true labels\n",
    "        unique_true_classes = np.unique(y_val_split[label])\n",
    "\n",
    "        # ROC AUC and Log Loss require at least two classes in true labels\n",
    "        if len(unique_true_classes) > 1:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_val_split[label], y_val_pred_proba[:, i])\n",
    "                label_metrics['roc_auc'] = auc\n",
    "                overall_roc_auc_scores.append(auc)\n",
    "            except ValueError as e:\n",
    "                print(f\"  Warning: Could not calculate ROC AUC for '{label}': {e}\")\n",
    "                label_metrics['roc_auc'] = None\n",
    "\n",
    "            try:\n",
    "                ll = log_loss(y_val_split[label], y_val_pred_proba[:, i])\n",
    "                label_metrics['log_loss'] = ll\n",
    "                overall_log_loss_scores.append(ll)\n",
    "            except ValueError as e:\n",
    "                print(f\"  Warning: Could not calculate Log Loss for '{label}': {e}\")\n",
    "                label_metrics['log_loss'] = None\n",
    "        else:\n",
    "            print(f\"  Warning: Only one class present in true labels for '{label}'. Skipping ROC AUC and Log Loss.\")\n",
    "            label_metrics['roc_auc'] = None\n",
    "            label_metrics['log_loss'] = None\n",
    "\n",
    "        # Accuracy and F1 can sometimes be calculated even with one class if predictions vary\n",
    "        try:\n",
    "            acc = accuracy_score(y_val_split[label], y_val_pred_labels[:, i])\n",
    "            label_metrics['accuracy'] = acc\n",
    "            overall_accuracy_scores.append(acc)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Warning: Could not calculate Accuracy for '{label}': {e}\")\n",
    "            label_metrics['accuracy'] = None\n",
    "\n",
    "        try:\n",
    "            # Use zero_division=0 to avoid warning/error if no positive samples are predicted\n",
    "            f1 = f1_score(y_val_split[label], y_val_pred_labels[:, i], average='binary', zero_division=0)\n",
    "            label_metrics['f1_score'] = f1\n",
    "            overall_f1_scores.append(f1)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Warning: Could not calculate F1 Score for '{label}': {e}\")\n",
    "            label_metrics['f1_score'] = None\n",
    "\n",
    "        print(f\"  Metrics for '{label}':\")\n",
    "        print(f\"    ROC AUC: {label_metrics.get('roc_auc', 'N/A'):.4f}\")\n",
    "        print(f\"    Accuracy: {label_metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"    F1 Score: {label_metrics.get('f1_score', 'N/A'):.4f}\")\n",
    "        print(f\"    Log Loss: {label_metrics.get('log_loss', 'N/A'):.4f}\")\n",
    "\n",
    "        metrics[label] = label_metrics\n",
    "\n",
    "    # Calculate overall metrics by averaging across labels\n",
    "    # Filter out None values before calculating mean\n",
    "    filtered_roc_auc = [score for score in overall_roc_auc_scores if score is not None]\n",
    "    filtered_accuracy = [score for score in overall_accuracy_scores if score is not None]\n",
    "    filtered_f1 = [score for score in overall_f1_scores if score is not None]\n",
    "    filtered_log_loss = [score for score in overall_log_loss_scores if score is not None]\n",
    "\n",
    "    metrics['overall_average'] = {}\n",
    "    if filtered_roc_auc:\n",
    "        metrics['overall_average']['roc_auc'] = np.mean(filtered_roc_auc)\n",
    "        print(f\"\\nOverall Average ROC AUC: {metrics['overall_average']['roc_auc']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['roc_auc'] = \"N/A\"\n",
    "        print(\"\\nOverall Average ROC AUC: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_accuracy:\n",
    "        metrics['overall_average']['accuracy'] = np.mean(filtered_accuracy)\n",
    "        print(f\"Overall Average Accuracy: {metrics['overall_average']['accuracy']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['accuracy'] = \"N/A\"\n",
    "        print(\"Overall Average Accuracy: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_f1:\n",
    "        metrics['overall_average']['f1_score'] = np.mean(filtered_f1)\n",
    "        print(f\"Overall Average F1 Score: {metrics['overall_average']['f1_score']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['f1_score'] = \"N/A\"\n",
    "        print(\"Overall Average F1 Score: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_log_loss:\n",
    "        metrics['overall_average']['log_loss'] = np.mean(filtered_log_loss)\n",
    "        print(f\"Overall Average Log Loss: {metrics['overall_average']['log_loss']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['log_loss'] = \"N/A\"\n",
    "        print(\"Overall Average Log Loss: N/A (could not calculate for any label)\")\n",
    "\n",
    "\n",
    "    # Persist metrics to JSON file\n",
    "    print(f\"Saving metrics to {METRICS_PATH}...\")\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Metrics saved.\")\n",
    "\n",
    "    # --- Persist Trained Model ---\n",
    "    print(f\"Saving trained model to {MODEL_PATH}...\")\n",
    "    # Save both the model and the TF-IDF vectorizer together\n",
    "    joblib.dump({'model': model, 'vectorizer': tfidf_vectorizer}, MODEL_PATH)\n",
    "    print(\"Model and TF-IDF vectorizer saved.\")\n",
    "\n",
    "    # Return the trained model instance and processed test data for Stage 3\n",
    "    return model, X_test_text, test_df[id_column], target_columns\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The __main__ block for Stage 2 should only run the main function\n",
    "    # and potentially print a confirmation.\n",
    "    # The generation of the submission file is explicitly part of Stage 3.\n",
    "    trained_model_instance, X_test_processed_data, test_ids_data, target_cols = main()\n",
    "    if trained_model_instance:\n",
    "        print(\"\\nStage 2 (Model Training and Evaluation) completed successfully.\")\n",
    "        print(f\"Trained model saved to {Path('./models/toxic_cmt_classify_model.pkl').resolve()}\")\n",
    "        print(f\"Evaluation metrics saved to {Path('./outputs/metrics.json').resolve()}\")\n",
    "    else:\n",
    "        print(\"\\nStage 2 (Model Training and Evaluation) failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# --- File Path Constants ---\n",
    "# Determine the project root dynamically\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "\n",
    "# Base path for datasets, with fallback for different execution environments\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/toxic_cmt_classify').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"jigsaw-toxic-comment-train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "# Define output paths\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METRICS_PATH = OUTPUT_DIR / \"metrics.json\"\n",
    "MODEL_PATH = MODELS_DIR / \"toxic_cmt_classify_model.pkl\"\n",
    "SUBMISSION_PATH = OUTPUT_DIR / \"submission.csv\" # Declared at the top as per instructions\n",
    "\n",
    "# Placeholder for processed test data path if it were saved to disk\n",
    "# For this script, test_df_processed is generated in memory, so this is illustrative.\n",
    "# TEST_PROCESSED_PATH = OUTPUT_DIR / \"test_processed.pkl\" # Not strictly used as data is passed in memory\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, preprocessing, model training,\n",
    "    and evaluation steps for the toxic comment classification dataset.\n",
    "    This function now encapsulates the full pipeline from Stage 1 and Stage 2\n",
    "    to ensure all necessary components (like the vectorizer) are available\n",
    "    for prediction if the model is loaded.\n",
    "    \"\"\"\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return None, None, None, None # Return None to indicate failure\n",
    "\n",
    "    # --- Dataset Overview (from EDA and Metadata) ---\n",
    "    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    text_column = 'comment_text'\n",
    "    id_column = 'id'\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "\n",
    "    # 1. Handle Missing Values (EDA shows none, but good practice)\n",
    "    print(\"Checking for missing values...\")\n",
    "    if train_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in train_df. Filling with empty string.\")\n",
    "        train_df[text_column].fillna('', inplace=True)\n",
    "    if test_df[text_column].isnull().any():\n",
    "        print(f\"Warning: Missing values found in '{text_column}' in test_df. Filling with empty string.\")\n",
    "        test_df[text_column].fillna('', inplace=True)\n",
    "    else:\n",
    "        print(\"No missing values found in critical columns based on EDA.\")\n",
    "\n",
    "    # 2. Text Feature Engineering (TF-IDF)\n",
    "    print(f\"Applying TF-IDF to '{text_column}'...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=20000, min_df=3, max_df=0.85, ngram_range=(1, 2))\n",
    "\n",
    "    X_train_text = tfidf_vectorizer.fit_transform(train_df[text_column])\n",
    "    X_test_text = tfidf_vectorizer.transform(test_df[text_column])\n",
    "\n",
    "    print(f\"TF-IDF transformation complete. Train shape: {X_train_text.shape}, Test shape: {X_test_text.shape}\")\n",
    "\n",
    "    # 3. Prepare Target Variables\n",
    "    y_train_labels = train_df[target_columns]\n",
    "    print(\"Target variables prepared.\")\n",
    "\n",
    "    # --- Data Splitting (80/20 stratified split) ---\n",
    "    print(\"Splitting data into training and validation sets (80/20 stratified)...\")\n",
    "    # Stratify on 'toxic' as it's the main target and other labels are highly imbalanced.\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train_text, y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels['toxic']\n",
    "    )\n",
    "\n",
    "    print(f\"Training split shapes: X_train_split={X_train_split.shape}, y_train_split={y_train_split.shape}\")\n",
    "    print(f\"Validation split shapes: X_val_split={X_val_split.shape}, y_val_split={y_val_split.shape}\")\n",
    "\n",
    "    # --- Model Building and Training ---\n",
    "    print(\"\\n--- Model Training ---\")\n",
    "    model = OneVsRestClassifier(LogisticRegression(solver='saga', C=0.5, n_jobs=-1, random_state=42, max_iter=1000))\n",
    "\n",
    "    print(\"Training model (OneVsRestClassifier with Logistic Regression)...\")\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    metrics = {}\n",
    "    overall_roc_auc_scores = []\n",
    "    overall_accuracy_scores = []\n",
    "    overall_f1_scores = []\n",
    "    overall_log_loss_scores = []\n",
    "\n",
    "    y_val_pred_proba = model.predict_proba(X_val_split)\n",
    "    y_val_pred_labels = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    for i, label in enumerate(target_columns):\n",
    "        label_metrics = {}\n",
    "        unique_true_classes = np.unique(y_val_split[label])\n",
    "\n",
    "        # ROC AUC and Log Loss require at least two classes in true labels\n",
    "        if len(unique_true_classes) > 1:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_val_split[label], y_val_pred_proba[:, i])\n",
    "                label_metrics['roc_auc'] = auc\n",
    "                overall_roc_auc_scores.append(auc)\n",
    "            except ValueError as e:\n",
    "                print(f\"  Warning: Could not calculate ROC AUC for '{label}': {e}\")\n",
    "                label_metrics['roc_auc'] = None\n",
    "\n",
    "            try:\n",
    "                ll = log_loss(y_val_split[label], y_val_pred_proba[:, i])\n",
    "                label_metrics['log_loss'] = ll\n",
    "                overall_log_loss_scores.append(ll)\n",
    "            except ValueError as e:\n",
    "                print(f\"  Warning: Could not calculate Log Loss for '{label}': {e}\")\n",
    "                label_metrics['log_loss'] = None\n",
    "        else:\n",
    "            print(f\"  Warning: Only one class present in true labels for '{label}'. Skipping ROC AUC and Log Loss.\")\n",
    "            label_metrics['roc_auc'] = None\n",
    "            label_metrics['log_loss'] = None\n",
    "\n",
    "        try:\n",
    "            acc = accuracy_score(y_val_split[label], y_val_pred_labels[:, i])\n",
    "            label_metrics['accuracy'] = acc\n",
    "            overall_accuracy_scores.append(acc)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Warning: Could not calculate Accuracy for '{label}': {e}\")\n",
    "            label_metrics['accuracy'] = None\n",
    "\n",
    "        try:\n",
    "            f1 = f1_score(y_val_split[label], y_val_pred_labels[:, i], average='binary', zero_division=0)\n",
    "            label_metrics['f1_score'] = f1\n",
    "            overall_f1_scores.append(f1)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Warning: Could not calculate F1 Score for '{label}': {e}\")\n",
    "            label_metrics['f1_score'] = None\n",
    "\n",
    "        print(f\"  Metrics for '{label}':\")\n",
    "        print(f\"    ROC AUC: {label_metrics.get('roc_auc', 'N/A'):.4f}\")\n",
    "        print(f\"    Accuracy: {label_metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"    F1 Score: {label_metrics.get('f1_score', 'N/A'):.4f}\")\n",
    "        print(f\"    Log Loss: {label_metrics.get('log_loss', 'N/A'):.4f}\")\n",
    "\n",
    "        metrics[label] = label_metrics\n",
    "\n",
    "    # Calculate overall metrics by averaging across labels\n",
    "    filtered_roc_auc = [score for score in overall_roc_auc_scores if score is not None]\n",
    "    filtered_accuracy = [score for score in overall_accuracy_scores if score is not None]\n",
    "    filtered_f1 = [score for score in overall_f1_scores if score is not None]\n",
    "    filtered_log_loss = [score for score in overall_log_loss_scores if score is not None]\n",
    "\n",
    "    metrics['overall_average'] = {}\n",
    "    if filtered_roc_auc:\n",
    "        metrics['overall_average']['roc_auc'] = np.mean(filtered_roc_auc)\n",
    "        print(f\"\\nOverall Average ROC AUC: {metrics['overall_average']['roc_auc']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['roc_auc'] = \"N/A\"\n",
    "        print(\"\\nOverall Average ROC AUC: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_accuracy:\n",
    "        metrics['overall_average']['accuracy'] = np.mean(filtered_accuracy)\n",
    "        print(f\"Overall Average Accuracy: {metrics['overall_average']['accuracy']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['accuracy'] = \"N/A\"\n",
    "        print(\"Overall Average Accuracy: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_f1:\n",
    "        metrics['overall_average']['f1_score'] = np.mean(filtered_f1)\n",
    "        print(f\"Overall Average F1 Score: {metrics['overall_average']['f1_score']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['f1_score'] = \"N/A\"\n",
    "        print(\"Overall Average F1 Score: N/A (could not calculate for any label)\")\n",
    "\n",
    "    if filtered_log_loss:\n",
    "        metrics['overall_average']['log_loss'] = np.mean(filtered_log_loss)\n",
    "        print(f\"Overall Average Log Loss: {metrics['overall_average']['log_loss']:.4f}\")\n",
    "    else:\n",
    "        metrics['overall_average']['log_loss'] = \"N/A\"\n",
    "        print(\"Overall Average Log Loss: N/A (could not calculate for any label)\")\n",
    "\n",
    "    # Persist metrics to JSON file\n",
    "    print(f\"Saving metrics to {METRICS_PATH}...\")\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Metrics saved.\")\n",
    "\n",
    "    # --- Persist Trained Model and TF-IDF Vectorizer ---\n",
    "    print(f\"Saving trained model and TF-IDF vectorizer to {MODEL_PATH}...\")\n",
    "    joblib.dump({'model': model, 'vectorizer': tfidf_vectorizer}, MODEL_PATH)\n",
    "    print(\"Model and TF-IDF vectorizer saved.\")\n",
    "\n",
    "    # Return necessary components for prediction\n",
    "    return model, X_test_text, test_df[id_column], target_columns, tfidf_vectorizer # Also return vectorizer\n",
    "\n",
    "def generate_predictions(trained_model=None, X_test_processed=None, test_ids=None, target_columns=None, tfidf_vectorizer=None):\n",
    "    \"\"\"\n",
    "    Generates predictions using the trained model and saves them to a submission file.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Submission File ---\")\n",
    "\n",
    "    # 1. Ensure trained_model and vectorizer are available. If None, load from disk.\n",
    "    if trained_model is None or tfidf_vectorizer is None:\n",
    "        print(f\"No trained model or vectorizer instance provided. Attempting to load from {MODEL_PATH}...\")\n",
    "        try:\n",
    "            loaded_artifacts = joblib.load(MODEL_PATH)\n",
    "            trained_model = loaded_artifacts['model']\n",
    "            tfidf_vectorizer = loaded_artifacts['vectorizer']\n",
    "            print(\"Model and vectorizer loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Cannot generate predictions.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}. Cannot generate predictions.\")\n",
    "            return\n",
    "\n",
    "        # If X_test_processed or test_ids or target_columns are also None,\n",
    "        # it means we are running prediction independently and need to re-process the test data.\n",
    "        if X_test_processed is None or test_ids is None or target_columns is None:\n",
    "            print(\"Re-loading test data and re-processing for prediction...\")\n",
    "            try:\n",
    "                test_df = pd.read_csv(TEST_FILE)\n",
    "                text_column = 'comment_text'\n",
    "                id_column = 'id'\n",
    "                target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] # Re-define\n",
    "                test_ids = test_df[id_column]\n",
    "\n",
    "                if test_df[text_column].isnull().any():\n",
    "                    test_df[text_column].fillna('', inplace=True)\n",
    "\n",
    "                X_test_processed = tfidf_vectorizer.transform(test_df[text_column])\n",
    "                print(f\"Test data re-processed. Shape: {X_test_processed.shape}\")\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error loading test data: {e}. Cannot generate predictions.\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Error re-processing test data: {e}. Cannot generate predictions.\")\n",
    "                return\n",
    "\n",
    "    # 2. Generate predictions (predict_proba for classification)\n",
    "    print(\"Generating test predictions...\")\n",
    "    test_predictions_proba = trained_model.predict_proba(X_test_processed)\n",
    "\n",
    "    # 3. Build submission_df following the sample submission format\n",
    "    # The problem asks for 'toxic' probability.\n",
    "    toxic_col_idx = target_columns.index('toxic')\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'toxic': test_predictions_proba[:, toxic_col_idx]\n",
    "    })\n",
    "\n",
    "    # 4. Save the submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    # 5. Print confirmation message\n",
    "    print(f\"Submission file saved to {SUBMISSION_PATH}\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main training and evaluation pipeline\n",
    "    trained_model_instance, X_test_processed_data, test_ids_data, target_cols, tfidf_vectorizer_instance = main()\n",
    "\n",
    "    # Generate predictions using the returned model and processed data\n",
    "    # This ensures that if main() successfully trained a model, we use that instance\n",
    "    # and the already processed test data, avoiding re-loading/re-processing.\n",
    "    if trained_model_instance is not None:\n",
    "        generate_predictions(trained_model=trained_model_instance,\n",
    "                             X_test_processed=X_test_processed_data,\n",
    "                             test_ids=test_ids_data,\n",
    "                             target_columns=target_cols,\n",
    "                             tfidf_vectorizer=tfidf_vectorizer_instance)\n",
    "    else:\n",
    "        print(\"Model training failed. Attempting to generate predictions by loading saved model (if available).\")\n",
    "        # If main() failed, try to generate predictions by loading from disk\n",
    "        generate_predictions()\n",
    "\n",
    "    print(\"\\nScript execution finished successfully.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
