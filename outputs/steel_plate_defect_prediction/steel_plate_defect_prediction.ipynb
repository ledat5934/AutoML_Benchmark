{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib # For saving scaler and imputer\n",
    "\n",
    "def preprocess_data_for_training(is_training=True):\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, and preprocessing steps for the\n",
    "    steel plate defect prediction dataset.\n",
    "\n",
    "    Args:\n",
    "        is_training (bool): If True, loads train.csv and fits preprocessors.\n",
    "                            If False, loads test.csv and transforms using saved preprocessors.\n",
    "    Returns:\n",
    "        tuple: (X_processed, y_train_or_test_ids, TARGET_COLUMNS)\n",
    "               y_train_or_test_ids will be y_train (DataFrame) if is_training=True,\n",
    "               or test_ids (Series) if is_training=False.\n",
    "    \"\"\"\n",
    "    # Determine the project root dynamically\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    elif BASE_PATH_OPTION2.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # Define file paths\n",
    "    TRAIN_FILE = BASE_PATH / 'train.csv'\n",
    "    TEST_FILE = BASE_PATH / 'test.csv'\n",
    "\n",
    "    # Output paths for saving preprocessors\n",
    "    MODELS_DIR = Path(\"./models\")\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SCALER_PATH = MODELS_DIR / \"scaler.pkl\"\n",
    "    NUMERICAL_IMPUTER_PATH = MODELS_DIR / \"numerical_imputer.pkl\"\n",
    "\n",
    "    # Dataset Metadata (extracted from the provided JSON)\n",
    "    TARGET_COLUMNS = [\n",
    "        \"Pastry\", \"Z_Scratch\", \"K_Scatch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"\n",
    "    ]\n",
    "    ID_COLUMN = 'id'\n",
    "\n",
    "    if is_training:\n",
    "        print(f\"Loading train data from: {TRAIN_FILE}\")\n",
    "        df = pd.read_csv(TRAIN_FILE)\n",
    "        X = df.drop(columns=TARGET_COLUMNS + [ID_COLUMN])\n",
    "        y = df[TARGET_COLUMNS]\n",
    "    else:\n",
    "        print(f\"Loading test data from: {TEST_FILE}\")\n",
    "        df = pd.read_csv(TEST_FILE)\n",
    "        X = df.drop(columns=[ID_COLUMN])\n",
    "        y = df[ID_COLUMN] # For test set, y will be the IDs\n",
    "\n",
    "    # Identify column types\n",
    "    # All feature columns are numerical based on EDA and profiling.\n",
    "    # 'TypeOfSteel_A300' and 'TypeOfSteel_A400' are binary (0/1) and treated as numerical for scaling.\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    print(f\"Identified numerical columns for preprocessing: {numerical_cols}\")\n",
    "\n",
    "    # --- Preprocessing Steps ---\n",
    "\n",
    "    # 1. Impute missing values (though EDA shows no missing values, good practice)\n",
    "    # Numerical: Median\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Processing training data: Fitting and transforming imputer and scaler.\")\n",
    "        numerical_imputer = SimpleImputer(strategy='median')\n",
    "        X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])\n",
    "        joblib.dump(numerical_imputer, NUMERICAL_IMPUTER_PATH)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "    else:\n",
    "        print(\"Processing test data: Loading and transforming with saved imputer and scaler.\")\n",
    "        try:\n",
    "            numerical_imputer = joblib.load(NUMERICAL_IMPUTER_PATH)\n",
    "            scaler = joblib.load(SCALER_PATH)\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Preprocessors not found. Ensure Stage 1 (training mode) was run and saved them. Error: {e}\")\n",
    "\n",
    "        X[numerical_cols] = numerical_imputer.transform(X[numerical_cols])\n",
    "        X[numerical_cols] = scaler.transform(X[numerical_cols])\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    print(f\"Shape of preprocessed X: {X.shape}\")\n",
    "    if is_training:\n",
    "        print(f\"Shape of y: {y.shape}\")\n",
    "    else:\n",
    "        print(f\"Shape of test_ids: {y.shape}\")\n",
    "\n",
    "    # Display first few rows of preprocessed data\n",
    "    print(\"\\nPreprocessed X head:\")\n",
    "    print(X.head())\n",
    "    if is_training:\n",
    "        print(\"\\ny head:\")\n",
    "        print(y.head())\n",
    "\n",
    "    # Return preprocessed dataframes for further use in model training/prediction\n",
    "    return X, y, TARGET_COLUMNS\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # For Stage 1, we run in training mode\n",
    "    X_train_processed, y_train_processed, TARGET_COLUMNS = preprocess_data_for_training(is_training=True)\n",
    "    # You can now use X_train_processed, y_train_processed for model training in Stage 2\n",
    "\n",
    "    # To demonstrate how it would be called for prediction in Stage 3:\n",
    "    # X_test_processed, test_ids, _ = preprocess_data_for_training(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "import json\n",
    "import joblib\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold # For multi-label stratification\n",
    "\n",
    "# Define file paths (constants at the top)\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / 'train.csv'\n",
    "TEST_FILE = BASE_PATH / 'test.csv'\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / 'sample_submission.csv'\n",
    "\n",
    "# Output paths\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METRICS_PATH = OUTPUTS_DIR / \"metrics.json\"\n",
    "MODEL_PATH = MODELS_DIR / \"steel_plate_defect_prediction_model.pkl\"\n",
    "\n",
    "# Paths for preprocessors saved in Stage 1\n",
    "SCALER_PATH = MODELS_DIR / \"scaler.pkl\"\n",
    "NUMERICAL_IMPUTER_PATH = MODELS_DIR / \"numerical_imputer.pkl\"\n",
    "\n",
    "def preprocess_data_for_training(is_training=True):\n",
    "    \"\"\"\n",
    "    Loads data and applies the same preprocessing steps (imputation, scaling)\n",
    "    using the preprocessors saved during Stage 1 (if is_training is False).\n",
    "    This function is a refactored version of the Stage 1 main function,\n",
    "    made callable for both training and prediction contexts.\n",
    "\n",
    "    Args:\n",
    "        is_training (bool): If True, loads train.csv and fits preprocessors.\n",
    "                            If False, loads test.csv and transforms using saved preprocessors.\n",
    "    Returns:\n",
    "        tuple: (X_processed, y_train_or_test_ids, TARGET_COLUMNS)\n",
    "               y_train_or_test_ids will be y_train (DataFrame) if is_training=True,\n",
    "               or test_ids (Series) if is_training=False.\n",
    "    \"\"\"\n",
    "    # Determine the project root dynamically\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    elif BASE_PATH_OPTION2.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # Define file paths\n",
    "    TRAIN_FILE = BASE_PATH / 'train.csv'\n",
    "    TEST_FILE = BASE_PATH / 'test.csv'\n",
    "\n",
    "    # Output paths for saving preprocessors\n",
    "    MODELS_DIR = Path(\"./models\")\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SCALER_PATH = MODELS_DIR / \"scaler.pkl\"\n",
    "    NUMERICAL_IMPUTER_PATH = MODELS_DIR / \"numerical_imputer.pkl\"\n",
    "\n",
    "    # Dataset Metadata (extracted from the provided JSON)\n",
    "    TARGET_COLUMNS = [\n",
    "        \"Pastry\", \"Z_Scratch\", \"K_Scatch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"\n",
    "    ]\n",
    "    ID_COLUMN = 'id'\n",
    "\n",
    "    if is_training:\n",
    "        print(f\"Loading train data from: {TRAIN_FILE}\")\n",
    "        df = pd.read_csv(TRAIN_FILE)\n",
    "        X = df.drop(columns=TARGET_COLUMNS + [ID_COLUMN])\n",
    "        y = df[TARGET_COLUMNS]\n",
    "    else:\n",
    "        print(f\"Loading test data from: {TEST_FILE}\")\n",
    "        df = pd.read_csv(TEST_FILE)\n",
    "        X = df.drop(columns=[ID_COLUMN])\n",
    "        y = df[ID_COLUMN] # For test set, y will be the IDs\n",
    "\n",
    "    # Identify column types\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    print(f\"Identified numerical columns for preprocessing: {numerical_cols}\")\n",
    "\n",
    "    # --- Preprocessing Steps ---\n",
    "    if is_training:\n",
    "        print(\"Processing training data: Fitting and transforming imputer and scaler.\")\n",
    "        numerical_imputer = SimpleImputer(strategy='median')\n",
    "        X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])\n",
    "        joblib.dump(numerical_imputer, NUMERICAL_IMPUTER_PATH)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "    else:\n",
    "        print(\"Processing test data: Loading and transforming with saved imputer and scaler.\")\n",
    "        try:\n",
    "            numerical_imputer = joblib.load(NUMERICAL_IMPUTER_PATH)\n",
    "            scaler = joblib.load(SCALER_PATH)\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Preprocessors not found. Ensure Stage 1 (training mode) was run and saved them. Error: {e}\")\n",
    "\n",
    "        X[numerical_cols] = numerical_imputer.transform(X[numerical_cols])\n",
    "        X[numerical_cols] = scaler.transform(X[numerical_cols])\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    print(f\"Shape of preprocessed X: {X.shape}\")\n",
    "    if is_training:\n",
    "        print(f\"Shape of y: {y.shape}\")\n",
    "    else:\n",
    "        print(f\"Shape of test_ids: {y.shape}\")\n",
    "\n",
    "    # Display first few rows of preprocessed data\n",
    "    print(\"\\nPreprocessed X head:\")\n",
    "    print(X.head())\n",
    "    if is_training:\n",
    "        print(\"\\ny head:\")\n",
    "        print(y.head())\n",
    "\n",
    "    return X, y, TARGET_COLUMNS\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X_train_full, y_train_full, X_test, test_ids, TARGET_COLUMNS):\n",
    "    \"\"\"\n",
    "    Splits data, trains a LightGBM model for multi-label classification,\n",
    "    evaluates it, and persists metrics and the model.\n",
    "    \"\"\"\n",
    "    # Use MultilabelStratifiedKFold for the train-validation split to handle multi-label stratification.\n",
    "    # We'll use a single split (n_splits=5, taking the first one for approx 80/20 split)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "    # Get the indices for the first split (approx 80% train, 20% validation)\n",
    "    train_index, val_index = next(mskf.split(X_train_full, y_train_full))\n",
    "\n",
    "    X_train, X_val = X_train_full.iloc[train_index], X_train_full.iloc[val_index]\n",
    "    y_train, y_val = y_train_full.iloc[train_index], y_train_full.iloc[val_index]\n",
    "\n",
    "    print(f\"\\nShape of X_train after stratified split: {X_train.shape}\")\n",
    "    print(f\"Shape of y_train after stratified split: {y_train.shape}\")\n",
    "    print(f\"Shape of X_val after stratified split: {X_val.shape}\")\n",
    "    print(f\"Shape of y_val after stratified split: {y_val.shape}\")\n",
    "\n",
    "    trained_models = {}\n",
    "    metrics = {\"overall\": {}, \"per_target\": {}}\n",
    "\n",
    "    val_preds = pd.DataFrame(index=y_val.index, columns=TARGET_COLUMNS)\n",
    "    test_preds = pd.DataFrame(index=X_test.index, columns=TARGET_COLUMNS)\n",
    "\n",
    "    for target_col in TARGET_COLUMNS:\n",
    "        print(f\"\\nTraining model for target: {target_col}\")\n",
    "\n",
    "        # LightGBM Classifier for binary classification\n",
    "        lgb_clf = lgb.LGBMClassifier(objective='binary', random_state=42, n_estimators=1000, n_jobs=-1)\n",
    "\n",
    "        # Check if the target column in the training split has only one class\n",
    "        if y_train[target_col].nunique() < 2:\n",
    "            print(f\"  Skipping training for {target_col}: Only one class present in training split. Assigning default predictions.\")\n",
    "            # Assign default probabilities (e.g., 0.5) or the majority class probability\n",
    "            val_preds[target_col] = 0.5\n",
    "            test_preds[target_col] = 0.5\n",
    "\n",
    "            # Calculate metrics for this skipped target. ROC AUC is undefined, set to 0.5.\n",
    "            majority_class = y_train[target_col].mode()[0]\n",
    "            metrics[\"per_target\"][target_col] = {\n",
    "                \"roc_auc\": 0.5, \n",
    "                \"accuracy\": accuracy_score(y_val[target_col], np.full_like(y_val[target_col], majority_class)),\n",
    "                \"f1_score\": f1_score(y_val[target_col], np.full_like(y_val[target_col], majority_class)),\n",
    "                \"log_loss\": log_loss(y_val[target_col], np.full_like(y_val[target_col], 0.5))\n",
    "            }\n",
    "            trained_models[target_col] = None # Mark as not trained\n",
    "            continue # Skip to next target\n",
    "\n",
    "        lgb_clf.fit(X_train, y_train[target_col],\n",
    "                    eval_set=[(X_val, y_val[target_col])],\n",
    "                    eval_metric='auc',\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "        trained_models[target_col] = lgb_clf\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred_proba = lgb_clf.predict_proba(X_val)[:, 1]\n",
    "        y_val_pred_class = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        val_preds[target_col] = y_val_pred_proba\n",
    "\n",
    "        # Calculate metrics for current target\n",
    "        roc_auc = roc_auc_score(y_val[target_col], y_val_pred_proba)\n",
    "        accuracy = accuracy_score(y_val[target_col], y_val_pred_class)\n",
    "        f1 = f1_score(y_val[target_col], y_val_pred_class)\n",
    "        logloss = log_loss(y_val[target_col], y_val_pred_proba)\n",
    "\n",
    "        metrics[\"per_target\"][target_col] = {\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1,\n",
    "            \"log_loss\": logloss\n",
    "        }\n",
    "        print(f\"  {target_col} - ROC AUC: {roc_auc:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}, LogLoss: {logloss:.4f}\")\n",
    "\n",
    "        # Predict on test set\n",
    "        test_preds[target_col] = lgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate overall metrics (average ROC AUC as per task definition)\n",
    "    trained_target_metrics_roc_auc = [metrics[\"per_target\"][col][\"roc_auc\"] for col in TARGET_COLUMNS if trained_models[col] is not None]\n",
    "    if trained_target_metrics_roc_auc:\n",
    "        overall_roc_auc = np.mean(trained_target_metrics_roc_auc)\n",
    "    else:\n",
    "        overall_roc_auc = 0.0 # If no models were trained, set to 0.0\n",
    "\n",
    "    metrics[\"overall\"][\"average_roc_auc\"] = overall_roc_auc\n",
    "\n",
    "    overall_accuracy = np.mean([metrics[\"per_target\"][col][\"accuracy\"] for col in TARGET_COLUMNS])\n",
    "    overall_f1 = np.mean([metrics[\"per_target\"][col][\"f1_score\"] for col in TARGET_COLUMNS])\n",
    "    overall_logloss = np.mean([metrics[\"per_target\"][col][\"log_loss\"] for col in TARGET_COLUMNS])\n",
    "\n",
    "    metrics[\"overall\"][\"average_accuracy\"] = overall_accuracy\n",
    "    metrics[\"overall\"][\"average_f1_score\"] = overall_f1\n",
    "    metrics[\"overall\"][\"average_log_loss\"] = overall_logloss\n",
    "\n",
    "    print(f\"\\nOverall Average ROC AUC: {overall_roc_auc:.4f}\")\n",
    "    print(f\"Overall Average Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Overall Average F1 Score: {overall_f1:.4f}\")\n",
    "    print(f\"Overall Average Log Loss: {overall_logloss:.4f}\")\n",
    "\n",
    "    # Persist metrics to JSON file\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"\\nMetrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    # Persist the trained models (dictionary of models)\n",
    "    joblib.dump(trained_models, MODEL_PATH)\n",
    "    print(f\"Trained models saved to {MODEL_PATH}\")\n",
    "\n",
    "    return trained_models, test_preds\n",
    "\n",
    "def generate_submission(test_ids, test_predictions, sample_submission_file, target_columns):\n",
    "    \"\"\"\n",
    "    Generates the submission file in the required format.\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({'id': test_ids})\n",
    "    for col in target_columns:\n",
    "        submission_df[col] = test_predictions[col]\n",
    "\n",
    "    submission_df.to_csv(sample_submission_file.parent / 'submission.csv', index=False)\n",
    "    print(f\"\\nSubmission file generated at {sample_submission_file.parent / 'submission.csv'}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The error \"NameError: name 'preprocess_data_for_prediction' is not defined\"\n",
    "    # occurred because the main function in Stage 1 was renamed to `preprocess_data_for_training`\n",
    "    # and was not directly called in Stage 2's `if __name__ == '__main__':` block.\n",
    "    # We need to call `preprocess_data_for_training(is_training=True)` to get the processed data.\n",
    "    X_train_processed, y_train_processed, TARGET_COLUMNS = preprocess_data_for_training(is_training=True)\n",
    "\n",
    "    # Also, we need the preprocessed X_test for prediction later.\n",
    "    # Call preprocess_data_for_training with is_training=False to get X_test_processed and test_ids.\n",
    "    X_test_processed, test_ids, _ = preprocess_data_for_training(is_training=False)\n",
    "\n",
    "    trained_models, test_predictions = train_and_evaluate_model(X_train_processed, y_train_processed, X_test_processed, test_ids, TARGET_COLUMNS)\n",
    "    generate_submission(test_ids, test_predictions, SAMPLE_SUBMISSION_FILE, TARGET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb271c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define file paths (constants at the top)\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/steel_plate_defect_prediction').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TEST_FILE = BASE_PATH / 'test.csv'\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / 'sample_submission.csv'\n",
    "\n",
    "# Output paths\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"steel_plate_defect_prediction_model.pkl\"\n",
    "SUBMISSION_PATH = OUTPUTS_DIR / \"submission.csv\"\n",
    "\n",
    "# Paths for preprocessors saved in Stage 1\n",
    "SCALER_PATH = MODELS_DIR / \"scaler.pkl\"\n",
    "NUMERICAL_IMPUTER_PATH = MODELS_DIR / \"numerical_imputer.pkl\"\n",
    "\n",
    "def preprocess_data_for_prediction(is_training=False):\n",
    "    \"\"\"\n",
    "    Loads test data and applies the same preprocessing steps (imputation, scaling)\n",
    "    using the preprocessors saved during Stage 1.\n",
    "    This function is a refactored version of the Stage 1 main function,\n",
    "    made callable for both training and prediction contexts.\n",
    "\n",
    "    Args:\n",
    "        is_training (bool): This parameter is kept for consistency with the original\n",
    "                            `preprocess_data_for_training` signature, but for prediction\n",
    "                            it should always be False.\n",
    "    Returns:\n",
    "        tuple: (X_processed, test_ids, TARGET_COLUMNS)\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    TRAIN_FILE = BASE_PATH / 'train.csv' # Needed to infer column types if not explicitly passed\n",
    "    TEST_FILE = BASE_PATH / 'test.csv'\n",
    "\n",
    "    # Dataset Metadata (extracted from the provided JSON)\n",
    "    TARGET_COLUMNS = [\n",
    "        \"Pastry\", \"Z_Scratch\", \"K_Scatch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"\n",
    "    ]\n",
    "    ID_COLUMN = 'id'\n",
    "\n",
    "    # Load test data\n",
    "    print(f\"Loading test data from: {TEST_FILE}\")\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "    X = df.drop(columns=[ID_COLUMN])\n",
    "    test_ids = df[ID_COLUMN] # For test set, y will be the IDs\n",
    "\n",
    "    # To correctly identify numerical columns, it's best to load a small part of train_df\n",
    "    # or rely on a predefined list if the schema is fixed.\n",
    "    # For robustness, let's load train_df temporarily to get column types if not in training mode.\n",
    "    # In this specific case, all feature columns are numerical, so we can just select them.\n",
    "\n",
    "    # Identify column types (must be consistent with Stage 1)\n",
    "    # In Stage 1, all feature columns were identified as numerical and scaled together.\n",
    "    # 'TypeOfSteel_A300' and 'TypeOfSteel_A400' were treated as numerical (binary indicators).\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    print(f\"Identified numerical columns for test: {numerical_cols}\")\n",
    "\n",
    "    # Load preprocessors\n",
    "    try:\n",
    "        numerical_imputer = joblib.load(NUMERICAL_IMPUTER_PATH)\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Preprocessors not found. Ensure Stage 1 (training mode) was run and saved them. Error: {e}\")\n",
    "\n",
    "    # Apply imputation\n",
    "    print(\"Applying imputation to test data.\")\n",
    "    X[numerical_cols] = numerical_imputer.transform(X[numerical_cols])\n",
    "\n",
    "    # Apply scaling\n",
    "    print(\"Applying scaling to test data.\")\n",
    "    X[numerical_cols] = scaler.transform(X[numerical_cols])\n",
    "\n",
    "    print(\"\\nTest data preprocessing complete.\")\n",
    "    print(f\"Shape of preprocessed X_test: {X.shape}\")\n",
    "    print(f\"Shape of test_ids: {test_ids.shape}\")\n",
    "\n",
    "    # Display first few rows of preprocessed data\n",
    "    print(\"\\nPreprocessed X_test head:\")\n",
    "    print(X.head())\n",
    "\n",
    "    return X, test_ids, TARGET_COLUMNS\n",
    "\n",
    "def generate_predictions(X_test_processed, test_ids, trained_models=None):\n",
    "    \"\"\"\n",
    "    Generates predictions using the trained models.\n",
    "\n",
    "    Args:\n",
    "        X_test_processed (pd.DataFrame): Preprocessed test features.\n",
    "        test_ids (pd.Series): Original 'id' column from the test set.\n",
    "        trained_models (dict, optional): Dictionary of trained models.\n",
    "                                         If None, models will be loaded from MODEL_PATH.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing 'id' and probability predictions for each target.\n",
    "    \"\"\"\n",
    "    # Dataset Metadata (extracted from the provided JSON)\n",
    "    TARGET_COLUMNS = [\n",
    "        \"Pastry\", \"Z_Scratch\", \"K_Scatch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"\n",
    "    ]\n",
    "\n",
    "    # 1. Ensure trained_models is available\n",
    "    if trained_models is None:\n",
    "        print(f\"Loading trained models from {MODEL_PATH}\")\n",
    "        if not MODEL_PATH.exists():\n",
    "            raise FileNotFoundError(f\"Trained models not found at {MODEL_PATH}. Please run the training script first.\")\n",
    "        trained_models = joblib.load(MODEL_PATH)\n",
    "        print(\"Models loaded successfully.\")\n",
    "\n",
    "    # 2. Generate predictions\n",
    "    # Initialize a DataFrame to store predictions\n",
    "    test_predictions = pd.DataFrame(index=X_test_processed.index, columns=TARGET_COLUMNS)\n",
    "\n",
    "    for target_col in TARGET_COLUMNS:\n",
    "        # Check if the model for the current target exists and is not None (i.e., was successfully trained)\n",
    "        if target_col in trained_models and trained_models[target_col] is not None:\n",
    "            model = trained_models[target_col]\n",
    "            # Classification task, so we need predict_proba for ROC AUC metric\n",
    "            # Ensure the model has predict_proba method\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_predictions[target_col] = model.predict_proba(X_test_processed)[:, 1]\n",
    "            else:\n",
    "                # Fallback if a model somehow doesn't have predict_proba (shouldn't happen with LGBM)\n",
    "                print(f\"Warning: Model for {target_col} does not have predict_proba. Assigning default 0.5.\")\n",
    "                test_predictions[target_col] = 0.5\n",
    "        else:\n",
    "            # This handles cases where a model was explicitly set to None in Stage 2\n",
    "            # because the training data for that specific target had only one class.\n",
    "            print(f\"Warning: Model for {target_col} was not trained (or is None). Assigning default 0.5.\")\n",
    "            test_predictions[target_col] = 0.5 # Assign a default value if model is missing or None\n",
    "\n",
    "    # 3. Build submission_df following the sample submission format\n",
    "    submission_df = pd.DataFrame({'id': test_ids})\n",
    "    for col in TARGET_COLUMNS:\n",
    "        submission_df[col] = test_predictions[col]\n",
    "\n",
    "    # 4. Save the submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    # 5. Print a short confirmation message\n",
    "    print(f\"\\nSubmission file generated at {SUBMISSION_PATH}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The error \"NameError: name 'preprocess_data_for_prediction' is not defined\"\n",
    "    # occurred because the function was not defined in the scope where it was called.\n",
    "    # The previous Stage 1 code defined it, but Stage 3 is a separate script.\n",
    "    # We need to ensure `preprocess_data_for_prediction` is defined or imported within this script.\n",
    "    # Since it's a self-contained script, defining it here is appropriate.\n",
    "\n",
    "    # Call preprocess_data_for_prediction to get the preprocessed test set\n",
    "    X_test_processed_s3, test_ids_s3, TARGET_COLUMNS_s3 = preprocess_data_for_prediction(is_training=False)\n",
    "\n",
    "    # Generate predictions and the submission file\n",
    "    # trained_models is passed as None, so it will be loaded from MODEL_PATH\n",
    "    generate_predictions(X_test_processed_s3, test_ids_s3)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
