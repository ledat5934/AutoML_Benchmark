{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "def load_data(base_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the train and test datasets from the specified base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory where the dataset files are located.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the train and test DataFrames.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(base_path, \"train.csv\")\n",
    "    test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        print(f\"Successfully loaded train.csv from: {train_path}\")\n",
    "        print(f\"Successfully loaded test.csv from: {test_path}\")\n",
    "        print(\"\\nTrain data info:\")\n",
    "        train_df.info()\n",
    "        print(\"\\nTest data info:\")\n",
    "        test_df.info()\n",
    "\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please check the BASE_PATH and file names.\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame, \n",
    "    target_columns: list[str]\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataset:\n",
    "    - Separates features and target columns.\n",
    "    - Scales all numerical features using StandardScaler.\n",
    "    - Handles the 'id' column by separating it for the test set.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The raw training DataFrame.\n",
    "        test_df (pd.DataFrame): The raw test DataFrame.\n",
    "        target_columns (list[str]): A list of column names that represent the targets.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "            - X_train_processed (pd.DataFrame): Preprocessed training features.\n",
    "            - y_train (pd.DataFrame): Training target columns.\n",
    "            - X_test_processed (pd.DataFrame): Preprocessed test features.\n",
    "            - test_ids (pd.Series): The 'id' column from the test set.\n",
    "            - scaler (StandardScaler): The fitted StandardScaler object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store 'id' column for potential submission file generation\n",
    "    test_ids = test_df['id']\n",
    "\n",
    "    # Define feature columns (all columns except 'id' and target columns)\n",
    "    feature_columns = [col for col in train_df.columns if col not in target_columns + ['id']]\n",
    "\n",
    "    # Separate features and targets for training data\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df[target_columns]\n",
    "\n",
    "    # Select feature columns for test data (assuming test.csv does not contain target columns)\n",
    "    X_test = test_df[feature_columns]\n",
    "\n",
    "    # All identified features are numerical based on the dataset metadata.\n",
    "    # Binary columns like 'TypeOfSteel_A300' and 'TypeOfSteel_A400' are treated as numerical\n",
    "    # and will be scaled along with other numerical features.\n",
    "    numerical_features = feature_columns\n",
    "\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training features and transform both training and test features\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    # Convert scaled arrays back to DataFrames, preserving column names and index\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
    "\n",
    "    print(\"\\nData preprocessing completed.\")\n",
    "    print(f\"Shape of preprocessed training features (X_train_processed): {X_train_processed.shape}\")\n",
    "    print(f\"Shape of training targets (y_train): {y_train.shape}\")\n",
    "    print(f\"Shape of preprocessed test features (X_test_processed): {X_test_processed.shape}\")\n",
    "\n",
    "    return X_train_processed, y_train, X_test_processed, test_ids, scaler\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data loading and preprocessing pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- File Path Constants ---\n",
    "    # Default paths mirroring the standard Kaggle notebook directory layout\n",
    "    KAGGLE_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\"\n",
    "\n",
    "    # Fallback for local development or different environments\n",
    "    # Adjust this path if your local dataset directory structure is different\n",
    "    LOCAL_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\" \n",
    "\n",
    "    # Determine the actual base path based on existence\n",
    "    if os.path.exists(KAGGLE_BASE_PATH):\n",
    "        BASE_PATH = KAGGLE_BASE_PATH\n",
    "    elif os.path.exists(LOCAL_BASE_PATH):\n",
    "        BASE_PATH = LOCAL_BASE_PATH\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset base path not found at {KAGGLE_BASE_PATH} or {LOCAL_BASE_PATH}\")\n",
    "\n",
    "    # Define target columns based on the dataset metadata\n",
    "    TARGET_COLUMNS = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "\n",
    "    # 1. Load the dataset\n",
    "    train_df, test_df = load_data(BASE_PATH)\n",
    "\n",
    "    # 2. Perform preprocessing\n",
    "    X_train_processed, y_train, X_test_processed, test_ids, scaler = preprocess_data(\n",
    "        train_df.copy(), test_df.copy(), TARGET_COLUMNS\n",
    "    )\n",
    "\n",
    "    # Return the preprocessed data and scaler for subsequent ML stages\n",
    "    return X_train_processed, y_train, X_test_processed, test_ids, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main pipeline when the script is run\n",
    "    X_train, y_train, X_test, test_ids, scaler = main()\n",
    "\n",
    "    # Display a sample of the preprocessed data to verify\n",
    "    print(\"\\n--- Sample of Preprocessed Data ---\")\n",
    "    print(\"\\nFirst 5 rows of preprocessed X_train:\")\n",
    "    print(X_train.head())\n",
    "    print(\"\\nFirst 5 rows of y_train:\")\n",
    "    print(y_train.head())\n",
    "    print(\"\\nFirst 5 rows of preprocessed X_test:\")\n",
    "    print(X_test.head())\n",
    "    print(\"\\nFirst 5 test IDs:\")\n",
    "    print(test_ids.head())\n",
    "    print(f\"\\nShape of X_train: {X_train.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    print(f\"Shape of X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb # Import lightgbm for early stopping callback\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- File Path Constants ---\n",
    "# Default paths mirroring the standard Kaggle notebook directory layout\n",
    "KAGGLE_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\"\n",
    "\n",
    "# Fallback for local development or different environments\n",
    "# Adjust this path if your local dataset directory structure is different\n",
    "LOCAL_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\" \n",
    "\n",
    "# Determine the actual base path based on existence\n",
    "if os.path.exists(KAGGLE_BASE_PATH):\n",
    "    BASE_PATH = KAGGLE_BASE_PATH\n",
    "elif os.path.exists(LOCAL_BASE_PATH):\n",
    "    BASE_PATH = LOCAL_BASE_PATH\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset base path not found at {KAGGLE_BASE_PATH} or {LOCAL_BASE_PATH}\")\n",
    "\n",
    "# Define target columns based on the dataset metadata\n",
    "TARGET_COLUMNS = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "\n",
    "# Model persistence path\n",
    "MODEL_DIR = \"./models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"steel_plate_defect_prediction_model.pkl\")\n",
    "\n",
    "def load_data(base_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the train and test datasets from the specified base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory where the dataset files are located.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the train and test DataFrames.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(base_path, \"train.csv\")\n",
    "    test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        print(f\"Successfully loaded train.csv from: {train_path}\")\n",
    "        print(f\"Successfully loaded test.csv from: {test_path}\")\n",
    "        print(\"\\nTrain data info:\")\n",
    "        train_df.info()\n",
    "        print(\"\\nTest data info:\")\n",
    "        test_df.info()\n",
    "\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please check the BASE_PATH and file names.\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame, \n",
    "    target_columns: list[str]\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataset:\n",
    "    - Separates features and target columns.\n",
    "    - Scales all numerical features using StandardScaler.\n",
    "    - Handles the 'id' column by separating it for the test set.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The raw training DataFrame.\n",
    "        test_df (pd.DataFrame): The raw test DataFrame.\n",
    "        target_columns (list[str]): A list of column names that represent the targets.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "            - X_train_processed (pd.DataFrame): Preprocessed training features.\n",
    "            - y_train (pd.DataFrame): Training target columns.\n",
    "            - X_test_processed (pd.DataFrame): Preprocessed test features.\n",
    "            - test_ids (pd.Series): The 'id' column from the test set.\n",
    "            - scaler (StandardScaler): The fitted StandardScaler object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store 'id' column for potential submission file generation\n",
    "    test_ids = test_df['id']\n",
    "\n",
    "    # Define feature columns (all columns except 'id' and target columns)\n",
    "    feature_columns = [col for col in train_df.columns if col not in target_columns + ['id']]\n",
    "\n",
    "    # Separate features and targets for training data\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df[target_columns]\n",
    "\n",
    "    # Select feature columns for test data (assuming test.csv does not contain target columns)\n",
    "    X_test = test_df[feature_columns]\n",
    "\n",
    "    # All identified features are numerical based on the dataset metadata.\n",
    "    # Binary columns like 'TypeOfSteel_A300' and 'TypeOfSteel_A400' are treated as numerical\n",
    "    # and will be scaled along with other numerical features.\n",
    "    numerical_features = feature_columns\n",
    "\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training features and transform both training and test features\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    # Convert scaled arrays back to DataFrames, preserving column names and index\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
    "\n",
    "    print(\"\\nData preprocessing completed.\")\n",
    "    print(f\"Shape of preprocessed training features (X_train_processed): {X_train_processed.shape}\")\n",
    "    print(f\"Shape of training targets (y_train): {y_train.shape}\")\n",
    "    print(f\"Shape of preprocessed test features (X_test_processed): {X_test_processed.shape}\")\n",
    "\n",
    "    return X_train_processed, y_train, X_test_processed, test_ids, scaler\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    X_train_processed: pd.DataFrame, \n",
    "    y_train: pd.DataFrame, \n",
    "    X_test_processed: pd.DataFrame, \n",
    "    test_ids: pd.Series, \n",
    "    target_columns: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model for each target column, evaluates performance,\n",
    "    and persists the trained models.\n",
    "\n",
    "    Args:\n",
    "        X_train_processed (pd.DataFrame): Preprocessed training features.\n",
    "        y_train (pd.DataFrame): Training target columns.\n",
    "        X_test_processed (pd.DataFrame): Preprocessed test features.\n",
    "        test_ids (pd.Series): The 'id' column from the test set.\n",
    "        target_columns (list[str]): A list of column names that represent the targets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the trained models, one for each target.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Model Training and Evaluation ---\")\n",
    "\n",
    "    # Perform an 80/20 stratified split into training and validation sets\n",
    "    # For multi-label classification, stratify on one of the target columns.\n",
    "    # 'Other_Faults' is chosen as it's the least imbalanced among the targets.\n",
    "    # Ensure y_train['Other_Faults'] is not all zeros or all ones for stratification to work.\n",
    "    # If it is, stratification will fail. Based on EDA, it's not all zeros/ones.\n",
    "    try:\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_train_processed, y_train, test_size=0.2, random_state=42, stratify=y_train['Other_Faults']\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Stratification failed for 'Other_Faults'. Falling back to non-stratified split. Error: {e}\")\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_train_processed, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "    print(f\"Training split shapes: X_train_split={X_train_split.shape}, y_train_split={y_train_split.shape}\")\n",
    "    print(f\"Validation split shapes: X_val_split={X_val_split.shape}, y_val_split={y_val_split.shape}\")\n",
    "\n",
    "    trained_models = {}\n",
    "    val_preds_list = []\n",
    "    val_probas_list = []\n",
    "\n",
    "    for target_col in target_columns:\n",
    "        print(f\"\\nTraining model for target: {target_col}\")\n",
    "\n",
    "        # Initialize LGBMClassifier for binary classification\n",
    "        model = LGBMClassifier(objective='binary', metric='logloss', random_state=42, n_estimators=1000)\n",
    "\n",
    "        # Fit the model with early stopping\n",
    "        # Corrected: Use lgb.early_stopping instead of LGBMClassifier.early_stopping\n",
    "        model.fit(X_train_split, y_train_split[target_col],\n",
    "                  eval_set=[(X_val_split, y_val_split[target_col])],\n",
    "                  eval_metric='logloss',\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
    "\n",
    "        trained_models[target_col] = model\n",
    "\n",
    "        # Make predictions and probabilities on the validation set\n",
    "        val_pred = model.predict(X_val_split)\n",
    "        val_proba = model.predict_proba(X_val_split)[:, 1] # Probability of the positive class\n",
    "\n",
    "        val_preds_list.append(val_pred)\n",
    "        val_probas_list.append(val_proba)\n",
    "\n",
    "    # Convert lists of predictions/probabilities to DataFrames\n",
    "    y_val_pred = pd.DataFrame(val_preds_list).T\n",
    "    y_val_pred.columns = target_columns\n",
    "    y_val_proba = pd.DataFrame(val_probas_list).T\n",
    "    y_val_proba.columns = target_columns\n",
    "\n",
    "    # --- Evaluate Metrics ---\n",
    "    print(\"\\n--- Evaluation Metrics on Validation Set ---\")\n",
    "\n",
    "    overall_accuracy = []\n",
    "    overall_f1_macro = []\n",
    "    overall_f1_micro = []\n",
    "    overall_logloss = []\n",
    "\n",
    "    for i, target_col in enumerate(target_columns):\n",
    "        y_true_col = y_val_split[target_col]\n",
    "        y_pred_col = y_val_pred[target_col]\n",
    "        y_proba_col = y_val_proba[target_col]\n",
    "\n",
    "        acc = accuracy_score(y_true_col, y_pred_col)\n",
    "        # Handle cases where F1 score might be undefined due to no positive samples\n",
    "        try:\n",
    "            f1_macro = f1_score(y_true_col, y_pred_col, average='macro', zero_division=0)\n",
    "        except ValueError: # Raised if there are no positive samples in y_true or y_pred\n",
    "            f1_macro = 0.0\n",
    "        try:\n",
    "            f1_micro = f1_score(y_true_col, y_pred_col, average='micro', zero_division=0)\n",
    "        except ValueError:\n",
    "            f1_micro = 0.0\n",
    "\n",
    "        ll = log_loss(y_true_col, y_proba_col)\n",
    "\n",
    "        overall_accuracy.append(acc)\n",
    "        overall_f1_macro.append(f1_macro)\n",
    "        overall_f1_micro.append(f1_micro)\n",
    "        overall_logloss.append(ll)\n",
    "\n",
    "        print(f\"Metrics for {target_col}:\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  F1 (Macro): {f1_macro:.4f}\")\n",
    "        print(f\"  F1 (Micro): {f1_micro:.4f}\")\n",
    "        print(f\"  LogLoss: {ll:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Average Metrics Across All Targets ---\")\n",
    "    print(f\"Average Accuracy: {sum(overall_accuracy) / len(overall_accuracy):.4f}\")\n",
    "    print(f\"Average F1 (Macro): {sum(overall_f1_macro) / len(overall_f1_macro):.4f}\")\n",
    "    print(f\"Average F1 (Micro): {sum(overall_f1_micro) / len(overall_f1_micro):.4f}\")\n",
    "    print(f\"Average LogLoss: {sum(overall_logloss) / len(overall_logloss):.4f}\")\n",
    "\n",
    "    # --- Persist the trained models ---\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    joblib.dump(trained_models, MODEL_PATH)\n",
    "    print(f\"\\nTrained models saved to {MODEL_PATH}\")\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data loading, preprocessing, training, and evaluation pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the dataset\n",
    "    train_df, test_df = load_data(BASE_PATH)\n",
    "\n",
    "    # 2. Perform preprocessing\n",
    "    X_train_processed, y_train, X_test_processed, test_ids, scaler = preprocess_data(\n",
    "        train_df.copy(), test_df.copy(), TARGET_COLUMNS\n",
    "    )\n",
    "\n",
    "    # 3. Train and evaluate the model\n",
    "    trained_model = train_and_evaluate_model(\n",
    "        X_train_processed, y_train, X_test_processed, test_ids, TARGET_COLUMNS\n",
    "    )\n",
    "\n",
    "    # Return the trained model instance for potential further use (e.g., prediction on test_df)\n",
    "    return trained_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main pipeline when the script is run\n",
    "    trained_model_instance = main()\n",
    "\n",
    "    # Display a confirmation of the trained model\n",
    "    print(\"\\n--- Pipeline Execution Complete ---\")\n",
    "    print(f\"Trained model instance (dictionary of LGBMClassifiers): {trained_model_instance}\")\n",
    "    print(f\"Keys in trained_model_instance: {list(trained_model_instance.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import joblib # For loading the trained models\n",
    "\n",
    "# --- File Path Constants ---\n",
    "# Default paths mirroring the standard Kaggle notebook directory layout\n",
    "KAGGLE_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\"\n",
    "\n",
    "# Fallback for local development or different environments\n",
    "# Adjust this path if your local dataset directory structure is different\n",
    "LOCAL_BASE_PATH = \"input/Datasets/datasets/steel_plate_defect_prediction\" \n",
    "\n",
    "# Model persistence path (from previous training script)\n",
    "MODEL_DIR = \"./models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"steel_plate_defect_prediction_model.pkl\")\n",
    "\n",
    "# Submission file path\n",
    "SUBMISSION_DIR = \"./outputs\"\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n",
    "\n",
    "# Define target columns based on the dataset metadata\n",
    "TARGET_COLUMNS = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "\n",
    "def load_data(base_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the train and test datasets from the specified base path.\n",
    "    This function is re-used from the preprocessing script to ensure consistency.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory where the dataset files are located.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the train and test DataFrames.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(base_path, \"train.csv\")\n",
    "    test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        print(f\"Successfully loaded train.csv from: {train_path}\")\n",
    "        print(f\"Successfully loaded test.csv from: {test_path}\")\n",
    "\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please check the BASE_PATH and file names.\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame, \n",
    "    target_columns: list[str]\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataset.\n",
    "    This function is re-used from the preprocessing script to ensure consistency.\n",
    "    The `train_df` is required to fit the StandardScaler, even when only processing `test_df` for prediction.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The raw training DataFrame (used for scaler fitting).\n",
    "        test_df (pd.DataFrame): The raw test DataFrame.\n",
    "        target_columns (list[str]): A list of column names that represent the targets.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, StandardScaler]:\n",
    "            - X_train_processed (pd.DataFrame): Preprocessed training features (not directly used here, but returned).\n",
    "            - y_train (pd.DataFrame): Training target columns (not directly used here, but returned).\n",
    "            - X_test_processed (pd.DataFrame): Preprocessed test features.\n",
    "            - test_ids (pd.Series): The 'id' column from the test set.\n",
    "            - scaler (StandardScaler): The fitted StandardScaler object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store 'id' column for potential submission file generation\n",
    "    test_ids = test_df['id']\n",
    "\n",
    "    # Define feature columns (all columns except 'id' and target columns)\n",
    "    feature_columns = [col for col in train_df.columns if col not in target_columns + ['id']]\n",
    "\n",
    "    # Separate features and targets for training data\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df[target_columns] # y_train is not strictly needed for test preprocessing, but returned by the function signature\n",
    "\n",
    "    # Select feature columns for test data (assuming test.csv does not contain target columns)\n",
    "    X_test = test_df[feature_columns]\n",
    "\n",
    "    # All identified features are numerical based on the dataset metadata.\n",
    "    numerical_features = feature_columns\n",
    "\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training features and transform both training and test features\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    # Convert scaled arrays back to DataFrames, preserving column names and index\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
    "\n",
    "    print(\"\\nData preprocessing completed for prediction.\")\n",
    "    print(f\"Shape of preprocessed test features (X_test_processed): {X_test_processed.shape}\")\n",
    "\n",
    "    return X_train_processed, y_train, X_test_processed, test_ids, scaler\n",
    "\n",
    "def predict_and_submit():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the prediction and submission file generation pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the actual base path based on existence\n",
    "    if os.path.exists(KAGGLE_BASE_PATH):\n",
    "        BASE_PATH = KAGGLE_BASE_PATH\n",
    "    elif os.path.exists(LOCAL_BASE_PATH):\n",
    "        BASE_PATH = LOCAL_BASE_PATH\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset base path not found at {KAGGLE_BASE_PATH} or {LOCAL_BASE_PATH}\")\n",
    "\n",
    "    print(f\"Using BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # 1. Load the dataset\n",
    "    # train_df is loaded because it's needed by preprocess_data to fit the scaler.\n",
    "    train_df, test_df = load_data(BASE_PATH)\n",
    "\n",
    "    # 2. Preprocess data to get X_test_processed and test_ids\n",
    "    # The previous error was in Stage 2 related to LightGBM's early stopping callback.\n",
    "    # This Stage 3 code assumes Stage 2 has been successfully executed and models are saved.\n",
    "    # X_test_processed is generated in-memory here, consistent with Stage 1's output.\n",
    "    _, _, X_test_processed, test_ids, _ = preprocess_data(\n",
    "        train_df.copy(), test_df.copy(), TARGET_COLUMNS\n",
    "    )\n",
    "\n",
    "    # 3. Load the trained models\n",
    "    print(f\"\\nAttempting to load trained models from: {MODEL_PATH}\")\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Trained models not found at {MODEL_PATH}. \"\n",
    "                                \"Please ensure the training script has been run successfully to save the models.\")\n",
    "\n",
    "    trained_models = joblib.load(MODEL_PATH)\n",
    "    print(\"Trained models loaded successfully.\")\n",
    "    print(f\"Models loaded for targets: {list(trained_models.keys())}\")\n",
    "\n",
    "    # 4. Generate predictions (probabilities for multi-label classification)\n",
    "    print(\"\\nGenerating predictions on the test set...\")\n",
    "    test_predictions_proba = {}\n",
    "    for target_col in TARGET_COLUMNS:\n",
    "        if target_col in trained_models:\n",
    "            model = trained_models[target_col]\n",
    "            # Predict probabilities for the positive class (class 1)\n",
    "            test_predictions_proba[target_col] = model.predict_proba(X_test_processed)[:, 1]\n",
    "            print(f\"  Predictions generated for '{target_col}'.\")\n",
    "        else:\n",
    "            print(f\"  Warning: No model found for target '{target_col}'. Skipping and setting probabilities to 0.0.\")\n",
    "            # This fallback ensures the submission file can still be created even if a model is missing.\n",
    "            test_predictions_proba[target_col] = [0.0] * len(X_test_processed) \n",
    "\n",
    "    # 5. Build submission DataFrame\n",
    "    print(\"\\nBuilding submission file...\")\n",
    "    submission_df = pd.DataFrame({'id': test_ids})\n",
    "    for target_col in TARGET_COLUMNS:\n",
    "        submission_df[target_col] = test_predictions_proba[target_col]\n",
    "\n",
    "    # 6. Save the submission file\n",
    "    os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission file generated successfully at: {SUBMISSION_PATH}\")\n",
    "    print(f\"Submission file head:\\n{submission_df.head()}\")\n",
    "    print(f\"Submission file shape: {submission_df.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_and_submit()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
