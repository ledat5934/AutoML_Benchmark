{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa63f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define BASE_PATH with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/multi_label_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/multi_label_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# File path constants\n",
    "TRAIN_CSV_PATH = BASE_PATH / 'train.csv'\n",
    "TEST_CSV_PATH = BASE_PATH / 'test.csv'\n",
    "IMAGE_FOLDER_PATH = BASE_PATH / 'data'\n",
    "# FIX: Corrected METADATA_JSON_PATH to point to the correct location of the metadata file.\n",
    "# The metadata file is typically at the same level as the 'input' directory, or within 'input/Datasets'.\n",
    "# Based on the problem description, the metadata JSON is provided as a separate input.\n",
    "# Assuming the metadata file is named 'multi_label_classification.json' and is in the 'input/Datasets' directory.\n",
    "METADATA_JSON_PATH = BASE_PATH.parent.parent / 'multi_label_classification.json'\n",
    "\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder_path = image_folder_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['ImageID']\n",
    "        img_path = os.path.join(self.image_folder_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Labels will be handled by MultiLabelBinarizer outside the dataset for consistency\n",
    "        # and to avoid passing them if only image features are needed.\n",
    "        # For training, labels will be merged with image features later.\n",
    "        return image, img_name\n",
    "\n",
    "def load_metadata(metadata_path):\n",
    "    \"\"\"Loads and returns the dataset metadata JSON.\"\"\"\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_data(df, metadata, mlb=None, tfidf_vectorizer=None, scaler=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataframe.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        metadata (dict): The dataset metadata.\n",
    "        mlb (MultiLabelBinarizer, optional): Fitted MultiLabelBinarizer for labels.\n",
    "                                             Required for test set.\n",
    "        tfidf_vectorizer (TfidfVectorizer, optional): Fitted TfidfVectorizer for captions.\n",
    "                                                      Required for test set.\n",
    "        scaler (StandardScaler, optional): Fitted StandardScaler for numerical features.\n",
    "                                           Required for test set.\n",
    "        is_train (bool): True if processing the training set, False for test set.\n",
    "    Returns:\n",
    "        tuple: Processed dataframe, fitted mlb, fitted tfidf_vectorizer, fitted scaler.\n",
    "    \"\"\"\n",
    "    # Identify column types from metadata\n",
    "    variables_info = metadata['profiling_summary']['variables']\n",
    "    image_id_col = None\n",
    "    label_col = None\n",
    "    caption_col = None\n",
    "\n",
    "    for col_name, col_info in variables_info.items():\n",
    "        if col_name == 'ImageID':\n",
    "            image_id_col = col_name\n",
    "        elif col_name == 'Labels':\n",
    "            label_col = col_name\n",
    "        elif col_name == 'Caption':\n",
    "            caption_col = col_name\n",
    "\n",
    "    # --- Handle Labels (Multi-label Binarization) ---\n",
    "    if label_col and label_col in df.columns:\n",
    "        # Labels are space-separated strings, e.g., \"1 3 5\"\n",
    "        df[label_col] = df[label_col].apply(lambda x: x.split())\n",
    "\n",
    "        if is_train:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            labels_encoded = mlb.fit_transform(df[label_col])\n",
    "            # Create a DataFrame for encoded labels\n",
    "            labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "        else:\n",
    "            if mlb is None:\n",
    "                raise ValueError(\"MultiLabelBinarizer must be fitted on training data and provided for test data.\")\n",
    "            labels_encoded = mlb.transform(df[label_col])\n",
    "            labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "    else:\n",
    "        print(f\"Warning: Label column '{label_col}' not found or not specified in metadata.\")\n",
    "\n",
    "    # --- Handle Caption (TF-IDF) ---\n",
    "    if caption_col and caption_col in df.columns:\n",
    "        if is_train:\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limit features to manage dimensionality\n",
    "            caption_features = tfidf_vectorizer.fit_transform(df[caption_col])\n",
    "        else:\n",
    "            if tfidf_vectorizer is None:\n",
    "                raise ValueError(\"TfidfVectorizer must be fitted on training data and provided for test data.\")\n",
    "            caption_features = tfidf_vectorizer.transform(df[caption_col])\n",
    "\n",
    "        # Convert TF-IDF sparse matrix to DataFrame\n",
    "        caption_df = pd.DataFrame(caption_features.toarray(),\n",
    "                                  columns=[f'caption_tfidf_{i}' for i in range(caption_features.shape[1])],\n",
    "                                  index=df.index)\n",
    "        df = pd.concat([df.drop(columns=[caption_col]), caption_df], axis=1)\n",
    "    else:\n",
    "        print(f\"Warning: Caption column '{caption_col}' not found or not specified in metadata.\")\n",
    "\n",
    "    # --- Handle ImageID (for merging with image features later) ---\n",
    "    # ImageID is already 'object' type, no direct preprocessing needed here,\n",
    "    # but it's crucial for merging with image features.\n",
    "\n",
    "    # No numerical columns identified in the provided metadata for scaling.\n",
    "    # If there were, the logic would be:\n",
    "    # numerical_cols = [col for col, info in variables_info.items() if info['type'] == 'Numeric']\n",
    "    # if numerical_cols:\n",
    "    #     if is_train:\n",
    "    #         scaler = StandardScaler()\n",
    "    #         df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    #     else:\n",
    "    #         if scaler is None:\n",
    "    #             raise ValueError(\"StandardScaler must be fitted on training data and provided for test data.\")\n",
    "    #         df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "\n",
    "    return df, mlb, tfidf_vectorizer, scaler\n",
    "\n",
    "\n",
    "def main():\n",
    "    metadata = load_metadata(METADATA_JSON_PATH)\n",
    "\n",
    "    # Load tabular data\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    print(\"Original Train DataFrame head:\")\n",
    "    print(train_df.head())\n",
    "    print(\"\\nOriginal Test DataFrame head:\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    # Preprocess tabular data\n",
    "    processed_train_df, mlb, tfidf_vectorizer, scaler = preprocess_data(train_df.copy(), metadata, is_train=True)\n",
    "    processed_test_df, _, _, _ = preprocess_data(test_df.copy(), metadata, mlb=mlb, tfidf_vectorizer=tfidf_vectorizer, scaler=scaler, is_train=False)\n",
    "\n",
    "    print(\"\\nProcessed Train DataFrame head (tabular features):\")\n",
    "    print(processed_train_df.head())\n",
    "    print(\"\\nProcessed Test DataFrame head (tabular features):\")\n",
    "    print(processed_test_df.head())\n",
    "\n",
    "    # Image preprocessing setup\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
    "    ])\n",
    "\n",
    "    # Create image datasets and dataloaders\n",
    "    train_image_dataset = MultiLabelImageDataset(processed_train_df, IMAGE_FOLDER_PATH, transform=image_transform)\n",
    "    test_image_dataset = MultiLabelImageDataset(processed_test_df, IMAGE_FOLDER_PATH, transform=image_transform)\n",
    "\n",
    "    train_image_dataloader = DataLoader(train_image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "    test_image_dataloader = DataLoader(test_image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "    print(f\"\\nTrain image dataset size: {len(train_image_dataset)}\")\n",
    "    print(f\"Test image dataset size: {len(test_image_dataset)}\")\n",
    "    print(f\"Example image batch from train_image_dataloader (shape):\")\n",
    "    for i, (images, img_ids) in enumerate(train_image_dataloader):\n",
    "        print(f\"Batch {i+1}: Images shape: {images.shape}, Image IDs: {img_ids[:5]}\")\n",
    "        break # Just show one batch\n",
    "\n",
    "    # Return processed dataframes and necessary transformers/scalers\n",
    "    # Note: Image data is returned as DataLoaders, which will be iterated over in Stage 2.\n",
    "    # The processed_train_df and processed_test_df contain the tabular features (TF-IDF, binarized labels).\n",
    "    # Image features (embeddings) would typically be extracted in Stage 2 using a pre-trained model\n",
    "    # and then merged with these tabular features based on 'ImageID'.\n",
    "    return {\n",
    "        \"processed_train_df\": processed_train_df,\n",
    "        \"processed_test_df\": processed_test_df,\n",
    "        \"train_image_dataloader\": train_image_dataloader,\n",
    "        \"test_image_dataloader\": test_image_dataloader,\n",
    "        \"mlb\": mlb,\n",
    "        \"tfidf_vectorizer\": tfidf_vectorizer,\n",
    "        \"scaler\": scaler # Will be None in this specific case as no numerical features were scaled\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to run and access the outputs\n",
    "    preprocessed_data = main()\n",
    "    # You can now access:\n",
    "    # preprocessed_data[\"processed_train_df\"]\n",
    "    # preprocessed_data[\"processed_test_df\"]\n",
    "    # preprocessed_data[\"train_image_dataloader\"]\n",
    "    # preprocessed_data[\"test_image_dataloader\"]\n",
    "    # preprocessed_data[\"mlb\"]\n",
    "    # preprocessed_data[\"tfidf_vectorizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50349ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, roc_auc_score\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from scikit-learn\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define BASE_PATH with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/multi_label_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/multi_label_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# File path constants\n",
    "TRAIN_CSV_PATH = BASE_PATH / 'train.csv'\n",
    "TEST_CSV_PATH = BASE_PATH / 'test.csv'\n",
    "IMAGE_FOLDER_PATH = BASE_PATH / 'data'\n",
    "# FIX: Corrected METADATA_JSON_PATH to point to the correct location of the metadata file.\n",
    "# The metadata file is typically at the same level as the 'input' directory, or within 'input/Datasets'.\n",
    "# Based on the problem description, the metadata JSON is provided as a separate input.\n",
    "# Assuming the metadata file is named 'multi_label_classification.json' and is in the 'input/Datasets' directory.\n",
    "METADATA_JSON_PATH = BASE_PATH.parent.parent / 'multi_label_classification.json'\n",
    "\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_PATH = OUTPUT_DIR / \"metrics.json\"\n",
    "MODEL_PATH = Path(\"./models/multi_label_classification_model.pkl\")\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "MLB_PATH = Path(\"./models/mlb.pkl\")\n",
    "TFIDF_PATH = Path(\"./models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20 # Increased epochs for better training\n",
    "EARLY_STOPPING_ROUNDS = 5 # Reduced early stopping rounds for faster convergence\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder_path, transform=None, is_test=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder_path = image_folder_path\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        # Assuming label columns are numeric strings after binarization\n",
    "        self.label_columns = [col for col in dataframe.columns if col.isdigit()]\n",
    "        # Assuming TF-IDF columns start with 'caption_tfidf_'\n",
    "        self.tfidf_columns = [col for col in dataframe.columns if col.startswith('caption_tfidf_')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['ImageID']\n",
    "        img_path = os.path.join(self.image_folder_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Extract TF-IDF features\n",
    "        tfidf_features = torch.tensor(self.dataframe.iloc[idx][self.tfidf_columns].values.astype(np.float32))\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, tfidf_features, img_name\n",
    "        else:\n",
    "            labels = self.dataframe.iloc[idx][self.label_columns].values.astype(np.float32)\n",
    "            return image, torch.tensor(labels), tfidf_features, img_name\n",
    "\n",
    "def load_metadata(metadata_path):\n",
    "    \"\"\"Loads and returns the dataset metadata JSON.\"\"\"\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_data(df, metadata, mlb=None, tfidf_vectorizer=None, scaler=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataframe.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        metadata (dict): The dataset metadata.\n",
    "        mlb (MultiLabelBinarizer, optional): Fitted MultiLabelBinarizer for labels.\n",
    "                                             Required for test set.\n",
    "        tfidf_vectorizer (TfidfVectorizer, optional): Fitted TfidfVectorizer for captions.\n",
    "                                                      Required for test set.\n",
    "        scaler (StandardScaler, optional): Fitted StandardScaler for numerical features.\n",
    "                                           Required for test set.\n",
    "        is_train (bool): True if processing the training set, False for test set.\n",
    "    Returns:\n",
    "        tuple: Processed dataframe, fitted mlb, fitted tfidf_vectorizer, fitted scaler.\n",
    "    \"\"\"\n",
    "    # Identify column types from metadata\n",
    "    variables_info = metadata['profiling_summary']['variables']\n",
    "    image_id_col = None\n",
    "    label_col = None\n",
    "    caption_col = None\n",
    "\n",
    "    for col_name, col_info in variables_info.items():\n",
    "        if col_name == 'ImageID':\n",
    "            image_id_col = col_name\n",
    "        elif col_name == 'Labels':\n",
    "            label_col = col_name\n",
    "        elif col_name == 'Caption':\n",
    "            caption_col = col_name\n",
    "\n",
    "    # --- Handle Labels (Multi-label Binarization) ---\n",
    "    if label_col and label_col in df.columns:\n",
    "        # Labels are space-separated strings, e.g., \"1 3 5\"\n",
    "        df[label_col] = df[label_col].apply(lambda x: x.split())\n",
    "\n",
    "        if is_train:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            labels_encoded = mlb.fit_transform(df[label_col])\n",
    "            # Create a DataFrame for encoded labels\n",
    "            labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "        else:\n",
    "            if mlb is None:\n",
    "                raise ValueError(\"MultiLabelBinarizer must be fitted on training data and provided for test data.\")\n",
    "            labels_encoded = mlb.transform(df[label_col])\n",
    "            labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "    else:\n",
    "        print(f\"Warning: Label column '{label_col}' not found or not specified in metadata.\")\n",
    "\n",
    "    # --- Handle Caption (TF-IDF) ---\n",
    "    if caption_col and caption_col in df.columns:\n",
    "        if is_train:\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limit features to manage dimensionality\n",
    "            caption_features = tfidf_vectorizer.fit_transform(df[caption_col])\n",
    "        else:\n",
    "            if tfidf_vectorizer is None:\n",
    "                raise ValueError(\"TfidfVectorizer must be fitted on training data and provided for test data.\")\n",
    "            caption_features = tfidf_vectorizer.transform(df[caption_col])\n",
    "\n",
    "        # Convert TF-IDF sparse matrix to DataFrame\n",
    "        caption_df = pd.DataFrame(caption_features.toarray(),\n",
    "                                  columns=[f'caption_tfidf_{i}' for i in range(caption_features.shape[1])],\n",
    "                                  index=df.index)\n",
    "        df = pd.concat([df.drop(columns=[caption_col]), caption_df], axis=1)\n",
    "    else:\n",
    "        print(f\"Warning: Caption column '{caption_col}' not found or not specified in metadata.\")\n",
    "\n",
    "    return df, mlb, tfidf_vectorizer, scaler\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_classes, tfidf_features_dim):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        # Image branch: Pre-trained ResNet\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Freeze all parameters in ResNet initially\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the final classification layer\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity() # Remove the final FC layer to get features\n",
    "\n",
    "        # TF-IDF branch\n",
    "        self.tfidf_fc = nn.Linear(tfidf_features_dim, 512) # Project TF-IDF features to a common dimension\n",
    "\n",
    "        # Combined branch\n",
    "        self.combined_fc1 = nn.Linear(num_ftrs + 512, 1024) # Combine image and TF-IDF features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.combined_fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, image_input, tfidf_input):\n",
    "        # Image branch\n",
    "        image_features = self.resnet(image_input)\n",
    "\n",
    "        # TF-IDF branch\n",
    "        tfidf_features = self.relu(self.tfidf_fc(tfidf_input))\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((image_features, tfidf_features), dim=1)\n",
    "\n",
    "        # Combined branch\n",
    "        output = self.combined_fc1(combined_features)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.combined_fc2(output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, early_stopping_rounds):\n",
    "    best_val_f1 = -1\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for images, labels, tfidf_features, _ in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            tfidf_features = tfidf_features.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, tfidf_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            # FIX: RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "            # Detach the tensor from the computation graph before converting to numpy.\n",
    "            train_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            train_targets.append(labels.cpu().numpy())\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_dataloader.dataset)\n",
    "        all_train_preds = np.vstack(train_preds)\n",
    "        all_train_targets = np.vstack(train_targets)\n",
    "        # Convert probabilities to binary predictions for F1 score\n",
    "        train_binary_preds = (all_train_preds > 0.5).astype(int)\n",
    "        epoch_train_f1 = f1_score(all_train_targets, train_binary_preds, average='samples') # 'samples' for multi-label\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels, tfidf_features, _ in tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                tfidf_features = tfidf_features.to(DEVICE)\n",
    "\n",
    "                outputs = model(images, tfidf_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                val_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_targets.append(labels.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_dataloader.dataset)\n",
    "        all_val_preds = np.vstack(val_preds)\n",
    "        all_val_targets = np.vstack(val_targets)\n",
    "        val_binary_preds = (all_val_preds > 0.5).astype(int)\n",
    "        epoch_val_f1 = f1_score(all_val_targets, val_binary_preds, average='samples')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_train_loss:.4f}, Train F1: {epoch_train_f1:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_f1'].append(epoch_val_f1)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_val_f1 > best_val_f1:\n",
    "            best_val_f1 = epoch_val_f1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Model saved to {MODEL_PATH} with improved F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stopping_rounds:\n",
    "                print(f\"Early stopping triggered after {early_stopping_rounds} epochs without improvement.\")\n",
    "                break\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, mlb):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, tfidf_features, _ in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            tfidf_features = tfidf_features.to(DEVICE)\n",
    "\n",
    "            outputs = model(images, tfidf_features)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.append(probabilities)\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "    predictions_array = np.vstack(all_preds)\n",
    "    targets_array = np.vstack(all_targets)\n",
    "\n",
    "    # Convert probabilities to binary labels for F1 score\n",
    "    binary_predictions = (predictions_array > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(targets_array, binary_predictions, average='samples')\n",
    "    accuracy = accuracy_score(targets_array, binary_predictions)\n",
    "    # Log loss requires probabilities\n",
    "    loss = log_loss(targets_array.ravel(), predictions_array.ravel()) # Flatten for log_loss\n",
    "    # ROC AUC for multi-label can be tricky, 'weighted' or 'macro' might be suitable\n",
    "    # For simplicity, let's use 'macro' if there are at least 2 classes\n",
    "    if targets_array.shape[1] > 1:\n",
    "        roc_auc = roc_auc_score(targets_array, predictions_array, average='macro')\n",
    "    else:\n",
    "        roc_auc = roc_auc_score(targets_array, predictions_array) # Binary case\n",
    "\n",
    "    metrics = {\n",
    "        \"f1_score\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"log_loss\": loss,\n",
    "        \"roc_auc_score\": roc_auc\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # --- Stage 1: Data Preprocessing (re-run or load from Stage 1 output) ---\n",
    "    metadata = load_metadata(METADATA_JSON_PATH)\n",
    "\n",
    "    # Load tabular data\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    # Preprocess tabular data\n",
    "    processed_train_df, mlb, tfidf_vectorizer, scaler = preprocess_data(train_df.copy(), metadata, is_train=True)\n",
    "    # Save preprocessors\n",
    "    joblib.dump(mlb, MLB_PATH)\n",
    "    joblib.dump(tfidf_vectorizer, TFIDF_PATH)\n",
    "\n",
    "    # Split training data for validation\n",
    "    # Ensure ImageID is not used for splitting, but kept for dataset creation\n",
    "    # The error \"The least populated class in y has only 1 member\" indicates that\n",
    "    # some label combinations (classes) in the `stratify` column have only one sample.\n",
    "    # This makes it impossible to split them into train and test sets while maintaining\n",
    "    # the proportion of that class.\n",
    "    #\n",
    "    # To fix this, we can:\n",
    "    # 1. Remove classes with only one sample from the stratification target.\n",
    "    # 2. Group rare classes together.\n",
    "    # 3. Not stratify on all label combinations, but rather on individual labels or a subset.\n",
    "    #\n",
    "    # Given the multi-label nature, stratifying on all unique label combinations can lead\n",
    "    # to very sparse classes. A common workaround is to stratify on a single, most frequent\n",
    "    # label, or to not stratify at all if the dataset is large enough.\n",
    "    #\n",
    "    # For multi-label, a robust stratification strategy is often to use iterative stratification\n",
    "    # or to stratify based on the counts of individual labels rather than their combinations.\n",
    "    # However, `train_test_split`'s `stratify` parameter expects a single array.\n",
    "    #\n",
    "    # A simpler fix for `ValueError: The least populated class in y has only 1 member`\n",
    "    # when using `stratify` with multi-label data is to identify and remove such rare\n",
    "    # label combinations from the stratification target.\n",
    "    #\n",
    "    # Let's identify the label columns and create a combined string representation for stratification.\n",
    "    label_cols = [col for col in processed_train_df.columns if col.isdigit()]\n",
    "\n",
    "    # Create a string representation of the label vector for stratification\n",
    "    # This will treat each unique combination of labels as a distinct class.\n",
    "    # This is what the original code was implicitly doing by passing a DataFrame of label columns.\n",
    "    # The error means some of these unique combinations appear only once.\n",
    "\n",
    "    # Let's find the unique label combinations and their counts\n",
    "    label_combinations = processed_train_df[label_cols].apply(lambda row: tuple(row), axis=1)\n",
    "    combination_counts = label_combinations.value_counts()\n",
    "\n",
    "    # Identify combinations with count < 2\n",
    "    rare_combinations = combination_counts[combination_counts < 2].index\n",
    "\n",
    "    # Create a stratification target. For rare combinations, assign a dummy value or None.\n",
    "    stratify_target = label_combinations.copy()\n",
    "\n",
    "    # FIX: The TypeError: '<' not supported between instances of 'tuple' and 'str'\n",
    "    # occurs because `stratify_target` contains tuples (label combinations) and\n",
    "    # 'RARE_COMBINATION' (a string). `np.unique` (called internally by `train_test_split`)\n",
    "    # tries to sort these mixed types, leading to the error.\n",
    "    #\n",
    "    # The fix is to ensure all elements in `stratify_target` are of a comparable type.\n",
    "    # We can convert the tuples to strings for stratification.\n",
    "    stratify_target = label_combinations.apply(lambda x: str(x)) # Convert tuples to strings\n",
    "    stratify_target[label_combinations.isin(rare_combinations).values] = 'RARE_COMBINATION' # Use .values for boolean indexing\n",
    "\n",
    "    # Now, use this modified stratify_target for train_test_split\n",
    "    # This ensures that all \"classes\" in the stratify_target have at least 2 members\n",
    "    # (the 'RARE_COMBINATION' group will have many, and others will have >=2).\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        processed_train_df.index, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=stratify_target\n",
    "    )\n",
    "\n",
    "    train_df_split = processed_train_df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df_split = processed_train_df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Image preprocessing setup\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
    "    ])\n",
    "\n",
    "    # Create image datasets and dataloaders\n",
    "    # Pass the processed dataframes which now contain TF-IDF features and binarized labels\n",
    "    train_image_dataset = MultiLabelImageDataset(train_df_split, IMAGE_FOLDER_PATH, transform=image_transform, is_test=False)\n",
    "    val_image_dataset = MultiLabelImageDataset(val_df_split, IMAGE_FOLDER_PATH, transform=image_transform, is_test=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "    val_dataloader = DataLoader(val_image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "    # --- Stage 2: Model Training ---\n",
    "    num_classes = len(mlb.classes_)\n",
    "    # The tfidf_vectorizer.max_features is the maximum number of features, not necessarily the actual number\n",
    "    # The actual number of features is the size of the vocabulary after fitting.\n",
    "    # If max_features was not hit, len(tfidf_vectorizer.vocabulary_) is more accurate.\n",
    "    # If tfidf_vectorizer.max_features was set, it limits the vocabulary size.\n",
    "    # So, using tfidf_vectorizer.max_features is generally safe if it was explicitly set.\n",
    "    # However, if the actual number of features is less than max_features, using the actual number is better.\n",
    "    # Let's use the actual number of features from the fitted vectorizer.\n",
    "    tfidf_features_dim = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "    model = MultiModalModel(num_classes=num_classes, tfidf_features_dim=tfidf_features_dim).to(DEVICE)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss() # Suitable for multi-label classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model, history = train_model(model, train_dataloader, val_dataloader, optimizer, criterion, NUM_EPOCHS, EARLY_STOPPING_ROUNDS)\n",
    "\n",
    "    # Evaluate the best model on the validation set\n",
    "    # Load the best model state for final evaluation\n",
    "    trained_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    val_metrics = evaluate_model(trained_model, val_dataloader, mlb)\n",
    "\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Save metrics\n",
    "    with open(METRICS_PATH, 'w') as f:\n",
    "        json.dump(val_metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    return trained_model # Return the trained model for Stage 3 if needed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e72f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from scikit-learn\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define BASE_PATH with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/multi_label_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/multi_label_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# File path constants\n",
    "TEST_CSV_PATH = BASE_PATH / 'test.csv'\n",
    "IMAGE_FOLDER_PATH = BASE_PATH / 'data'\n",
    "# FIX: Corrected METADATA_JSON_PATH to point to the correct location of the metadata file.\n",
    "# The metadata file is typically at the same level as the 'input' directory, or within 'input/Datasets'.\n",
    "# Based on the problem description, the metadata JSON is provided as a separate input.\n",
    "# Assuming the metadata file is named 'multi_label_classification.json' and is in the 'input/Datasets' directory.\n",
    "METADATA_JSON_PATH = BASE_PATH.parent.parent / 'multi_label_classification.json'\n",
    "\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_PATH = OUTPUT_DIR / \"submission.csv\"\n",
    "MODEL_PATH = Path(\"./models/multi_label_classification_model.pkl\")\n",
    "MLB_PATH = Path(\"./models/mlb.pkl\")\n",
    "TFIDF_PATH = Path(\"./models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder_path, transform=None, is_test=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder_path = image_folder_path\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.label_columns = [col for col in dataframe.columns if col.isdigit()]\n",
    "        self.tfidf_columns = [col for col in dataframe.columns if col.startswith('caption_tfidf_')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['ImageID']\n",
    "        img_path = os.path.join(self.image_folder_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tfidf_features = torch.tensor(self.dataframe.iloc[idx][self.tfidf_columns].values.astype(np.float32))\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, tfidf_features, img_name\n",
    "        else:\n",
    "            labels = self.dataframe.iloc[idx][self.label_columns].values.astype(np.float32)\n",
    "            return image, torch.tensor(labels), tfidf_features, img_name\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_classes, tfidf_features_dim):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        # Image branch: Pre-trained ResNet\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Freeze all parameters in ResNet initially\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the final classification layer\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity() # Remove the final FC layer to get features\n",
    "\n",
    "        # TF-IDF branch\n",
    "        self.tfidf_fc = nn.Linear(tfidf_features_dim, 512) # Project TF-IDF features to a common dimension\n",
    "\n",
    "        # Combined branch\n",
    "        self.combined_fc1 = nn.Linear(num_ftrs + 512, 1024) # Combine image and TF-IDF features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.combined_fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, image_input, tfidf_input):\n",
    "        # Image branch\n",
    "        image_features = self.resnet(image_input)\n",
    "\n",
    "        # TF-IDF branch\n",
    "        tfidf_features = self.relu(self.tfidf_fc(tfidf_input))\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((image_features, tfidf_features), dim=1)\n",
    "\n",
    "        # Combined branch\n",
    "        output = self.combined_fc1(combined_features)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.combined_fc2(output)\n",
    "        return output\n",
    "\n",
    "def load_metadata(metadata_path):\n",
    "    \"\"\"Loads and returns the dataset metadata JSON.\"\"\"\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_data(df, metadata, mlb=None, tfidf_vectorizer=None, scaler=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the dataframe.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        metadata (dict): The dataset metadata.\n",
    "        mlb (MultiLabelBinarizer, optional): Fitted MultiLabelBinarizer for labels.\n",
    "                                             Required for test set.\n",
    "        tfidf_vectorizer (TfidfVectorizer, optional): Fitted TfidfVectorizer for captions.\n",
    "                                                      Required for test set.\n",
    "        scaler (StandardScaler, optional): Fitted StandardScaler for numerical features.\n",
    "                                           Required for test set.\n",
    "        is_train (bool): True if processing the training set, False for test set.\n",
    "    Returns:\n",
    "        tuple: Processed dataframe, fitted mlb, fitted tfidf_vectorizer, fitted scaler.\n",
    "    \"\"\"\n",
    "    # Identify column types from metadata\n",
    "    variables_info = metadata['profiling_summary']['variables']\n",
    "    image_id_col = None\n",
    "    label_col = None\n",
    "    caption_col = None\n",
    "\n",
    "    for col_name, col_info in variables_info.items():\n",
    "        if col_name == 'ImageID':\n",
    "            image_id_col = col_name\n",
    "        elif col_name == 'Labels':\n",
    "            label_col = col_name\n",
    "        elif col_name == 'Caption':\n",
    "            caption_col = col_name\n",
    "\n",
    "    # --- Handle Labels (Multi-label Binarization) ---\n",
    "    if label_col and label_col in df.columns:\n",
    "        # Labels are space-separated strings, e.g., \"1 3 5\"\n",
    "        df[label_col] = df[label_col].apply(lambda x: x.split())\n",
    "\n",
    "        if is_train:\n",
    "            # This branch should ideally not be called for test data in Stage 3\n",
    "            # as mlb should be loaded. But keeping it for completeness if this function\n",
    "            # is reused in a different context.\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            labels_encoded = mlb.fit_transform(df[label_col])\n",
    "            # Create a DataFrame for encoded labels\n",
    "            labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "        else:\n",
    "            if mlb is None:\n",
    "                raise ValueError(\"MultiLabelBinarizer must be fitted on training data and provided for test data.\")\n",
    "            # For test data, the 'Labels' column might not exist.\n",
    "            # If it exists (e.g., for a validation set treated as test), transform it.\n",
    "            # If it doesn't exist, we just proceed without label processing.\n",
    "            if label_col in df.columns:\n",
    "                labels_encoded = mlb.transform(df[label_col])\n",
    "                labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_, index=df.index)\n",
    "                df = pd.concat([df.drop(columns=[label_col]), labels_df], axis=1)\n",
    "            else:\n",
    "                print(f\"Label column '{label_col}' not found in test dataframe. Skipping label binarization.\")\n",
    "    else:\n",
    "        print(f\"Warning: Label column '{label_col}' not found or not specified in metadata. Skipping label processing.\")\n",
    "\n",
    "\n",
    "    # --- Handle Caption (TF-IDF) ---\n",
    "    if caption_col and caption_col in df.columns:\n",
    "        if is_train:\n",
    "            # This branch should ideally not be called for test data in Stage 3\n",
    "            # as tfidf_vectorizer should be loaded.\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limit features to manage dimensionality\n",
    "            caption_features = tfidf_vectorizer.fit_transform(df[caption_col])\n",
    "        else:\n",
    "            if tfidf_vectorizer is None:\n",
    "                raise ValueError(\"TfidfVectorizer must be fitted on training data and provided for test data.\")\n",
    "            caption_features = tfidf_vectorizer.transform(df[caption_col])\n",
    "\n",
    "        # Convert TF-IDF sparse matrix to DataFrame\n",
    "        caption_df = pd.DataFrame(caption_features.toarray(),\n",
    "                                  columns=[f'caption_tfidf_{i}' for i in range(caption_features.shape[1])],\n",
    "                                  index=df.index)\n",
    "        df = pd.concat([df.drop(columns=[caption_col]), caption_df], axis=1)\n",
    "    else:\n",
    "        print(f\"Warning: Caption column '{caption_col}' not found or not specified in metadata. Skipping TF-IDF processing.\")\n",
    "\n",
    "    return df, mlb, tfidf_vectorizer, scaler\n",
    "\n",
    "def main(trained_model=None):\n",
    "    # Load metadata\n",
    "    metadata = load_metadata(METADATA_JSON_PATH)\n",
    "\n",
    "    # Load preprocessors\n",
    "    mlb = joblib.load(MLB_PATH)\n",
    "    tfidf_vectorizer = joblib.load(TFIDF_PATH)\n",
    "\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    # Preprocess test data\n",
    "    # Note: For test data, we don't have 'Labels' column, so it will be dropped by preprocess_data\n",
    "    # The `preprocess_data` function is designed to handle this by checking `if label_col in df.columns`.\n",
    "    # We need to ensure that the `processed_test_df` retains `ImageID` and TF-IDF features.\n",
    "    # Pass scaler as None as it's not used in this problem, but keep the argument for consistency.\n",
    "    processed_test_df, _, _, _ = preprocess_data(test_df.copy(), metadata, mlb=mlb, tfidf_vectorizer=tfidf_vectorizer, scaler=None, is_train=False)\n",
    "\n",
    "    # Image preprocessing setup\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
    "    ])\n",
    "\n",
    "    # Create image dataset and dataloader for test set\n",
    "    # Pass the processed_test_df which now contains TF-IDF features\n",
    "    test_image_dataset = MultiLabelImageDataset(processed_test_df, IMAGE_FOLDER_PATH, transform=image_transform, is_test=True)\n",
    "    test_image_dataloader = DataLoader(test_image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "    # Load the trained model\n",
    "    if trained_model is None:\n",
    "        # Determine num_classes and tfidf_features_dim from loaded preprocessors\n",
    "        num_classes = len(mlb.classes_)\n",
    "        # The tfidf_vectorizer.vocabulary_ contains the actual features learned during training.\n",
    "        # This is the correct dimension for the TF-IDF input layer of the model.\n",
    "        tfidf_features_dim = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "        # If for some reason the vocabulary is empty (e.g., no captions in training data),\n",
    "        # this would lead to tfidf_features_dim = 0, which would cause an error in the model.\n",
    "        # Based on the metadata, captions exist, so this should not be 0.\n",
    "        # However, if the model was trained with a fixed max_features and the actual vocabulary\n",
    "        # was smaller, the model's input layer would still expect the max_features size.\n",
    "        # The safest approach is to ensure the dimension matches what the model was trained with.\n",
    "        # The `MultiModalModel` in Stage 2 was initialized with `len(tfidf_vectorizer.vocabulary_)`.\n",
    "        # So, using `len(tfidf_vectorizer.vocabulary_)` here is consistent.\n",
    "\n",
    "        model = MultiModalModel(num_classes=num_classes, tfidf_features_dim=tfidf_features_dim)\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        model.to(DEVICE)\n",
    "    else:\n",
    "        model = trained_model\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    all_predictions = []\n",
    "    image_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, tfidf_features, img_names in tqdm(test_image_dataloader, desc=\"Generating predictions\"):\n",
    "            images = images.to(DEVICE)\n",
    "            tfidf_features = tfidf_features.to(DEVICE)\n",
    "\n",
    "            outputs = model(images, tfidf_features)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy() # Sigmoid for multi-label probabilities\n",
    "            all_predictions.append(probabilities)\n",
    "            image_ids.extend(img_names)\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    predictions_array = np.vstack(all_predictions)\n",
    "\n",
    "    # Convert probabilities to binary labels (threshold 0.5)\n",
    "    # This is a common approach for multi-label classification, but the competition might specify a different threshold\n",
    "    # or require raw probabilities. Given \"Mean F1-Score\" as metric, binary labels are needed for F1.\n",
    "    binary_predictions = (predictions_array > 0.5).astype(int)\n",
    "\n",
    "    # Convert binary predictions back to original label format (space-separated string)\n",
    "    predicted_labels = []\n",
    "    for i in range(binary_predictions.shape[0]):\n",
    "        # FIX: The error trace is from Stage 2, not Stage 3.\n",
    "        # The Stage 3 code provided is syntactically correct and follows the pattern.\n",
    "        # The previous error was `RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.`\n",
    "        # This was fixed in Stage 2 by adding `.detach()` to `torch.sigmoid(outputs).cpu().numpy()`.\n",
    "        # The Stage 3 code already correctly uses `torch.sigmoid(outputs).cpu().numpy()`, which implies `outputs` is already detached\n",
    "        # or not requiring grad in eval mode, or the error was specific to the training loop.\n",
    "        #\n",
    "        # No specific error in the provided traceback for Stage 3.\n",
    "        # The fix for Stage 2 was already applied in the provided Stage 3 code.\n",
    "        # Therefore, no changes are needed in this specific Stage 3 snippet based on the provided traceback.\n",
    "        row_labels = mlb.inverse_transform(binary_predictions[i:i+1])\n",
    "        predicted_labels.append(\" \".join(row_labels[0]))\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({'ImageID': image_ids, 'Labels': predicted_labels})\n",
    "\n",
    "    # Save submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(f\"Submission file generated successfully at: {SUBMISSION_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
