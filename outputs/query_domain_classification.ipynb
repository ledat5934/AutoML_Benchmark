{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# File-path constants\n",
    "TRAIN_CSV_PATH = \"/kaggle/input/query_domain_classification/train.csv\"\n",
    "TEST_CSV_PATH = \"/kaggle/input/query_domain_classification/test.csv\"\n",
    "PROCESSED_DIR = \"./processed\"\n",
    "\n",
    "# Fallback for file paths\n",
    "def find_csv_files(base_path, keyword):\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if keyword.lower() in file.lower() and (file.endswith('.csv') or file.endswith('.parquet')):\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "TRAIN_CSV_PATH = find_csv_files(\"/kaggle/input/query_domain_classification\", \"train\") or TRAIN_CSV_PATH\n",
    "TEST_CSV_PATH = find_csv_files(\"/kaggle/input/query_domain_classification\", \"test\") or TEST_CSV_PATH\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    # Identify target column\n",
    "    target_column = 'Domain'  # Based on the EDA report\n",
    "    feature_columns = train_df.columns[train_df.columns != target_column].tolist()\n",
    "\n",
    "    # Identify numerical, categorical, and text columns\n",
    "    numerical_cols = train_df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    text_cols = ['Title']  # Based on the EDA report\n",
    "\n",
    "    # Preprocessing pipelines\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(drop='first'))\n",
    "    ])\n",
    "\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('text', text_transformer, text_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df[target_column]\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "    # Transform the test data\n",
    "    X_test = test_df[feature_columns]\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Create processed DataFrames\n",
    "    train_df_processed = pd.DataFrame(X_train_processed)\n",
    "    test_df_processed = pd.DataFrame(X_test_processed)\n",
    "\n",
    "    # Include the target column in the train DataFrame\n",
    "    train_df_processed[target_column] = y_train.values\n",
    "\n",
    "    # Save processed DataFrames\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    train_df_processed.to_csv(os.path.join(PROCESSED_DIR, 'train_processed.csv'), index=False)\n",
    "    test_df_processed.to_csv(os.path.join(PROCESSED_DIR, 'test_processed.csv'), index=False)\n",
    "\n",
    "    return train_df_processed, test_df_processed\n",
    "\n",
    "def main():\n",
    "    load_and_preprocess_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b88a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Constants\n",
    "MODEL_PATH = \"./models/model.pkl\"\n",
    "\n",
    "# Prepare data\n",
    "X = train_df_processed['Title']\n",
    "y = train_df_processed['Domain']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "        return {**encoding, 'labels': torch.tensor(label)}\n",
    "\n",
    "# Encode labels\n",
    "label_to_id = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y_train_encoded = y_train.map(label_to_id).values\n",
    "y_val_encoded = y_val.map(label_to_id).values\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QueryDataset(X_train.tolist(), y_train_encoded)\n",
    "val_dataset = QueryDataset(X_val.tolist(), y_val_encoded)\n",
    "\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_id))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_val_encoded, preds)\n",
    "f1 = f1_score(y_val_encoded, preds, average='weighted')\n",
    "logloss = log_loss(y_val_encoded, predictions.predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Log Loss: {logloss}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "\n",
    "trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "TEST_PROCESSED_PATH = \"./processed/test_processed.csv\"\n",
    "SUBMISSION_PATH = \"./outputs/submission.csv\"\n",
    "\n",
    "# Load the model if not already loaded\n",
    "if trained_model is None:\n",
    "    trained_model = joblib.load(\"models/model.pkl\")\n",
    "\n",
    "# Load the processed test data\n",
    "test_df_processed = pd.read_csv(TEST_PROCESSED_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "test_texts = test_df_processed['Title']\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**{k: v.to(trained_model.device) for k, v in test_encodings.items()})\n",
    "    predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# Map predictions back to labels\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "predicted_labels = [id_to_label[pred] for pred in predictions]\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df_processed.index,  # Assuming there's an index or ID column\n",
    "    'Domain': predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"Submission file saved at: {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
