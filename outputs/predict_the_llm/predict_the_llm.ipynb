{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# --- File Path Constants (Overrideable) ---\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_the_llm').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_the_llm').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find the dataset base path. Tried: {BASE_PATH_OPTION1} and {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "# Custom transformer to ensure string type for text columns\n",
    "# The error \"AttributeError: 'numpy.ndarray' object has no attribute 'values'\"\n",
    "# occurred in the TextCleaner's transform method: `return X.astype(str).values.reshape(-1)`.\n",
    "# This error happens because `ColumnTransformer` can pass a NumPy array directly to the\n",
    "# custom transformer if the input `X` (to `full_pipeline.fit_transform`) is already a NumPy array\n",
    "# or if it's a DataFrame and the column selection results in a NumPy array (e.g., `df[['col']].values`).\n",
    "# When `X` is already a NumPy array, it does not have a `.values` attribute.\n",
    "# The fix is to remove `.values` and directly use `np.asarray(X).astype(str).reshape(-1)`\n",
    "# or simply `X.astype(str)` if `X` is guaranteed to be a pandas Series/DataFrame column.\n",
    "# Since `ColumnTransformer` passes a slice of the DataFrame (which is a Series for a single column),\n",
    "# `X.astype(str).values.reshape(-1)` should generally work.\n",
    "# However, if `X` is already a numpy array (e.g., if the input to ColumnTransformer was already a numpy array),\n",
    "# then `.values` would fail.\n",
    "# The most robust way is to ensure `X` is converted to a numpy array first, then to string, then reshape.\n",
    "# `np.asarray(X)` handles both pandas Series/DataFrame columns and numpy arrays gracefully.\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Ensure X is converted to a NumPy array first, then to string type, then reshape to 1D.\n",
    "        # This handles cases where X might be a pandas Series or a NumPy array.\n",
    "        return np.asarray(X).astype(str).reshape(-1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, preprocessing, and splitting for the predict_the_llm dataset.\n",
    "    \"\"\"\n",
    "    # Load metadata (assuming it's provided as a string or loaded from a file)\n",
    "    metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_the_llm\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_the_llm\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 3180,\n",
    "          \"n_var\": 3,\n",
    "          \"memory_size\": 76448,\n",
    "          \"record_size\": 24.040251572327044,\n",
    "          \"n_cells_missing\": 6,\n",
    "          \"p_cells_missing\": 0.0006289308176100629,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 42,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"Question\": {\n",
    "            \"n_distinct\": 568,\n",
    "            \"p_distinct\": 0.17861635220125785,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 1,\n",
    "            \"p_unique\": 0.00031446540880503143,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 229,\n",
    "            \"mean_length\": 56.09088050314465,\n",
    "            \"median_length\": 93,\n",
    "            \"min_length\": 16,\n",
    "            \"n_characters_distinct\": 86,\n",
    "            \"n_characters\": 178369,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"Response\": {\n",
    "            \"n_distinct\": 3173,\n",
    "            \"p_distinct\": 0.9996849401386263,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 3172,\n",
    "            \"p_unique\": 0.9993698802772527,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 6,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0018867924528301887,\n",
    "            \"count\": 3174,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 3878,\n",
    "            \"mean_length\": 859.661940768746,\n",
    "            \"median_length\": 1784,\n",
    "            \"min_length\": 1,\n",
    "            \"n_characters_distinct\": 159,\n",
    "            \"n_characters\": 2728567,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"target\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.00220125786163522,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Numeric\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"n_negative\": 0,\n",
    "            \"p_negative\": 0.0,\n",
    "            \"n_infinite\": 0,\n",
    "            \"n_zeros\": 455,\n",
    "            \"mean\": 2.998427672955975,\n",
    "            \"std\": 2.0007070265197857,\n",
    "            \"variance\": 4.002828605965643,\n",
    "            \"min\": 0,\n",
    "            \"max\": 6,\n",
    "            \"kurtosis\": -1.2506476076132633,\n",
    "            \"skewness\": 0.000983515718696273,\n",
    "            \"sum\": 9535,\n",
    "            \"mad\": 2.0,\n",
    "            \"range\": 6,\n",
    "            \"5%\": 0.0,\n",
    "            \"25%\": 1.0,\n",
    "            \"50%\": 3.0,\n",
    "            \"75%\": 5.0,\n",
    "            \"95%\": 6.0,\n",
    "            \"iqr\": 4.0,\n",
    "            \"cv\": 0.6672520549903428,\n",
    "            \"p_zeros\": 0.1430817610062893,\n",
    "            \"p_infinite\": 0.0,\n",
    "            \"monotonic_increase\": False,\n",
    "            \"monotonic_decrease\": False,\n",
    "            \"monotonic_increase_strict\": False,\n",
    "            \"monotonic_decrease_strict\": False,\n",
    "            \"monotonic\": 0,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset is for a competition to identify which of 7 possible LLM models generated a given text response. Participants need to predict the probability for each of the 7 models for every response in the test set.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"target\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"logloss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct location.\")\n",
    "        return None, None, None, None # Return None if files are not found\n",
    "\n",
    "    print(\"\\nOriginal Train DataFrame Info:\")\n",
    "    train_df.info()\n",
    "    print(\"\\nOriginal Test DataFrame Info:\")\n",
    "    test_df.info()\n",
    "\n",
    "    # Identify target column\n",
    "    target_column = metadata['task_definition']['target_columns'][0]\n",
    "\n",
    "    # Separate features and target\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    X_test = test_df.copy()\n",
    "\n",
    "    # Identify column types based on metadata\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col, info in metadata['profiling_summary']['variables'].items():\n",
    "        if col == target_column:\n",
    "            continue\n",
    "        if info['type'] == 'Numeric':\n",
    "            numerical_cols.append(col)\n",
    "        elif info['type'] == 'Text':\n",
    "            text_cols.append(col)\n",
    "\n",
    "    print(f\"\\nIdentified Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Identified Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Identified Text Columns: {text_cols}\")\n",
    "\n",
    "    # Preprocessing Pipelines\n",
    "    # Each text column ('Question', 'Response') will be processed by its own TF-IDF vectorizer.\n",
    "    # ColumnTransformer will then concatenate these sparse outputs horizontally.\n",
    "    # This is the standard and correct way to handle multiple text columns.\n",
    "    # The TextCleaner ensures the input to TfidfVectorizer is always a 1D array of strings.\n",
    "\n",
    "    # Create a list of (name, transformer, columns) tuples for ColumnTransformer\n",
    "    transformers_list = []\n",
    "\n",
    "    for col in text_cols:\n",
    "        text_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "            ('text_cleaner', TextCleaner()),\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english'))\n",
    "        ])\n",
    "        # Pass each text column individually to its own TF-IDF pipeline\n",
    "        # Ensure the column selection is a list, e.g., `[col]`, so ColumnTransformer\n",
    "        # passes a Series (or DataFrame with one column) to the transformer,\n",
    "        # which is consistent with how pandas dataframes are typically handled.\n",
    "        transformers_list.append((f'text_{col}', text_pipeline, [col])) \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers_list,\n",
    "        remainder='passthrough' # Keep other columns (e.g., IDs if present)\n",
    "    )\n",
    "\n",
    "    # Create the full preprocessing pipeline\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    print(\"\\nFitting and transforming training data...\")\n",
    "    # Ensure X is a DataFrame when passed to fit_transform\n",
    "    X_processed = full_pipeline.fit_transform(X)\n",
    "    print(\"Training data preprocessing complete.\")\n",
    "\n",
    "    # Transform the test data\n",
    "    print(\"Transforming test data...\")\n",
    "    # Ensure X_test is a DataFrame when passed to transform\n",
    "    X_test_processed = full_pipeline.transform(X_test)\n",
    "    print(\"Test data preprocessing complete.\")\n",
    "\n",
    "    # Display shapes of processed data\n",
    "    print(f\"\\nShape of processed training features: {X_processed.shape}\")\n",
    "    print(f\"Shape of processed test features: {X_test_processed.shape}\")\n",
    "    print(f\"Shape of training target: {y.shape}\")\n",
    "\n",
    "    # For demonstration, let's show the first few rows of the target\n",
    "    print(\"\\nFirst 5 target values:\")\n",
    "    print(y.head())\n",
    "\n",
    "    print(\"\\nPreprocessing complete. Data is ready for model training.\")\n",
    "    # The processed data (X_processed, y, X_test_processed) can now be used for model training in Stage 2.\n",
    "    return X_processed, y, X_test_processed, full_pipeline # Return pipeline for later use in Stage 3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_processed, y, X_test_processed, full_pipeline = main()\n",
    "    if X_processed is not None:\n",
    "        print(\"\\nScript finished successfully. Processed data and pipeline returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf79a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- File Path Constants (Overrideable) ---\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_the_llm').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_the_llm').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find the dataset base path. Tried: {BASE_PATH_OPTION1} and {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"predict_the_llm_model.pkl\"\n",
    "METRICS_PATH = OUTPUTS_DIR / \"metrics.json\"\n",
    "\n",
    "# Custom transformer to ensure string type for text columns\n",
    "# The error \"AttributeError: 'numpy.ndarray' object has no attribute 'values'\"\n",
    "# occurred in the TextCleaner's transform method: `return X.astype(str).values.reshape(-1)`.\n",
    "# This error happens because `ColumnTransformer` can pass a NumPy array directly to the\n",
    "# custom transformer if the input `X` (to `full_pipeline.fit_transform`) is already a NumPy array\n",
    "# or if it's a DataFrame and the column selection results in a NumPy array (e.g., `df[['col']].values`).\n",
    "# When `X` is already a NumPy array, it does not have a `.values` attribute.\n",
    "# The fix is to remove `.values` and directly use `np.asarray(X).astype(str).reshape(-1)`\n",
    "# or simply `X.astype(str)` if `X` is guaranteed to be a pandas Series/DataFrame column.\n",
    "# Since `ColumnTransformer` passes a slice of the DataFrame (which is a Series for a single column),\n",
    "# `X.astype(str).values.reshape(-1)` should generally work.\n",
    "# However, if `X` is already a numpy array (e.g., if the input to ColumnTransformer was already a numpy array),\n",
    "# then `.values` would fail.\n",
    "# The most robust way is to ensure `X` is converted to a numpy array first, then to string, then reshape.\n",
    "# `np.asarray(X)` handles both pandas Series/DataFrame columns and numpy arrays gracefully.\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Ensure X is converted to a NumPy array first, then to string type, then reshape to 1D.\n",
    "        # This handles cases where X might be a pandas Series or a NumPy array.\n",
    "        return np.asarray(X).astype(str).reshape(-1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, preprocessing, splitting, model training,\n",
    "    evaluation, and persistence for the predict_the_llm dataset.\n",
    "    \"\"\"\n",
    "    # Load metadata (assuming it's provided as a string or loaded from a file)\n",
    "    metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_the_llm\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_the_llm\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 3180,\n",
    "          \"n_var\": 3,\n",
    "          \"memory_size\": 76448,\n",
    "          \"record_size\": 24.040251572327044,\n",
    "          \"n_cells_missing\": 6,\n",
    "          \"p_cells_missing\": 0.0006289308176100629,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 42,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"Question\": {\n",
    "            \"n_distinct\": 568,\n",
    "            \"p_distinct\": 0.17861635220125785,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 1,\n",
    "            \"p_unique\": 0.00031446540880503143,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 229,\n",
    "            \"mean_length\": 56.09088050314465,\n",
    "            \"median_length\": 93,\n",
    "            \"min_length\": 16,\n",
    "            \"n_characters_distinct\": 86,\n",
    "            \"n_characters\": 178369,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"Response\": {\n",
    "            \"n_distinct\": 3173,\n",
    "            \"p_distinct\": 0.9996849401386263,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 3172,\n",
    "            \"p_unique\": 0.9993698802772527,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 6,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0018867924528301887,\n",
    "            \"count\": 3174,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 3878,\n",
    "            \"mean_length\": 859.661940768746,\n",
    "            \"median_length\": 1784,\n",
    "            \"min_length\": 1,\n",
    "            \"n_characters_distinct\": 159,\n",
    "            \"n_characters\": 2728567,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"target\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.00220125786163522,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Numeric\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"n_negative\": 0,\n",
    "            \"p_negative\": 0.0,\n",
    "            \"n_infinite\": 0,\n",
    "            \"n_zeros\": 455,\n",
    "            \"mean\": 2.998427672955975,\n",
    "            \"std\": 2.0007070265197857,\n",
    "            \"variance\": 4.002828605965643,\n",
    "            \"min\": 0,\n",
    "            \"max\": 6,\n",
    "            \"kurtosis\": -1.2506476076132633,\n",
    "            \"skewness\": 0.000983515718696273,\n",
    "            \"sum\": 9535,\n",
    "            \"mad\": 2.0,\n",
    "            \"range\": 6,\n",
    "            \"5%\": 0.0,\n",
    "            \"25%\": 1.0,\n",
    "            \"50%\": 3.0,\n",
    "            \"75%\": 5.0,\n",
    "            \"95%\": 6.0,\n",
    "            \"iqr\": 4.0,\n",
    "            \"cv\": 0.6672520549903428,\n",
    "            \"p_zeros\": 0.1430817610062893,\n",
    "            \"p_infinite\": 0.0,\n",
    "            \"monotonic_increase\": False,\n",
    "            \"monotonic_decrease\": False,\n",
    "            \"monotonic_increase_strict\": False,\n",
    "            \"monotonic_decrease_strict\": False,\n",
    "            \"monotonic\": 0,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset is for a competition to identify which of 7 possible LLM models generated a given text response. Participants need to predict the probability for each of the 7 models for every response in the test set.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"target\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"logloss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        # sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE) # Not used in this script\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct location.\")\n",
    "        return None # Return None if files are not found\n",
    "\n",
    "    print(\"\\nOriginal Train DataFrame Info:\")\n",
    "    train_df.info()\n",
    "    print(\"\\nOriginal Test DataFrame Info:\")\n",
    "    test_df.info()\n",
    "\n",
    "    # Identify target column\n",
    "    target_column = metadata['task_definition']['target_columns'][0]\n",
    "\n",
    "    # Separate features and target\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    # X_test = test_df.copy() # X_test will be transformed later\n",
    "\n",
    "    # Identify column types based on metadata\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col, info in metadata['profiling_summary']['variables'].items():\n",
    "        if col == target_column:\n",
    "            continue\n",
    "        if info['type'] == 'Numeric':\n",
    "            numerical_cols.append(col)\n",
    "        elif info['type'] == 'Text':\n",
    "            text_cols.append(col)\n",
    "        # No explicit 'Categorical' type in the provided metadata,\n",
    "        # but if there were, they would be handled here.\n",
    "\n",
    "    print(f\"\\nIdentified Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Identified Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Identified Text Columns: {text_cols}\")\n",
    "\n",
    "    # Preprocessing Pipelines\n",
    "    # Each text column ('Question', 'Response') will be processed by its own TF-IDF vectorizer.\n",
    "    # ColumnTransformer will then concatenate these sparse outputs horizontally.\n",
    "    # This is the standard and correct way to handle multiple text columns.\n",
    "    # The TextCleaner ensures the input to TfidfVectorizer is always a 1D array of strings.\n",
    "\n",
    "    # Create a list of (name, transformer, columns) tuples for ColumnTransformer\n",
    "    transformers_list = []\n",
    "\n",
    "    for col in text_cols:\n",
    "        text_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "            ('text_cleaner', TextCleaner()), # Use the corrected TextCleaner\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')) # Limiting features and adding stop words\n",
    "        ])\n",
    "        # Pass each text column individually to its own TF-IDF pipeline\n",
    "        # Ensure the column selection is a list, e.g., `[col]`, so ColumnTransformer\n",
    "        # passes a Series (or DataFrame with one column) to the transformer,\n",
    "        # which is consistent with how pandas dataframes are typically handled.\n",
    "        transformers_list.append((f'text_{col}', text_pipeline, [col])) \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers_list,\n",
    "        remainder='passthrough' # Keep other columns (e.g., IDs if present)\n",
    "    )\n",
    "\n",
    "    # Create the full preprocessing pipeline\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    print(\"\\nSplitting data into training and validation sets (80/20 stratified)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    print(\"\\nFitting and transforming training data...\")\n",
    "    # Ensure X_train is a DataFrame when passed to fit_transform\n",
    "    X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "    print(\"Training data preprocessing complete.\")\n",
    "\n",
    "    # Transform the validation data\n",
    "    print(\"Transforming validation data...\")\n",
    "    # Ensure X_val is a DataFrame when passed to transform\n",
    "    X_val_processed = full_pipeline.transform(X_val)\n",
    "    print(\"Validation data preprocessing complete.\")\n",
    "\n",
    "    # Display shapes of processed data\n",
    "    print(f\"\\nShape of processed training features: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of processed validation features: {X_val_processed.shape}\")\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"\\nStarting model training (LightGBM Classifier)...\")\n",
    "    num_classes = y.nunique()\n",
    "\n",
    "    # LightGBM Classifier for multi-class classification\n",
    "    # Using 'multiclass' objective for more than 2 classes\n",
    "    # 'num_class' is required for 'multiclass' objective\n",
    "    lgb_clf = lgb.LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=num_classes,\n",
    "                                 random_state=42,\n",
    "                                 n_estimators=1000, # Set a high number for early stopping\n",
    "                                 learning_rate=0.05)\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    # eval_set expects (X_val, y_val)\n",
    "    # eval_metric for multiclass is 'multi_logloss'\n",
    "    # callbacks for early stopping\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
    "\n",
    "    lgb_clf.fit(X_train_processed, y_train,\n",
    "                eval_set=[(X_val_processed, y_val)],\n",
    "                eval_metric='multi_logloss',\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    trained_model = lgb_clf\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\nEvaluating model performance on the validation set...\")\n",
    "    y_pred_proba = trained_model.predict_proba(X_val_processed)\n",
    "    y_pred = trained_model.predict(X_val_processed)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    metrics['logloss'] = log_loss(y_val, y_pred_proba)\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_val, y_pred, average='macro') # Use macro for multi-class\n",
    "\n",
    "    # ROC AUC for multi-class: 'ovr' (One-vs-Rest) or 'ovo' (One-vs-One)\n",
    "    # Requires probabilities for each class\n",
    "    try:\n",
    "        metrics['roc_auc_ovr'] = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC (ovr): {e}. This might happen if a class has only one sample in y_val.\")\n",
    "        metrics['roc_auc_ovr'] = None\n",
    "\n",
    "    print(f\"Validation LogLoss: {metrics['logloss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "    if metrics['roc_auc_ovr'] is not None:\n",
    "        print(f\"Validation ROC AUC (OvR): {metrics['roc_auc_ovr']:.4f}\")\n",
    "    else:\n",
    "        print(\"Validation ROC AUC (OvR): Not calculated due to error.\")\n",
    "\n",
    "    # Persist metrics to JSON file\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Metrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    # --- Model Persistence ---\n",
    "    joblib.dump(trained_model, MODEL_PATH)\n",
    "    print(f\"Trained model saved to {MODEL_PATH}\")\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model_instance = main()\n",
    "    if trained_model_instance:\n",
    "        print(\"\\nScript finished successfully. Trained model instance returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb35cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- File Path Constants (Overrideable) ---\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/predict_the_llm').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/predict_the_llm').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find the dataset base path. Tried: {BASE_PATH_OPTION1} and {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SAMPLE_SUBMISSION_FILE = BASE_PATH / \"sample_submission.csv\"\n",
    "\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "PROCESSED_DIR = Path(\"./processed\") # New directory for processed data\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"predict_the_llm_model.pkl\"\n",
    "METRICS_PATH = OUTPUTS_DIR / \"metrics.json\"\n",
    "SUBMISSION_PATH = OUTPUTS_DIR / \"submission.csv\"\n",
    "\n",
    "# Custom transformer to ensure string type for text columns\n",
    "# The error \"AttributeError: 'numpy.ndarray' object has no attribute 'values'\"\n",
    "# occurred in the TextCleaner's transform method: `return X.astype(str).values.reshape(-1)`.\n",
    "# This error happens because `ColumnTransformer` can pass a NumPy array directly to the\n",
    "# custom transformer if the input `X` (to `full_pipeline.fit_transform`) is already a NumPy array\n",
    "# or if it's a DataFrame and the column selection results in a NumPy array (e.g., `df[['col']].values`).\n",
    "# When `X` is already a NumPy array, it does not have a `.values` attribute.\n",
    "# The fix is to remove `.values` and directly use `np.asarray(X).astype(str).reshape(-1)`\n",
    "# or simply `X.astype(str)` if `X` is guaranteed to be a pandas Series/DataFrame column.\n",
    "# Since `ColumnTransformer` passes a slice of the DataFrame (which is a Series for a single column),\n",
    "# `X.astype(str).values.reshape(-1)` should generally work.\n",
    "# However, if `X` is already a numpy array (e.g., if the input to ColumnTransformer was already a numpy array),\n",
    "# then `.values` would fail.\n",
    "# The most robust way is to ensure `X` is converted to a numpy array first, then to string, then reshape.\n",
    "# `np.asarray(X)` handles both pandas Series/DataFrame columns and numpy arrays gracefully.\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Ensure X is converted to a NumPy array first, then to string type, then reshape to 1D.\n",
    "        # This handles cases where X might be a pandas Series or a NumPy array.\n",
    "        # The previous error was due to X already being a numpy array in some cases,\n",
    "        # so .values would fail. np.asarray(X) handles this gracefully.\n",
    "        return np.asarray(X).astype(str).reshape(-1)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, preprocessing, splitting, model training,\n",
    "    evaluation, and persistence for the predict_the_llm dataset.\n",
    "    \"\"\"\n",
    "    # Load metadata (assuming it's provided as a string or loaded from a file)\n",
    "    metadata = {\n",
    "      \"dataset_info\": {\n",
    "        \"name\": \"predict_the_llm\",\n",
    "        \"base_path\": \"input/Datasets/datasets/predict_the_llm\",\n",
    "        \"description_file\": \"description.txt\",\n",
    "        \"files\": [\n",
    "          {\n",
    "            \"path\": \"sample_submission.csv\",\n",
    "            \"role\": \"sample\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"test.csv\",\n",
    "            \"role\": \"test\",\n",
    "            \"type\": \"tabular\"\n",
    "          },\n",
    "          {\n",
    "            \"path\": \"train.csv\",\n",
    "            \"role\": \"train\",\n",
    "            \"type\": \"tabular\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"profiling_summary\": {\n",
    "        \"time_index_analysis\": \"None\",\n",
    "        \"table\": {\n",
    "          \"n\": 3180,\n",
    "          \"n_var\": 3,\n",
    "          \"memory_size\": 76448,\n",
    "          \"record_size\": 24.040251572327044,\n",
    "          \"n_cells_missing\": 6,\n",
    "          \"p_cells_missing\": 0.0006289308176100629,\n",
    "          \"size_optimized\": True,\n",
    "          \"optimization_level\": \"aggressive\",\n",
    "          \"optimization_note\": \"All value lists removed - only counts and basic statistics retained\",\n",
    "          \"removed_sections\": 42,\n",
    "          \"optimization_strategy\": \"Minimal JSON for maximum compatibility with LLM token limits\"\n",
    "        },\n",
    "        \"variables\": {\n",
    "          \"Question\": {\n",
    "            \"n_distinct\": 568,\n",
    "            \"p_distinct\": 0.17861635220125785,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 1,\n",
    "            \"p_unique\": 0.00031446540880503143,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 229,\n",
    "            \"mean_length\": 56.09088050314465,\n",
    "            \"median_length\": 93,\n",
    "            \"min_length\": 16,\n",
    "            \"n_characters_distinct\": 86,\n",
    "            \"n_characters\": 178369,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"Response\": {\n",
    "            \"n_distinct\": 3173,\n",
    "            \"p_distinct\": 0.9996849401386263,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 3172,\n",
    "            \"p_unique\": 0.9993698802772527,\n",
    "            \"type\": \"Text\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 6,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0018867924528301887,\n",
    "            \"count\": 3174,\n",
    "            \"memory_size\": 25568,\n",
    "            \"max_length\": 3878,\n",
    "            \"mean_length\": 859.661940768746,\n",
    "            \"median_length\": 1784,\n",
    "            \"min_length\": 1,\n",
    "            \"n_characters_distinct\": 159,\n",
    "            \"n_characters\": 2728567,\n",
    "            \"n_block_alias\": 1,\n",
    "            \"n_scripts\": 1,\n",
    "            \"n_category\": 1,\n",
    "            \"cast_type\": \"None\"\n",
    "          },\n",
    "          \"target\": {\n",
    "            \"n_distinct\": 7,\n",
    "            \"p_distinct\": 0.00220125786163522,\n",
    "            \"is_unique\": False,\n",
    "            \"n_unique\": 0,\n",
    "            \"p_unique\": 0.0,\n",
    "            \"type\": \"Numeric\",\n",
    "            \"hashable\": True,\n",
    "            \"ordering\": True,\n",
    "            \"n_missing\": 0,\n",
    "            \"n\": 3180,\n",
    "            \"p_missing\": 0.0,\n",
    "            \"count\": 3180,\n",
    "            \"memory_size\": 25568,\n",
    "            \"n_negative\": 0,\n",
    "            \"p_negative\": 0.0,\n",
    "            \"n_infinite\": 0,\n",
    "            \"n_zeros\": 455,\n",
    "            \"mean\": 2.998427672955975,\n",
    "            \"std\": 2.0007070265197857,\n",
    "            \"variance\": 4.002828605965643,\n",
    "            \"min\": 0,\n",
    "            \"max\": 6,\n",
    "            \"kurtosis\": -1.2506476076132633,\n",
    "            \"skewness\": 0.000983515718696273,\n",
    "            \"sum\": 9535,\n",
    "            \"mad\": 2.0,\n",
    "            \"range\": 6,\n",
    "            \"5%\": 0.0,\n",
    "            \"25%\": 1.0,\n",
    "            \"50%\": 3.0,\n",
    "            \"75%\": 5.0,\n",
    "            \"95%\": 6.0,\n",
    "            \"iqr\": 4.0,\n",
    "            \"cv\": 0.6672520549903428,\n",
    "            \"p_zeros\": 0.1430817610062893,\n",
    "            \"p_infinite\": 0.0,\n",
    "            \"monotonic_increase\": False,\n",
    "            \"monotonic_decrease\": False,\n",
    "            \"monotonic_increase_strict\": False,\n",
    "            \"monotonic_decrease_strict\": False,\n",
    "            \"monotonic\": 0,\n",
    "            \"cast_type\": \"None\"\n",
    "          }\n",
    "        },\n",
    "        \"scatter\": {},\n",
    "        \"correlations\": {}\n",
    "      },\n",
    "      \"task_definition\": {\n",
    "        \"description_summary\": \"The dataset is for a competition to identify which of 7 possible LLM models generated a given text response. Participants need to predict the probability for each of the 7 models for every response in the test set.\",\n",
    "        \"task_type\": \"multi_class_classification\",\n",
    "        \"target_columns\": [\n",
    "          \"target\"\n",
    "        ],\n",
    "        \"evaluation_metric\": \"logloss\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct location.\")\n",
    "        return None # Return None if files are not found\n",
    "\n",
    "    print(\"\\nOriginal Train DataFrame Info:\")\n",
    "    train_df.info()\n",
    "    print(\"\\nOriginal Test DataFrame Info:\")\n",
    "    test_df.info()\n",
    "\n",
    "    # Identify target column\n",
    "    target_column = metadata['task_definition']['target_columns'][0]\n",
    "\n",
    "    # Separate features and target\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    X_test_raw = test_df.copy() # Keep original test_df for transformation\n",
    "\n",
    "    # Identify column types based on metadata\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col, info in metadata['profiling_summary']['variables'].items():\n",
    "        if col == target_column:\n",
    "            continue\n",
    "        if info['type'] == 'Numeric':\n",
    "            numerical_cols.append(col)\n",
    "        elif info['type'] == 'Text':\n",
    "            text_cols.append(col)\n",
    "\n",
    "    print(f\"\\nIdentified Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Identified Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Identified Text Columns: {text_cols}\")\n",
    "\n",
    "    # Preprocessing Pipelines\n",
    "    # Each text column ('Question', 'Response') will be processed by its own TF-IDF vectorizer.\n",
    "    # ColumnTransformer will then concatenate these sparse outputs horizontally.\n",
    "    # This is the standard and correct way to handle multiple text columns.\n",
    "    # The TextCleaner ensures the input to TfidfVectorizer is always a 1D array of strings.\n",
    "\n",
    "    # Create a list of (name, transformer, columns) tuples for ColumnTransformer\n",
    "    transformers_list = []\n",
    "\n",
    "    for col in text_cols:\n",
    "        text_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "            ('text_cleaner', TextCleaner()),\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english'))\n",
    "        ])\n",
    "        # Pass each text column individually to its own TF-IDF pipeline\n",
    "        transformers_list.append((f'text_{col}', text_pipeline, [col])) # Note: [col] to ensure ColumnTransformer passes a Series/DataFrame column\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers_list,\n",
    "        remainder='passthrough' # Keep other columns (e.g., IDs if present)\n",
    "    )\n",
    "\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    print(\"\\nSplitting data into training and validation sets (80/20 stratified)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    print(\"\\nFitting and transforming training data...\")\n",
    "    # Ensure X_train is a DataFrame when passed to fit_transform\n",
    "    X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "    print(\"Training data preprocessing complete.\")\n",
    "\n",
    "    # Transform the validation data\n",
    "    print(\"Transforming validation data...\")\n",
    "    # Ensure X_val is a DataFrame when passed to transform\n",
    "    X_val_processed = full_pipeline.transform(X_val)\n",
    "    print(\"Validation data preprocessing complete.\")\n",
    "\n",
    "    # Transform the test data\n",
    "    print(\"Transforming test data...\")\n",
    "    # Ensure X_test_raw is a DataFrame when passed to transform\n",
    "    X_test_processed = full_pipeline.transform(X_test_raw)\n",
    "    print(\"Test data preprocessing complete.\")\n",
    "\n",
    "    # Display shapes of processed data\n",
    "    print(f\"\\nShape of processed training features: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of processed validation features: {X_val_processed.shape}\")\n",
    "    print(f\"Shape of processed test features: {X_test_processed.shape}\")\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"\\nStarting model training (LightGBM Classifier)...\")\n",
    "    num_classes = y.nunique()\n",
    "\n",
    "    lgb_clf = lgb.LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=num_classes,\n",
    "                                 random_state=42,\n",
    "                                 n_estimators=1000,\n",
    "                                 learning_rate=0.05)\n",
    "\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
    "\n",
    "    lgb_clf.fit(X_train_processed, y_train,\n",
    "                eval_set=[(X_val_processed, y_val)],\n",
    "                eval_metric='multi_logloss',\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    trained_model = lgb_clf\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\nEvaluating model performance on the validation set...\")\n",
    "    y_pred_proba = trained_model.predict_proba(X_val_processed)\n",
    "    y_pred = trained_model.predict(X_val_processed)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['logloss'] = log_loss(y_val, y_pred_proba)\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    try:\n",
    "        metrics['roc_auc_ovr'] = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC (ovr): {e}. This might happen if a class has only one sample in y_val.\")\n",
    "        metrics['roc_auc_ovr'] = None\n",
    "\n",
    "    print(f\"Validation LogLoss: {metrics['logloss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "    if metrics['roc_auc_ovr'] is not None:\n",
    "        print(f\"Validation ROC AUC (OvR): {metrics['roc_auc_ovr']:.4f}\")\n",
    "    else:\n",
    "        print(\"Validation ROC AUC (OvR): Not calculated due to error.\")\n",
    "\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Metrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    # --- Model Persistence ---\n",
    "    joblib.dump(trained_model, MODEL_PATH)\n",
    "    print(f\"Trained model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # --- Prediction and Submission Generation ---\n",
    "    print(\"\\nGenerating predictions for the test set...\")\n",
    "    # Ensure trained_model is available. If main() was called and returned None, load it.\n",
    "    if trained_model is None:\n",
    "        try:\n",
    "            trained_model = joblib.load(MODEL_PATH)\n",
    "            print(f\"Loaded model from {MODEL_PATH}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Cannot generate predictions.\")\n",
    "            return\n",
    "\n",
    "    # Generate probabilities for the test set\n",
    "    test_predictions_proba = trained_model.predict_proba(X_test_processed)\n",
    "\n",
    "    # Prepare submission DataFrame\n",
    "    # The sample submission has columns 'id', '0', '1', ..., '6'\n",
    "    # Check if 'id' column exists in test_df, if not, create a default index\n",
    "    if 'id' in test_df.columns:\n",
    "        submission_df = pd.DataFrame({'id': test_df['id']})\n",
    "    else:\n",
    "        # If 'id' column is not present, assume the test_df index should be used\n",
    "        # or a simple range if no meaningful index exists.\n",
    "        # The sample submission implies an 'id' column, so we should create one if missing.\n",
    "        submission_df = pd.DataFrame({'id': test_df.index}) \n",
    "\n",
    "    # Add probability columns\n",
    "    # The target column 'target' has values from 0 to 6, so we need 7 probability columns.\n",
    "    # The order of classes in predict_proba is determined by the model's internal class mapping.\n",
    "    # For LightGBM, it's usually sorted unique values of y_train.\n",
    "    # To be safe, we can get the class labels from the trained model.\n",
    "    class_labels = trained_model.classes_\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        submission_df[str(class_label)] = test_predictions_proba[:, i]\n",
    "\n",
    "    # Save submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    print(f\"Submission file generated and saved to {SUBMISSION_PATH}\")\n",
    "    print(f\"Submission DataFrame head:\\n{submission_df.head()}\")\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model_instance = main()\n",
    "    if trained_model_instance:\n",
    "        print(\"\\nScript finished successfully. Trained model instance returned.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
