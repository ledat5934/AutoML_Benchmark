{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a074a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define base path with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_LABELS_PATH = BASE_PATH / 'labels.csv'\n",
    "SAMPLE_SUBMISSION_PATH = BASE_PATH / 'sample_submission.csv'\n",
    "TRAIN_IMAGES_DIR = BASE_PATH / 'train'\n",
    "TEST_IMAGES_DIR = BASE_PATH / 'test'\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_image(image_path, label=None):\n",
    "    \"\"\"Loads and preprocesses an image.\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    if label is not None:\n",
    "        return img, label\n",
    "    return img\n",
    "\n",
    "def create_image_dataset(image_paths, labels=None, shuffle=False, augment=False):\n",
    "    \"\"\"Creates a tf.data.Dataset for images.\"\"\"\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "\n",
    "    # No explicit augmentation for now, but can be added here\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load, preprocess, and prepare the dataset.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 1: Data Loading and Preprocessing ---\")\n",
    "\n",
    "    # 1. Load Labels\n",
    "    print(f\"Loading training labels from: {TRAIN_LABELS_PATH}\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "        print(f\"Labels DataFrame head:\\n{labels_df.head()}\")\n",
    "        print(f\"Labels DataFrame shape: {labels_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {TRAIN_LABELS_PATH} not found. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # 2. Encode Breed Labels\n",
    "    print(\"Encoding breed labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_df['breed_encoded'] = label_encoder.fit_transform(labels_df['breed'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Number of unique breeds (classes): {num_classes}\")\n",
    "    print(f\"Encoded labels head:\\n{labels_df.head()}\")\n",
    "\n",
    "    # 3. Prepare Image Paths\n",
    "    print(\"Preparing image paths...\")\n",
    "    labels_df['image_path'] = labels_df['id'].apply(lambda x: str(TRAIN_IMAGES_DIR / f\"{x}.jpg\"))\n",
    "\n",
    "    # Verify all image paths exist\n",
    "    missing_images = [path for path in labels_df['image_path'] if not Path(path).exists()]\n",
    "    if missing_images:\n",
    "        print(f\"Warning: {len(missing_images)} training images not found. Example: {missing_images[0]}\")\n",
    "        # Filter out rows with missing images if necessary, or handle as an error\n",
    "        labels_df = labels_df[labels_df['image_path'].apply(lambda x: Path(x).exists())]\n",
    "        print(f\"Filtered labels DataFrame shape after removing missing images: {labels_df.shape}\")\n",
    "    else:\n",
    "        print(\"All training image paths verified.\")\n",
    "\n",
    "    # 4. Split Data into Training and Validation Sets\n",
    "    print(\"Splitting data into training and validation sets (80/20 split)...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        labels_df,\n",
    "        test_size=0.2,\n",
    "        stratify=labels_df['breed_encoded'],\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "    # 5. Create TensorFlow Datasets for Training and Validation\n",
    "    print(\"Creating TensorFlow Datasets for training and validation...\")\n",
    "    train_image_paths = train_df['image_path'].values\n",
    "    train_labels_encoded = train_df['breed_encoded'].values\n",
    "    val_image_paths = val_df['image_path'].values\n",
    "    val_labels_encoded = val_df['breed_encoded'].values\n",
    "\n",
    "    train_dataset = create_image_dataset(train_image_paths, train_labels_encoded, shuffle=True)\n",
    "    val_dataset = create_image_dataset(val_image_paths, val_labels_encoded)\n",
    "\n",
    "    print(\"Training dataset created.\")\n",
    "    print(\"Validation dataset created.\")\n",
    "\n",
    "    # 6. Prepare Test Image Paths\n",
    "    print(\"Preparing test image paths...\")\n",
    "    test_image_ids = [Path(f).stem for f in os.listdir(TEST_IMAGES_DIR) if f.endswith('.jpg')]\n",
    "    test_image_paths = [str(TEST_IMAGES_DIR / f\"{img_id}.jpg\") for img_id in test_image_ids]\n",
    "\n",
    "    # Verify all test image paths exist\n",
    "    missing_test_images = [path for path in test_image_paths if not Path(path).exists()]\n",
    "    if missing_test_images:\n",
    "        print(f\"Warning: {len(missing_test_images)} test images not found. Example: {missing_test_images[0]}\")\n",
    "        # Filter out missing test images\n",
    "        test_image_paths = [path for path in test_image_paths if Path(path).exists()]\n",
    "        print(f\"Filtered test image count: {len(test_image_paths)}\")\n",
    "    else:\n",
    "        print(\"All test image paths verified.\")\n",
    "\n",
    "    # 7. Create TensorFlow Dataset for Test Images (without labels)\n",
    "    print(\"Creating TensorFlow Dataset for test images...\")\n",
    "    test_dataset = create_image_dataset(test_image_paths)\n",
    "    print(\"Test dataset created.\")\n",
    "\n",
    "    print(\"--- Data Preprocessing Complete ---\")\n",
    "\n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'num_classes': num_classes,\n",
    "        'label_encoder': label_encoder,\n",
    "        'train_df': train_df, # Return for potential further use\n",
    "        'val_df': val_df,     # Return for potential further use\n",
    "        'test_image_ids': test_image_ids # Return original test IDs for submission\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processed_data = main()\n",
    "    # You can now access the processed datasets and metadata:\n",
    "    # train_ds = processed_data['train_dataset']\n",
    "    # val_ds = processed_data['val_dataset']\n",
    "    # test_ds = processed_data['test_dataset']\n",
    "    # num_classes = processed_data['num_classes']\n",
    "    # label_encoder = processed_data['label_encoder']\n",
    "    # test_image_ids = processed_data['test_image_ids']\n",
    "    # print(f\"\\nExample batch from training dataset:\")\n",
    "    # for images, labels in processed_data['train_dataset'].take(1):\n",
    "    #     print(f\"Image batch shape: {images.shape}\")\n",
    "    #     print(f\"Label batch shape: {labels.shape}\")\n",
    "    #     print(f\"First 5 labels: {labels.numpy()[:5]}\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78489fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, roc_auc_score\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define base path with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_LABELS_PATH = BASE_PATH / 'labels.csv'\n",
    "SAMPLE_SUBMISSION_PATH = BASE_PATH / 'sample_submission.csv'\n",
    "TRAIN_IMAGES_DIR = BASE_PATH / 'train'\n",
    "TEST_IMAGES_DIR = BASE_PATH / 'test'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = ROOT_DIR / 'outputs'\n",
    "MODELS_DIR = ROOT_DIR / 'models'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METRICS_PATH = OUTPUT_DIR / \"metrics.json\"\n",
    "MODEL_PATH = MODELS_DIR / \"dog_breed_classification_model.keras\" # Keras models are saved with .keras extension\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_image(image_path, label=None):\n",
    "    \"\"\"Loads and preprocesses an image.\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    if label is not None:\n",
    "        return img, label\n",
    "    return img\n",
    "\n",
    "def create_image_dataset(image_paths, labels=None, shuffle=False, augment=False):\n",
    "    \"\"\"Creates a tf.data.Dataset for images.\"\"\"\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "\n",
    "    # No explicit augmentation for now, but can be added here\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def build_model(num_classes):\n",
    "    \"\"\"Builds a fine-tuned EfficientNetB0 model.\"\"\"\n",
    "    print(\"Building EfficientNetB0 model...\")\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load, preprocess, and prepare the dataset.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 1: Data Loading and Preprocessing ---\")\n",
    "\n",
    "    # 1. Load Labels\n",
    "    print(f\"Loading training labels from: {TRAIN_LABELS_PATH}\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "        print(f\"Labels DataFrame head:\\n{labels_df.head()}\")\n",
    "        print(f\"Labels DataFrame shape: {labels_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {TRAIN_LABELS_PATH} not found. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # 2. Encode Breed Labels\n",
    "    print(\"Encoding breed labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_df['breed_encoded'] = label_encoder.fit_transform(labels_df['breed'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Number of unique breeds (classes): {num_classes}\")\n",
    "    print(f\"Encoded labels head:\\n{labels_df.head()}\")\n",
    "\n",
    "    # 3. Prepare Image Paths\n",
    "    print(\"Preparing image paths...\")\n",
    "    labels_df['image_path'] = labels_df['id'].apply(lambda x: str(TRAIN_IMAGES_DIR / f\"{x}.jpg\"))\n",
    "\n",
    "    # Verify all image paths exist\n",
    "    missing_images = [path for path in labels_df['image_path'] if not Path(path).exists()]\n",
    "    if missing_images:\n",
    "        print(f\"Warning: {len(missing_images)} training images not found. Example: {missing_images[0]}\")\n",
    "        # Filter out rows with missing images if necessary, or handle as an error\n",
    "        labels_df = labels_df[labels_df['image_path'].apply(lambda x: Path(x).exists())]\n",
    "        print(f\"Filtered labels DataFrame shape after removing missing images: {labels_df.shape}\")\n",
    "    else:\n",
    "        print(\"All training image paths verified.\")\n",
    "\n",
    "    # 4. Split Data into Training and Validation Sets\n",
    "    print(\"Splitting data into training and validation sets (80/20 stratified split)...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        labels_df,\n",
    "        test_size=0.2,\n",
    "        stratify=labels_df['breed_encoded'],\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "    # 5. Create TensorFlow Datasets for Training and Validation\n",
    "    print(\"Creating TensorFlow Datasets for training and validation...\")\n",
    "    train_image_paths = train_df['image_path'].values\n",
    "    train_labels_encoded = train_df['breed_encoded'].values\n",
    "    val_image_paths = val_df['image_path'].values\n",
    "    val_labels_encoded = val_df['breed_encoded'].values\n",
    "\n",
    "    train_dataset = create_image_dataset(train_image_paths, train_labels_encoded, shuffle=True)\n",
    "    val_dataset = create_image_dataset(val_image_paths, val_labels_encoded)\n",
    "\n",
    "    print(\"Training dataset created.\")\n",
    "    print(\"Validation dataset created.\")\n",
    "\n",
    "    # 6. Prepare Test Image Paths\n",
    "    print(\"Preparing test image paths...\")\n",
    "    test_image_ids = [Path(f).stem for f in os.listdir(TEST_IMAGES_DIR) if f.endswith('.jpg')]\n",
    "    test_image_paths = [str(TEST_IMAGES_DIR / f\"{img_id}.jpg\") for img_id in test_image_ids]\n",
    "\n",
    "    # Verify all test image paths exist\n",
    "    missing_test_images = [path for path in test_image_paths if not Path(path).exists()]\n",
    "    if missing_test_images:\n",
    "        print(f\"Warning: {len(missing_test_images)} test images not found. Example: {missing_test_images[0]}\")\n",
    "        # Filter out missing test images\n",
    "        test_image_paths = [path for path in test_image_paths if Path(path).exists()]\n",
    "        print(f\"Filtered test image count: {len(test_image_paths)}\")\n",
    "    else:\n",
    "        print(\"All test image paths verified.\")\n",
    "\n",
    "    # 7. Create TensorFlow Dataset for Test Images (without labels)\n",
    "    print(\"Creating TensorFlow Dataset for test images...\")\n",
    "    test_dataset = create_image_dataset(test_image_paths)\n",
    "    print(\"Test dataset created.\")\n",
    "\n",
    "    print(\"--- Data Preprocessing Complete ---\")\n",
    "\n",
    "    print(\"\\n--- Stage 2: Model Training and Evaluation ---\")\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(num_classes)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=100, # Set a high number, early stopping will stop it\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model on the validation set...\")\n",
    "    val_predictions_proba = model.predict(val_dataset)\n",
    "    val_predictions = np.argmax(val_predictions_proba, axis=1)\n",
    "\n",
    "    # Convert one-hot encoded labels back to original integer labels if necessary\n",
    "    # For sparse_categorical_crossentropy, labels are already integers, so no conversion needed for val_labels_encoded\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['overall'] = {\n",
    "        'accuracy': accuracy_score(val_labels_encoded, val_predictions),\n",
    "        'f1_score_weighted': f1_score(val_labels_encoded, val_predictions, average='weighted'),\n",
    "        'log_loss': log_loss(val_labels_encoded, val_predictions_proba)\n",
    "    }\n",
    "\n",
    "    # ROC AUC for multi-class is typically calculated as One-vs-Rest (OvR) or One-vs-One (OvO)\n",
    "    # For simplicity, we'll use OvR if possible, but it requires binary labels for each class.\n",
    "    # If the number of classes is large, this can be computationally intensive.\n",
    "    # For multi_class_log_loss, ROC AUC is not the primary metric, but can be useful.\n",
    "    try:\n",
    "        metrics['overall']['roc_auc_ovr'] = roc_auc_score(val_labels_encoded, val_predictions_proba, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC (OvR): {e}. This might happen if there's only one class in a fold or other issues.\")\n",
    "        metrics['overall']['roc_auc_ovr'] = None\n",
    "\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    for metric_name, value in metrics['overall'].items():\n",
    "        print(f\"{metric_name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    # Persist metrics to JSON\n",
    "    print(f\"Saving metrics to {METRICS_PATH}\")\n",
    "    with open(METRICS_PATH, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Metrics saved.\")\n",
    "\n",
    "    # Persist the trained model\n",
    "    print(f\"Saving trained model to {MODEL_PATH}\")\n",
    "    model.save(MODEL_PATH) # Saves in Keras native format\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    print(\"--- Model Training and Evaluation Complete ---\")\n",
    "\n",
    "    return model # Return the trained model instance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = main()\n",
    "    # You can now use the trained_model for predictions or further analysis\n",
    "    # For example, to load the model later:\n",
    "    # from tensorflow.keras.models import load_model\n",
    "    # loaded_model = load_model(MODEL_PATH)\n",
    "    # print(f\"Model loaded from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11841a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Determine the project root\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # __file__ is not defined inside Kaggle/Jupyter\n",
    "    ROOT_DIR = Path.cwd()\n",
    "\n",
    "# Define base path with fallback\n",
    "BASE_PATH_CANDIDATE_1 = (ROOT_DIR / 'input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "BASE_PATH_CANDIDATE_2 = Path('input/Datasets/datasets/dog_breed_classification').resolve()\n",
    "\n",
    "if BASE_PATH_CANDIDATE_1.exists():\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_1\n",
    "else:\n",
    "    BASE_PATH = BASE_PATH_CANDIDATE_2\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_LABELS_PATH = BASE_PATH / 'labels.csv'\n",
    "SAMPLE_SUBMISSION_PATH = BASE_PATH / 'sample_submission.csv'\n",
    "TRAIN_IMAGES_DIR = BASE_PATH / 'train'\n",
    "TEST_IMAGES_DIR = BASE_PATH / 'test'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = ROOT_DIR / 'outputs'\n",
    "MODELS_DIR = ROOT_DIR / 'models'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METRICS_PATH = OUTPUT_DIR / \"metrics.json\"\n",
    "MODEL_PATH = MODELS_DIR / \"dog_breed_classification_model.keras\" # Keras models are saved with .keras extension\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"label_encoder.joblib\" # Path to save the label encoder\n",
    "SUBMISSION_PATH = OUTPUT_DIR / \"submission.csv\"\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_image(image_path, label=None):\n",
    "    \"\"\"Loads and preprocesses an image.\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    if label is not None:\n",
    "        return img, label\n",
    "    return img\n",
    "\n",
    "def create_image_dataset(image_paths, labels=None, shuffle=False, augment=False):\n",
    "    \"\"\"Creates a tf.data.Dataset for images.\"\"\"\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "\n",
    "    # No explicit augmentation for now, but can be added here\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load, preprocess, and prepare the dataset.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 1: Data Loading and Preprocessing ---\")\n",
    "\n",
    "    # 1. Load Labels\n",
    "    print(f\"Loading training labels from: {TRAIN_LABELS_PATH}\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "        print(f\"Labels DataFrame head:\\n{labels_df.head()}\")\n",
    "        print(f\"Labels DataFrame shape: {labels_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {TRAIN_LABELS_PATH} not found. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # 2. Encode Breed Labels\n",
    "    print(\"Encoding breed labels...\")\n",
    "    label_encoder = joblib.load(LABEL_ENCODER_PATH) if LABEL_ENCODER_PATH.exists() else LabelEncoder()\n",
    "\n",
    "    # If label_encoder was not loaded, fit it\n",
    "    if not LABEL_ENCODER_PATH.exists():\n",
    "        labels_df['breed_encoded'] = label_encoder.fit_transform(labels_df['breed'])\n",
    "        joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "        print(f\"Label encoder saved to {LABEL_ENCODER_PATH}\")\n",
    "    else:\n",
    "        # If loaded, ensure it's transformed consistently\n",
    "        labels_df['breed_encoded'] = label_encoder.transform(labels_df['breed'])\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Number of unique breeds (classes): {num_classes}\")\n",
    "    print(f\"Encoded labels head:\\n{labels_df.head()}\")\n",
    "\n",
    "    # 3. Prepare Image Paths\n",
    "    print(\"Preparing image paths...\")\n",
    "    labels_df['image_path'] = labels_df['id'].apply(lambda x: str(TRAIN_IMAGES_DIR / f\"{x}.jpg\"))\n",
    "\n",
    "    # Verify all image paths exist\n",
    "    missing_images = [path for path in labels_df['image_path'] if not Path(path).exists()]\n",
    "    if missing_images:\n",
    "        print(f\"Warning: {len(missing_images)} training images not found. Example: {missing_images[0]}\")\n",
    "        # Filter out rows with missing images if necessary, or handle as an error\n",
    "        labels_df = labels_df[labels_df['image_path'].apply(lambda x: Path(x).exists())]\n",
    "        print(f\"Filtered labels DataFrame shape after removing missing images: {labels_df.shape}\")\n",
    "    else:\n",
    "        print(\"All training image paths verified.\")\n",
    "\n",
    "    # 4. Split Data into Training and Validation Sets\n",
    "    print(\"Splitting data into training and validation sets (80/20 stratified split)...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        labels_df,\n",
    "        test_size=0.2,\n",
    "        stratify=labels_df['breed_encoded'],\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "    # 5. Create TensorFlow Datasets for Training and Validation\n",
    "    print(\"Creating TensorFlow Datasets for training and validation...\")\n",
    "    train_image_paths = train_df['image_path'].values\n",
    "    train_labels_encoded = train_df['breed_encoded'].values\n",
    "    val_image_paths = val_df['image_path'].values\n",
    "    val_labels_encoded = val_df['breed_encoded'].values\n",
    "\n",
    "    train_dataset = create_image_dataset(train_image_paths, train_labels_encoded, shuffle=True)\n",
    "    val_dataset = create_image_dataset(val_image_paths, val_labels_encoded)\n",
    "\n",
    "    print(\"Training dataset created.\")\n",
    "    print(\"Validation dataset created.\")\n",
    "\n",
    "    # 6. Prepare Test Image Paths\n",
    "    print(\"Preparing test image paths...\")\n",
    "    test_image_ids = [Path(f).stem for f in os.listdir(TEST_IMAGES_DIR) if f.endswith('.jpg')]\n",
    "    test_image_paths = [str(TEST_IMAGES_DIR / f\"{img_id}.jpg\") for img_id in test_image_ids]\n",
    "\n",
    "    # Verify all test image paths exist\n",
    "    missing_test_images = [path for path in test_image_paths if not Path(path).exists()]\n",
    "    if missing_test_images:\n",
    "        print(f\"Warning: {len(missing_test_images)} test images not found. Example: {missing_test_images[0]}\")\n",
    "        # Filter out missing test images\n",
    "        test_image_paths = [path for path in test_image_paths if Path(path).exists()]\n",
    "        print(f\"Filtered test image count: {len(test_image_paths)}\")\n",
    "    else:\n",
    "        print(\"All test image paths verified.\")\n",
    "\n",
    "    # 7. Create TensorFlow Dataset for Test Images (without labels)\n",
    "    print(\"Creating TensorFlow Dataset for test images...\")\n",
    "    test_dataset = create_image_dataset(test_image_paths)\n",
    "    print(\"Test dataset created.\")\n",
    "\n",
    "    print(\"--- Data Preprocessing Complete ---\")\n",
    "\n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'num_classes': num_classes,\n",
    "        'label_encoder': label_encoder,\n",
    "        'train_df': train_df, # Return for potential further use\n",
    "        'val_df': val_df,     # Return for potential further use\n",
    "        'test_image_ids': test_image_ids # Return original test IDs for submission\n",
    "    }\n",
    "\n",
    "def build_model(num_classes):\n",
    "    \"\"\"Builds a fine-tuned EfficientNetB0 model.\"\"\"\n",
    "    print(\"Building EfficientNetB0 model...\")\n",
    "    base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"\n",
    "    Main function to load, preprocess, train, and evaluate the model.\n",
    "    \"\"\"\n",
    "    processed_data = main()\n",
    "    train_dataset = processed_data['train_dataset']\n",
    "    val_dataset = processed_data['val_dataset']\n",
    "    test_dataset = processed_data['test_dataset']\n",
    "    num_classes = processed_data['num_classes']\n",
    "    label_encoder = processed_data['label_encoder']\n",
    "    val_labels_encoded = processed_data['val_df']['breed_encoded'].values # Get actual labels for evaluation\n",
    "    test_image_ids = processed_data['test_image_ids']\n",
    "\n",
    "    print(\"\\n--- Stage 2: Model Training and Evaluation ---\")\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(num_classes)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=100, # Set a high number, early stopping will stop it\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model on the validation set...\")\n",
    "    val_predictions_proba = model.predict(val_dataset)\n",
    "    val_predictions = np.argmax(val_predictions_proba, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['overall'] = {\n",
    "        'accuracy': accuracy_score(val_labels_encoded, val_predictions),\n",
    "        'f1_score_weighted': f1_score(val_labels_encoded, val_predictions, average='weighted'),\n",
    "        'log_loss': log_loss(val_labels_encoded, val_predictions_proba)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        metrics['overall']['roc_auc_ovr'] = roc_auc_score(val_labels_encoded, val_predictions_proba, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC (OvR): {e}. This might happen if there's only one class in a fold or other issues.\")\n",
    "        metrics['overall']['roc_auc_ovr'] = None\n",
    "\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    for metric_name, value in metrics['overall'].items():\n",
    "        print(f\"{metric_name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    # Persist metrics to JSON\n",
    "    print(f\"Saving metrics to {METRICS_PATH}\")\n",
    "    with open(METRICS_PATH, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Metrics saved.\")\n",
    "\n",
    "    # Persist the trained model\n",
    "    print(f\"Saving trained model to {MODEL_PATH}\")\n",
    "    model.save(MODEL_PATH) # Saves in Keras native format\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    print(\"--- Model Training and Evaluation Complete ---\")\n",
    "\n",
    "    return model, test_dataset, test_image_ids, label_encoder # Return necessary components for prediction\n",
    "\n",
    "def generate_predictions(trained_model, test_dataset, test_image_ids, label_encoder):\n",
    "    \"\"\"\n",
    "    Generates predictions on the test set and creates a submission file.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 3: Prediction and Submission ---\")\n",
    "\n",
    "    # Ensure trained_model is available. If None, load it.\n",
    "    if trained_model is None:\n",
    "        print(f\"Loading model from {MODEL_PATH}...\")\n",
    "        try:\n",
    "            trained_model = load_model(MODEL_PATH)\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return\n",
    "\n",
    "    # Generate predictions (probabilities for multi-class classification)\n",
    "    print(\"Generating predictions on the test set...\")\n",
    "    test_predictions_proba = trained_model.predict(test_dataset)\n",
    "    print(f\"Predictions shape: {test_predictions_proba.shape}\")\n",
    "\n",
    "    # Load sample submission to match format\n",
    "    print(f\"Loading sample submission from: {SAMPLE_SUBMISSION_PATH}\")\n",
    "    try:\n",
    "        sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "        print(f\"Sample submission head:\\n{sample_submission_df.head()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {SAMPLE_SUBMISSION_PATH} not found. Cannot create submission file.\")\n",
    "        return\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({'id': test_image_ids})\n",
    "\n",
    "    # Get breed names from label encoder\n",
    "    breed_names = label_encoder.classes_\n",
    "\n",
    "    # Add probability columns for each breed\n",
    "    for i, breed in enumerate(breed_names):\n",
    "        submission_df[breed] = test_predictions_proba[:, i]\n",
    "\n",
    "    # Ensure the order of columns matches sample submission (excluding 'id')\n",
    "    # This is crucial for Kaggle submissions\n",
    "    submission_columns = ['id'] + list(sample_submission_df.columns.drop('id'))\n",
    "    submission_df = submission_df[submission_columns]\n",
    "\n",
    "    # Save the submission file\n",
    "    print(f\"Saving submission file to {SUBMISSION_PATH}\")\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    print(f\"Submission file generated successfully at {SUBMISSION_PATH}\")\n",
    "    print(f\"Submission DataFrame head:\\n{submission_df.head()}\")\n",
    "    print(f\"Submission DataFrame shape: {submission_df.shape}\")\n",
    "\n",
    "    print(\"--- Prediction and Submission Complete ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run training and evaluation, then pass the trained model to prediction stage\n",
    "    trained_model_instance, test_ds, test_ids, lbl_encoder = train_and_evaluate()\n",
    "    generate_predictions(trained_model_instance, test_ds, test_ids, lbl_encoder)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
