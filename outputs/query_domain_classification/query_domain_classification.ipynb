{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, and preprocessing steps for the\n",
    "    query_domain_classification dataset.\n",
    "    \"\"\"\n",
    "    # Determine the project root dynamically\n",
    "    ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "    BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/query_domain_classification').resolve()\n",
    "    BASE_PATH_OPTION2 = Path('input/Datasets/datasets/query_domain_classification').resolve()\n",
    "\n",
    "    if BASE_PATH_OPTION1.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION1\n",
    "    elif BASE_PATH_OPTION2.exists():\n",
    "        BASE_PATH = BASE_PATH_OPTION2\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "    print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "    # Define file paths\n",
    "    TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "    TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "    SUBMISSION_FILE = BASE_PATH / \"Submission_file01.csv\" # This is likely a sample submission or template\n",
    "\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        print(\"Datasets loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return None, None, None, None # Return None for all outputs on error\n",
    "\n",
    "    print(\"\\n--- Original Train DataFrame Info ---\")\n",
    "    train_df.info()\n",
    "    print(\"\\n--- Original Train DataFrame Head ---\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    # --- Preprocessing Steps ---\n",
    "\n",
    "    # Identify target column\n",
    "    TARGET_COLUMN = \"Domain\"\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X_train = train_df.drop(columns=[TARGET_COLUMN, 'ID']) # ID is not a feature for classification\n",
    "    y_train = train_df[TARGET_COLUMN]\n",
    "    X_test = test_df.drop(columns=['ID']) # Assuming test_df also has an 'ID' column\n",
    "\n",
    "    # Identify column types based on metadata and EDA\n",
    "    # From metadata: 'ID' is Numeric (unique), 'Title' is Text, 'Domain' is Text (target)\n",
    "    # EDA confirms 'ID' is int64, 'Title' and 'Domain' are object (text)\n",
    "\n",
    "    # Numerical features (none relevant for direct use after dropping ID)\n",
    "    numerical_features = [] # 'ID' was the only numeric, but it's dropped as it's an identifier.\n",
    "\n",
    "    # Categorical features (none explicitly identified as categorical beyond the target)\n",
    "    categorical_features = []\n",
    "\n",
    "    # Text features\n",
    "    text_features = ['Title']\n",
    "\n",
    "    # Preprocessing pipelines for different column types\n",
    "    # Numerical pipeline (if any numerical features were kept)\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Categorical pipeline (if any categorical features were identified)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')), # Or 'constant', fill_value='missing'\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) # drop='first' to avoid multicollinearity\n",
    "    ])\n",
    "\n",
    "    # Text pipeline (TF-IDF Vectorizer)\n",
    "    # The error \"AttributeError: 'numpy.ndarray' object has no attribute 'lower'\"\n",
    "    # indicates that the TfidfVectorizer received a numpy array of objects (which could be NaNs)\n",
    "    # instead of a pandas Series of strings.\n",
    "    # The SimpleImputer(strategy='constant', fill_value='') should handle NaNs,\n",
    "    # but the ColumnTransformer might be passing a numpy array slice.\n",
    "    # To ensure the TF-IDF vectorizer always receives string-like objects,\n",
    "    # we explicitly convert the column to string type before passing it to the imputer.\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)) # Limit features to manage dimensionality\n",
    "    ])\n",
    "\n",
    "    # Create a preprocessor using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('text', text_transformer, text_features)\n",
    "        ],\n",
    "        remainder='passthrough' # Keep other columns (like 'ID' if not dropped, but we dropped it)\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    print(\"\\n--- Applying Preprocessing ---\")\n",
    "    # Fix: Ensure that the 'Title' column is treated as strings before passing to the preprocessor\n",
    "    # This is crucial to prevent the 'numpy.ndarray' object has no attribute 'lower' error\n",
    "    # by ensuring the TfidfVectorizer receives string-like objects.\n",
    "    # The ColumnTransformer passes slices of the DataFrame, which can become numpy arrays.\n",
    "    # Explicitly converting to string type here ensures consistency.\n",
    "    # Apply .fillna('') before .astype(str) to handle potential NaN values gracefully,\n",
    "    # although SimpleImputer should catch them, this adds robustness.\n",
    "    X_train['Title'] = X_train['Title'].fillna('').astype(str)\n",
    "    X_test['Title'] = X_test['Title'].fillna('').astype(str)\n",
    "\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test) # Use transform, not fit_transform for test set\n",
    "\n",
    "    print(f\"Shape of processed training data: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of processed test data: {X_test_processed.shape}\")\n",
    "\n",
    "    print(\"\\nPreprocessing complete. Data is ready for model training.\")\n",
    "\n",
    "    return X_train_processed, X_test_processed, y_train, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train_processed, X_test_processed, y_train, preprocessor = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf1725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# --- File Path Constants (Override-able) ---\n",
    "# Determine the project root dynamically\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/query_domain_classification').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/query_domain_classification').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SUBMISSION_FILE = BASE_PATH / \"Submission_file01.csv\" # Not directly used in this script, but good to keep.\n",
    "\n",
    "# Output paths\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_NAME = \"query_domain_classification\"\n",
    "METRICS_PATH = OUTPUTS_DIR / \"metrics.json\"\n",
    "MODEL_PATH = MODELS_DIR / f\"{DATASET_NAME}_model.pkl\"\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Orchestrates the data loading, cleaning, preprocessing, model training,\n",
    "    evaluation, and persistence steps for the query_domain_classification dataset.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        print(\"Datasets loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"\\n--- Original Train DataFrame Info ---\")\n",
    "    train_df.info()\n",
    "    print(\"\\n--- Original Train DataFrame Head ---\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    # --- Preprocessing Steps ---\n",
    "\n",
    "    # Identify target column\n",
    "    TARGET_COLUMN = \"Domain\"\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = train_df.drop(columns=[TARGET_COLUMN, 'ID']) # ID is not a feature for classification\n",
    "    y = train_df[TARGET_COLUMN]\n",
    "    X_test_final = test_df.drop(columns=['ID']) # Assuming test_df also has an 'ID' column\n",
    "\n",
    "    # Identify column types based on metadata and EDA\n",
    "    numerical_features = [] # 'ID' was the only numeric, but it's dropped as it's an identifier.\n",
    "    categorical_features = [] # No explicit categorical features identified other than target\n",
    "    text_features = ['Title']\n",
    "\n",
    "    # Preprocessing pipelines for different column types\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "    ])\n",
    "\n",
    "    # Text pipeline (TF-IDF Vectorizer)\n",
    "    # The error \"AttributeError: 'numpy.ndarray' object has no attribute 'lower'\"\n",
    "    # indicates that the TfidfVectorizer received a numpy array of objects (which could be NaNs)\n",
    "    # instead of a pandas Series of strings.\n",
    "    # The SimpleImputer(strategy='constant', fill_value='') should handle NaNs,\n",
    "    # but the ColumnTransformer might be passing a numpy array slice.\n",
    "    # To ensure the TF-IDF vectorizer always receives string-like objects,\n",
    "    # we explicitly convert the column to string type before passing it to the imputer.\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000))\n",
    "    ])\n",
    "\n",
    "    # Create a preprocessor using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('text', text_transformer, text_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing to the full training data before splitting\n",
    "    print(\"\\n--- Applying Preprocessing to full dataset ---\")\n",
    "    # FIX: Ensure 'Title' column is of string type before passing to preprocessor\n",
    "    # This prevents the 'AttributeError: 'numpy.ndarray' object has no attribute 'lower''\n",
    "    # by ensuring the TfidfVectorizer receives string-like objects.\n",
    "    # The .fillna('') is added for robustness, though SimpleImputer should handle it.\n",
    "    X['Title'] = X['Title'].fillna('').astype(str)\n",
    "    X_test_final['Title'] = X_test_final['Title'].fillna('').astype(str)\n",
    "\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    X_test_final_processed = preprocessor.transform(X_test_final)\n",
    "\n",
    "    print(f\"Shape of full processed training data: {X_processed.shape}\")\n",
    "    print(f\"Shape of processed final test data: {X_test_final_processed.shape}\")\n",
    "\n",
    "    # --- Stratified Train-Validation Split ---\n",
    "    # Convert target to numerical labels for LightGBM\n",
    "    # Get unique domain names and map them to integers\n",
    "    unique_domains = y.unique()\n",
    "    unique_domains.sort() # Ensure consistent order for mapping\n",
    "    label_mapping = {domain: i for i, domain in enumerate(unique_domains)}\n",
    "    y_encoded = y.map(label_mapping)\n",
    "\n",
    "    # Perform 80/20 stratified split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    print(f\"\\nShape of training split: {X_train.shape}\")\n",
    "    print(f\"Shape of validation split: {X_val.shape}\")\n",
    "\n",
    "    # --- Model Building and Training (LightGBM for Multi-class Classification) ---\n",
    "    print(\"\\n--- Training LightGBM Model ---\")\n",
    "\n",
    "    # Determine number of classes for LightGBM\n",
    "    num_classes = len(unique_domains)\n",
    "\n",
    "    # LightGBM Classifier\n",
    "    lgb_clf = lgb.LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=num_classes,\n",
    "                                 random_state=42,\n",
    "                                 n_estimators=1000) # Set a high number for early stopping\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    # For LightGBM, X_val and y_val are passed as eval_set\n",
    "    # eval_metric 'multi_logloss' is suitable for multiclass classification\n",
    "    # callbacks for early stopping\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
    "\n",
    "    lgb_clf.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='multi_logloss',\n",
    "                callbacks=callbacks)\n",
    "\n",
    "    trained_model = lgb_clf\n",
    "    print(\"\\nModel training complete.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\n--- Evaluating Model ---\")\n",
    "    y_pred_val = trained_model.predict(X_val)\n",
    "    y_proba_val = trained_model.predict_proba(X_val)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_val, y_pred_val)\n",
    "    metrics['f1_macro'] = f1_score(y_val, y_pred_val, average='macro') # Use macro for imbalanced classes\n",
    "    metrics['log_loss'] = log_loss(y_val, y_proba_val)\n",
    "\n",
    "    # ROC AUC for multiclass: One-vs-Rest (OvR)\n",
    "    # Check if there are at least two classes for ROC AUC calculation\n",
    "    if num_classes > 1:\n",
    "        try:\n",
    "            metrics['roc_auc_ovr_macro'] = roc_auc_score(y_val, y_proba_val, multi_class='ovr', average='macro')\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}. This might happen if a class has only one sample in the validation set.\")\n",
    "            metrics['roc_auc_ovr_macro'] = np.nan\n",
    "    else:\n",
    "        metrics['roc_auc_ovr_macro'] = np.nan # Not applicable for binary or single class\n",
    "\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Persist metrics to JSON\n",
    "    with open(METRICS_PATH, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Metrics saved to {METRICS_PATH}\")\n",
    "\n",
    "    # --- Persist Trained Model ---\n",
    "    joblib.dump(trained_model, MODEL_PATH)\n",
    "    print(f\"Trained model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Return the trained model, preprocessor, label_mapping, and processed test data\n",
    "    return trained_model, preprocessor, label_mapping, X_test_final_processed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, preprocessor, label_mapping, X_test_final_processed = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- File Path Constants ---\n",
    "# Determine the project root dynamically\n",
    "ROOT_DIR = Path(__file__).resolve().parent.parent\n",
    "BASE_PATH_OPTION1 = (ROOT_DIR / 'input/Datasets/datasets/query_domain_classification').resolve()\n",
    "BASE_PATH_OPTION2 = Path('input/Datasets/datasets/query_domain_classification').resolve()\n",
    "\n",
    "if BASE_PATH_OPTION1.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION1\n",
    "elif BASE_PATH_OPTION2.exists():\n",
    "    BASE_PATH = BASE_PATH_OPTION2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset base path not found at {BASE_PATH_OPTION1} or {BASE_PATH_OPTION2}\")\n",
    "\n",
    "print(f\"Resolved BASE_PATH: {BASE_PATH}\")\n",
    "\n",
    "TRAIN_FILE = BASE_PATH / \"train.csv\"\n",
    "TEST_FILE = BASE_PATH / \"test.csv\"\n",
    "SUBMISSION_SAMPLE_FILE = BASE_PATH / \"Submission_file01.csv\" # Sample submission file\n",
    "\n",
    "# Output paths\n",
    "OUTPUTS_DIR = Path(\"./outputs\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_NAME = \"query_domain_classification\"\n",
    "MODEL_PATH = MODELS_DIR / f\"{DATASET_NAME}_model.pkl\"\n",
    "SUBMISSION_PATH = OUTPUTS_DIR / \"submission.csv\"\n",
    "\n",
    "# Define the preprocessor construction function to be consistent with Stage 2\n",
    "# This function is crucial because the preprocessor itself was not saved in Stage 2.\n",
    "# In a production setting, the preprocessor and label_mapping would ideally be saved and loaded.\n",
    "# For this setup, we re-build and re-fit the preprocessor on the training data.\n",
    "def build_preprocessor():\n",
    "    \"\"\"\n",
    "    Builds and returns the ColumnTransformer preprocessor.\n",
    "    \"\"\"\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    text_features = ['Title']\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "    ])\n",
    "\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('text', text_transformer, text_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def generate_predictions(trained_model=None):\n",
    "    \"\"\"\n",
    "    Generates predictions on the test set and creates a submission file.\n",
    "\n",
    "    Args:\n",
    "        trained_model: The trained model object. If None, the model will be loaded from disk.\n",
    "    \"\"\"\n",
    "    # Ensure trained_model is available\n",
    "    if trained_model is None:\n",
    "        try:\n",
    "            trained_model = joblib.load(MODEL_PATH)\n",
    "            print(f\"Model loaded successfully from {MODEL_PATH}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model not found at {MODEL_PATH}. Please ensure the training script was run and model saved.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return\n",
    "\n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_FILE)\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        print(\"Train and Test datasets loaded successfully for preprocessing.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please ensure the dataset files are in the correct directory.\")\n",
    "        return\n",
    "\n",
    "    # --- Preprocessing for Test Data ---\n",
    "    # Re-initialize and fit the preprocessor on the training data, then transform test data.\n",
    "    preprocessor = build_preprocessor()\n",
    "\n",
    "    TARGET_COLUMN = \"Domain\"\n",
    "    X_train_for_preprocessor_fit = train_df.drop(columns=[TARGET_COLUMN, 'ID'])\n",
    "    X_test_final = test_df.drop(columns=['ID'])\n",
    "\n",
    "    # FIX: Ensure 'Title' column is of string type before passing to preprocessor\n",
    "    # The error \"AttributeError: 'numpy.ndarray' object has no attribute 'lower'\"\n",
    "    # indicates that the TfidfVectorizer received a numpy array of objects (which could be NaNs)\n",
    "    # instead of a pandas Series of strings.\n",
    "    # The SimpleImputer(strategy='constant', fill_value='') should handle NaNs,\n",
    "    # but the ColumnTransformer might be passing a numpy array slice.\n",
    "    # To ensure the TF-IDF vectorizer always receives string-like objects,\n",
    "    # we explicitly convert the column to string type before passing it to the imputer.\n",
    "    # Apply .fillna('') before .astype(str) to handle potential NaN values gracefully.\n",
    "    X_train_for_preprocessor_fit['Title'] = X_train_for_preprocessor_fit['Title'].fillna('').astype(str)\n",
    "    X_test_final['Title'] = X_test_final['Title'].fillna('').astype(str)\n",
    "\n",
    "    print(\"\\n--- Applying Preprocessing to test dataset ---\")\n",
    "    # Fit preprocessor on training data and transform test data\n",
    "    preprocessor.fit(X_train_for_preprocessor_fit)\n",
    "    X_test_final_processed = preprocessor.transform(X_test_final)\n",
    "\n",
    "    print(f\"Shape of processed test data for prediction: {X_test_final_processed.shape}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred_encoded = trained_model.predict(X_test_final_processed)\n",
    "\n",
    "    # Get the inverse mapping for domain labels\n",
    "    # This requires access to the original label mapping from training.\n",
    "    # Re-deriving it from the full training data's 'Domain' column is the safest bet here,\n",
    "    # ensuring consistency with how it was created in Stage 2.\n",
    "    y_train_original = train_df[TARGET_COLUMN]\n",
    "    unique_domains = y_train_original.unique()\n",
    "    # Sort unique_domains to ensure consistent mapping, as done in Stage 2\n",
    "    unique_domains.sort()\n",
    "    label_mapping = {domain: i for i, domain in enumerate(unique_domains)}\n",
    "    inverse_label_mapping = {i: domain for domain, i in label_mapping.items()}\n",
    "\n",
    "    y_pred_labels = pd.Series(y_pred_encoded).map(inverse_label_mapping)\n",
    "\n",
    "    # Build submission DataFrame\n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'Domain': y_pred_labels})\n",
    "\n",
    "    # Ensure submission format matches sample if available\n",
    "    try:\n",
    "        sample_submission_df = pd.read_csv(SUBMISSION_SAMPLE_FILE)\n",
    "        if not submission_df.columns.equals(sample_submission_df.columns):\n",
    "            print(\"Warning: Submission columns do not match sample submission columns.\")\n",
    "            print(f\"Expected: {sample_submission_df.columns.tolist()}\")\n",
    "            print(f\"Got: {submission_df.columns.tolist()}\")\n",
    "        # Check if 'ID' column needs reindexing\n",
    "        if not submission_df['ID'].equals(sample_submission_df['ID']):\n",
    "            print(\"Warning: Submission 'ID' column order or values do not match sample submission 'ID'.\")\n",
    "            # Reindex submission_df to match sample_submission_df if necessary\n",
    "            submission_df = submission_df.set_index('ID').reindex(sample_submission_df['ID']).reset_index()\n",
    "            print(\"Submission DataFrame reindexed to match sample submission ID order.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Sample submission file not found at {SUBMISSION_SAMPLE_FILE}. Proceeding with default submission format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample submission file: {e}\")\n",
    "\n",
    "    # Save submission file\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    print(f\"\\nSubmission file generated successfully at {SUBMISSION_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Attempt to load the model first\n",
    "    loaded_model = None\n",
    "    try:\n",
    "        loaded_model = joblib.load(MODEL_PATH)\n",
    "        print(f\"Model successfully loaded from {MODEL_PATH} for prediction.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model not found at {MODEL_PATH}. Please ensure Stage 2 was run to train and save the model.\")\n",
    "        exit() # Exit if model not found, as prediction cannot proceed\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "        exit()\n",
    "\n",
    "    generate_predictions(trained_model=loaded_model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
