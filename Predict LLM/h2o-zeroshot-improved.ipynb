{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63021,"databundleVersionId":6870560,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T12:53:30.435172Z","iopub.execute_input":"2025-07-01T12:53:30.435384Z","iopub.status.idle":"2025-07-01T12:53:32.368235Z","shell.execute_reply.started":"2025-07-01T12:53:30.435366Z","shell.execute_reply":"2025-07-01T12:53:32.367256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================================\n# === 1. SETUP AND CONFIGURATION ========================================================\n# =========================================================================================\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\n\n# Scikit-learn for metrics and data splitting\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    roc_auc_score, \n    log_loss\n)\n\n# Transformers and Datasets libraries from Hugging Face\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset, load_metric\n\n# Set a seed for reproducibility\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n# Configuration class to hold all hyperparameters\nclass CFG:\n    # General\n    seed = 42\n    \n    # Kaggle environment paths\n    data_path = \"/kaggle/input/h2oai-predict-the-llm\"\n    output_path = \"/kaggle/working/\"\n    \n    # Model configuration\n    model_name = \"microsoft/deberta-v3-base\"\n    \n    # Training hyperparameters\n    n_splits = 5\n    max_length = 512\n    learning_rate = 2e-5\n    train_batch_size = 8  # Per device batch size\n    eval_batch_size = 16  # Per device batch size\n    epochs = 3\n    weight_decay = 0.01\n    \n    # Task specific\n    num_classes = 7\n    target_cols = [f'target_{i}' for i in range(num_classes)]\n\n# Apply the seed\nset_seed(CFG.seed)\n\n# =========================================================================================\n# === 2. LOAD AND PREPARE DATA ==========================================================\n# =========================================================================================\n\nprint(\"Loading data...\")\n# Load datasets from CSV files\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\nsubmission_df = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n\n# Rename the target column for compatibility with Hugging Face's Trainer\n# The Trainer API expects the target column to be named 'label'\ntrain_df.rename(columns={'target': 'label'}, inplace=True)\n\n# Convert pandas DataFrames to Hugging Face Dataset objects\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# =========================================================================================\n# === 3. TOKENIZATION ===================================================================\n# =========================================================================================\n\nprint(\"Initializing tokenizer...\")\n# Load the tokenizer associated with the chosen model\n# DeBERTa-v3 uses a SentencePiece-based tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n\ndef tokenize_function(examples):\n    \"\"\"\n    Tokenizes the input text by treating Question and Response as a pair.\n    The tokenizer will automatically format this as: [CLS] Question [SEP] Response [SEP]\n    \"\"\"\n    return tokenizer(\n        examples['Question'], \n        examples['Response'], \n        truncation=True, \n        max_length=CFG.max_length, \n        padding='max_length' # Pad to max_length for consistent tensor shapes\n    )\n\nprint(\"Tokenizing datasets...\")\n# Apply the tokenization function to the entire datasets\n# Using batched=True for faster processing\ntokenized_train_ds = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_ds = test_dataset.map(tokenize_function, batched=True)\n\n# Data collator will dynamically pad sentences in each batch to the longest length\n# This is more efficient than padding all sentences to max_length globally\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# =========================================================================================\n# === 4. METRICS COMPUTATION ============================================================\n# =========================================================================================\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes and returns a dictionary of evaluation metrics.\n    This function is passed to the Trainer.\n    \"\"\"\n    # Unpack predictions and true labels\n    logits, labels = eval_pred\n    \n    # Get probabilities by applying softmax to logits\n    probabilities = torch.nn.functional.softmax(torch.from_numpy(logits), dim=-1).numpy()\n    \n    # Get predicted class by finding the index of the max logit\n    predictions = np.argmax(logits, axis=1)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(labels, predictions)\n    f1_macro = f1_score(labels, predictions, average='macro')\n    f1_weighted = f1_score(labels, predictions, average='weighted')\n    \n    # For multi-class ROC AUC, we need probabilities and 'ovr' (one-vs-rest) strategy\n    roc_auc_ovr = roc_auc_score(labels, probabilities, multi_class='ovr')\n    roc_auc_ovr_weighted = roc_auc_score(labels, probabilities, multi_class='ovr', average='weighted')\n    \n    # Log loss requires probabilities\n    loss = log_loss(labels, probabilities)\n    \n    return {\n        'accuracy': accuracy,\n        'log_loss': loss,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'roc_auc_ovr': roc_auc_ovr,\n        'roc_auc_ovr_weighted': roc_auc_ovr_weighted,\n    }\n\n# =========================================================================================\n# === 5. CROSS-VALIDATION TRAINING AND INFERENCE ========================================\n# =========================================================================================\n\n# Prepare for cross-validation\nskf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\n# Arrays to store out-of-fold (OOF) and test predictions\noof_predictions = np.zeros((len(train_df), CFG.num_classes))\ntest_predictions = np.zeros((len(test_df), CFG.num_classes))\noof_labels = np.zeros(len(train_df))\n\n# Start the cross-validation loop\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n    print(\"-\" * 50)\n    print(f\"=============== FOLD {fold + 1}/{CFG.n_splits} ===============\")\n    print(\"-\" * 50)\n    \n    # --- Create datasets for the current fold ---\n    train_fold_ds = tokenized_train_ds.select(train_idx)\n    val_fold_ds = tokenized_train_ds.select(val_idx)\n    \n    # Store true labels for the validation set for final OOF evaluation\n    oof_labels[val_idx] = train_df['label'].iloc[val_idx].values\n    \n    # --- Initialize Model ---\n    model = AutoModelForSequenceClassification.from_pretrained(\n        CFG.model_name, \n        num_labels=CFG.num_classes\n    )\n    \n    # --- Define Training Arguments ---\n    training_args = TrainingArguments(\n        output_dir=f\"{CFG.output_path}/fold_{fold}\",\n        # Training Strategy\n        learning_rate=CFG.learning_rate,\n        per_device_train_batch_size=CFG.train_batch_size,\n        per_device_eval_batch_size=CFG.eval_batch_size,\n        num_train_epochs=CFG.epochs,\n        weight_decay=CFG.weight_decay,\n        fp16=True,  # Use mixed precision for speed and memory efficiency\n        \n        # Evaluation and Checkpointing\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,  # Load the best model based on the metric\n        metric_for_best_model=\"log_loss\", # Use log_loss as the primary metric\n        greater_is_better=False,       # Lower log_loss is better\n        \n        # Logging and Reporting\n        logging_strategy=\"epoch\",\n        report_to=\"none\", # Disable reporting to external services like wandb\n        save_total_limit=1, # Only keep the best checkpoint\n    )\n    \n    # --- Initialize Trainer ---\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_fold_ds,\n        eval_dataset=val_fold_ds,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    \n    # --- Train the model ---\n    print(\"Starting training for fold...\", fold+1)\n    trainer.train()\n    \n    # --- Generate Predictions ---\n    print(\"Generating OOF predictions for fold...\", fold+1)\n    # Get predictions on the validation set for this fold\n    val_preds = trainer.predict(val_fold_ds)\n    oof_predictions[val_idx] = torch.nn.functional.softmax(torch.from_numpy(val_preds.predictions), dim=-1).numpy()\n    \n    print(\"Generating Test predictions for fold...\", fold+1)\n    # Get predictions on the test set\n    test_preds = trainer.predict(tokenized_test_ds)\n    # Average test predictions over folds\n    test_predictions += torch.nn.functional.softmax(torch.from_numpy(test_preds.predictions), dim=-1).numpy() / CFG.n_splits\n    \n    # --- Clean up resources for the next fold ---\n    del model, trainer\n    torch.cuda.empty_cache()\n\n# =========================================================================================\n# === 6. FINAL EVALUATION AND SUBMISSION ================================================\n# =========================================================================================\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"====== CROSS-VALIDATION FINISHED ======\")\nprint(\"=\" * 50)\n\n# Calculate final OOF metrics on the entire training set\nprint(\"Calculating final OOF metrics...\")\nfinal_oof_metrics = compute_metrics((oof_predictions, oof_labels))\n\nprint(\"\\nFinal OOF Metrics:\")\nfor metric, value in final_oof_metrics.items():\n    print(f\"- {metric}: {value:.5f}\")\n\n# --- Create submission file ---\nprint(\"\\nCreating submission file...\")\nsubmission_df[CFG.target_cols] = test_predictions\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nSubmission file 'submission.csv' created successfully!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T12:57:56.132204Z","iopub.execute_input":"2025-07-01T12:57:56.132481Z","iopub.status.idle":"2025-07-01T12:58:25.154265Z","shell.execute_reply.started":"2025-07-01T12:57:56.132461Z","shell.execute_reply":"2025-07-01T12:58:25.153306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}