{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":68699,"databundleVersionId":7659021,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:39:06.498999Z","iopub.execute_input":"2025-07-03T03:39:06.499234Z","iopub.status.idle":"2025-07-03T03:39:09.152465Z","shell.execute_reply.started":"2025-07-03T03:39:06.499212Z","shell.execute_reply":"2025-07-03T03:39:09.151835Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e3/sample_submission.csv\n/kaggle/input/playground-series-s4e3/train.csv\n/kaggle/input/playground-series-s4e3/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# 1. SETUP - Import Libraries and Define Configuration\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, log_loss, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(f\"TensorFlow Decision Forests Version: {tfdf.__version__}\")\n\n# Configuration class to hold all constants\nclass Config:\n    # File paths\n    TRAIN_PATH = '/kaggle/input/playground-series-s4e3/train.csv'\n    TEST_PATH = '/kaggle/input/playground-series-s4e3/test.csv'\n    SAMPLE_SUBMISSION_PATH = '/kaggle/input/playground-series-s4e3/sample_submission.csv'\n    SUBMISSION_PATH = 'submission.csv'\n\n    # Data columns\n    ID_COLUMN = 'id'\n    TARGET_COLUMNS = [\n        'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n        'Dirtiness', 'Bumps', 'Other_Faults'\n    ]\n\n    # Model parameters\n    VALIDATION_SET_SIZE = 0.2\n    RANDOM_STATE = 42\n    BATCH_SIZE = 128\n\n# Instantiate the configuration\nconfig = Config()\n\n# ==============================================================================\n# 2. DATA LOADING & INITIAL ANALYSIS\n# ==============================================================================\nprint(\"\\n[INFO] Loading data...\")\ntrain_df = pd.read_csv(config.TRAIN_PATH)\ntest_df = pd.read_csv(config.TEST_PATH)\nsample_submission_df = pd.read_csv(config.SAMPLE_SUBMISSION_PATH)\n\nfeature_cols = [col for col in train_df.columns if col not in [config.ID_COLUMN] + config.TARGET_COLUMNS]\n\nprint(f\"Training data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\nprint(f\"Number of features: {len(feature_cols)}\")\nprint(\"Features:\", feature_cols)\nprint(\"Targets:\", config.TARGET_COLUMNS)\n\n# ==============================================================================\n# 3. DATA SPLITTING\n# ==============================================================================\nprint(f\"\\n[INFO] Splitting training data into training and validation sets (test_size={config.VALIDATION_SET_SIZE})...\")\n\nX = train_df[feature_cols]\ny = train_df[config.TARGET_COLUMNS]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y,\n    test_size=config.VALIDATION_SET_SIZE,\n    random_state=config.RANDOM_STATE,\n)\n\n# Re-combine features and labels for easier conversion to tf.data.Dataset\ntrain_split_df = pd.concat([X_train, y_train], axis=1)\nval_split_df = pd.concat([X_val, y_val], axis=1)\n\nprint(f\"Training split shape: {train_split_df.shape}\")\nprint(f\"Validation split shape: {val_split_df.shape}\")\n\n# ==============================================================================\n# 4. DATA CONVERSION TO TENSORFLOW DATASET\n# ==============================================================================\nprint(\"\\n[INFO] Converting pandas DataFrames to TensorFlow Datasets...\")\n\ndef to_tf_dataset(dataframe: pd.DataFrame, is_training=True):\n    features_dict = {col: np.array(dataframe[col]) for col in feature_cols}\n    if is_training:\n        labels_dict = {col: np.array(dataframe[col]) for col in config.TARGET_COLUMNS}\n        dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels_dict))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n    # Use prefetch for better performance\n    dataset = dataset.batch(config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ntrain_ds = to_tf_dataset(train_split_df, is_training=True)\nval_ds = to_tf_dataset(val_split_df, is_training=True)\ntest_ds = to_tf_dataset(test_df, is_training=False)\n\n# ==============================================================================\n# 5. MODEL DEFINITION (CORRECTED)\n# ==============================================================================\nprint(\"\\n[INFO] Defining the TensorFlow Decision Forests model...\")\n\n# CORRECTED: Removed the `loss` argument as it's not a valid parameter for MultiTaskItem.\n# The loss function is automatically inferred from the `task` type.\nmulti_task_items = [\n    tfdf.keras.MultiTaskItem(\n        label=label,\n        task=tfdf.keras.Task.CLASSIFICATION\n    ) for label in config.TARGET_COLUMNS\n]\n\n# Note: TF-DF GBT models are trained on the CPU. The GPU devices detected will not be used for training this model.\nmodel = tfdf.keras.GradientBoostedTreesModel(\n    multitask=multi_task_items,\n    verbose=0,  # Set to 1 to see detailed training logs\n    growing_strategy=\"BEST_FIRST_GLOBAL\",\n    max_depth=8,\n    num_trees=1000,\n)\n\n# The compile step is lightweight and is mainly for setting up metrics to monitor.\nmodel.compile(metrics=[tf.keras.metrics.AUC(name=\"auc\", curve='ROC')])\n\n# ==============================================================================\n# 6. MODEL TRAINING\n# ==============================================================================\nprint(\"\\n[INFO] Starting model training...\")\nprint(\"This may take a few minutes...\")\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=1 # For GBT, the entire forest is built in one \"epoch\"\n)\n\nprint(\"\\n[INFO] Model training finished.\")\n\n# ==============================================================================\n# 7. MODEL EVALUATION\n# ==============================================================================\nprint(\"\\n[INFO] Evaluating model performance on the validation set...\")\n\nval_predictions = model.predict(val_ds)\n\ny_val_true = y_val.values\ny_val_pred_probs = np.hstack([val_predictions[label] for label in config.TARGET_COLUMNS])\ny_val_pred_labels = (y_val_pred_probs > 0.5).astype(int)\n\nmetrics_summary = {}\noverall_roc_auc = roc_auc_score(y_val_true, y_val_pred_probs, average='macro')\nmetrics_summary['Overall ROC AUC (Competition Metric)'] = overall_roc_auc\n\nall_roc_aucs, all_log_losses, all_f1_macros, all_f1_weighteds, all_accuracies = [], [], [], [], []\n\nfor i, label in enumerate(config.TARGET_COLUMNS):\n    all_roc_aucs.append(roc_auc_score(y_val_true[:, i], y_val_pred_probs[:, i]))\n    all_log_losses.append(log_loss(y_val_true[:, i], y_val_pred_probs[:, i]))\n    all_f1_macros.append(f1_score(y_val_true[:, i], y_val_pred_labels[:, i], average='macro', zero_division=0))\n    all_f1_weighteds.append(f1_score(y_val_true[:, i], y_val_pred_labels[:, i], average='weighted', zero_division=0))\n    all_accuracies.append(accuracy_score(y_val_true[:, i], y_val_pred_labels[:, i]))\n\nmetrics_summary['avg_accuracy'] = np.mean(all_accuracies)\nmetrics_summary['avg_log_loss'] = np.mean(all_log_losses)\nmetrics_summary['avg_f1_macro'] = np.mean(all_f1_macros)\nmetrics_summary['avg_f1_weighted'] = np.mean(all_f1_weighteds)\nmetrics_summary['roc_auc_ovr'] = overall_roc_auc\nmetrics_summary['roc_auc_ovr_weighted'] = roc_auc_score(y_val_true, y_val_pred_probs, average='weighted')\n\nprint(\"\\n--- Validation Metrics Summary ---\")\nfor name, value in metrics_summary.items():\n    print(f\"{name:<35}: {value:.5f}\")\nprint(\"----------------------------------\\n\")\n\nprint(\"--- Per-Target ROC AUC Scores ---\")\nfor i, label in enumerate(config.TARGET_COLUMNS):\n    print(f\"{label:<15}: {all_roc_aucs[i]:.5f}\")\nprint(\"---------------------------------\\n\")\n\n# ==============================================================================\n# 8. PREDICTION AND SUBMISSION\n# ==============================================================================\nprint(\"[INFO] Generating predictions on the test set...\")\ntest_predictions = model.predict(test_ds)\n\nprint(\"[INFO] Creating submission file...\")\nsubmission_df = pd.DataFrame({config.ID_COLUMN: test_df[config.ID_COLUMN]})\n\nfor label in config.TARGET_COLUMNS:\n    submission_df[label] = test_predictions[label].flatten()\n\nsubmission_df.to_csv(config.SUBMISSION_PATH, index=False)\n\nprint(f\"\\n[SUCCESS] Submission file '{config.SUBMISSION_PATH}' created successfully!\")\nprint(\"Submission file head:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:45:18.145706Z","iopub.execute_input":"2025-07-03T03:45:18.146016Z","iopub.status.idle":"2025-07-03T03:45:18.398962Z","shell.execute_reply.started":"2025-07-03T03:45:18.145993Z","shell.execute_reply":"2025-07-03T03:45:18.397777Z"}},"outputs":[{"name":"stdout","text":"TensorFlow Version: 2.18.0\nTensorFlow Decision Forests Version: 1.11.0\n\n[INFO] Loading data...\nTraining data shape: (19219, 35)\nTest data shape: (12814, 28)\nNumber of features: 27\nFeatures: ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas']\nTargets: ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n\n[INFO] Splitting training data into training and validation sets (test_size=0.2)...\nTraining split shape: (15375, 34)\nValidation split shape: (3844, 34)\n\n[INFO] Converting pandas DataFrames to TensorFlow Datasets...\n\n[INFO] Defining the TensorFlow Decision Forests model...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2234510324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# Note: TF-DF GBT models are trained on the CPU. The GPU devices detected will not be used for training this model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m model = tfdf.keras.GradientBoostedTreesModel(\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mmultitask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_task_items\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Set to 1 to see detailed training logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_decision_forests/keras/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m     )\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_decision_forests/keras/wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, features, exclude_non_specified_features, preprocessing, postprocessing, training_preprocessing, ranking_group, uplift_treatment, temp_directory, verbose, hyperparameter_template, advanced_arguments, num_threads, name, max_vocab_count, try_resume_training, check_dataset, tuner, discretize_numerical_features, num_discretized_numerical_bins, multitask, adapt_subsample_for_maximum_training_duration, allow_na_conditions, apply_link_function, categorical_algorithm, categorical_set_split_greedy_sampling, categorical_set_split_max_num_items, categorical_set_split_min_item_frequency, compute_permutation_variable_importance, cross_entropy_ndcg_truncation, dart_dropout, early_stopping, early_stopping_initial_iteration, early_stopping_num_trees_look_ahead, focal_loss_alpha, focal_loss_gamma, forest_extraction, goss_alpha, goss_beta, growing_strategy, honest, honest_fixed_separation, honest_ratio_leaf_examples, in_split_min_examples_check, keep_non_leaf_label_distribution, l1_regularization, l2_categorical_regularization, l2_regularization, lambda_loss, loss, max_depth, max_num_nodes, maximum_model_size_in_memory_in_bytes, maximum_training_duration_seconds, mhld_oblique_max_num_attributes, mhld_oblique_sample_attributes, min_examples, missing_value_policy, ndcg_truncation, num_candidate_attributes, num_candidate_attributes_ratio, num_trees, pure_serving_model, random_seed, sampling_method, selective_gradient_boosting_ratio, shrinkage, sorting_strategy, sparse_oblique_max_num_projections, sparse_oblique_normalization, sparse_oblique_num_projections_exponent, sparse_oblique_projection_density_factor, sparse_oblique_weights, split_axis, subsample, uplift_min_examples_in_treatment, uplift_split_score, use_hessian_gain, validation_interval_in_trees, validation_ratio, explicit_args)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         explicit_args)\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m     super(GradientBoostedTreesModel, self).__init__(task=task,\n\u001b[0m\u001b[1;32m   1469\u001b[0m       \u001b[0mlearner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GRADIENT_BOOSTED_TREES\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0mlearner_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_decision_forests/keras/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, learner, learner_params, features, exclude_non_specified_features, preprocessing, postprocessing, training_preprocessing, ranking_group, uplift_treatment, temp_directory, verbose, advanced_arguments, num_threads, name, max_vocab_count, try_resume_training, check_dataset, tuner, discretize_numerical_features, num_discretized_numerical_bins, multitask)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_validation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_decision_forests/keras/core.py\u001b[0m in \u001b[0;36m_check_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2282\u001b[0m     \u001b[0;34m\"\"\"Check the validity of the learner parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m     training_config = copy.deepcopy(\n\u001b[0m\u001b[1;32m   2285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advanced_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myggdrasil_training_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     )\n","\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/protobuf/message.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mclone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1332\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0;31m# Construct a new object to represent this field.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m             \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m             \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m           \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'FieldDescriptor' object has no attribute '_default_constructor'"],"ename":"AttributeError","evalue":"'FieldDescriptor' object has no attribute '_default_constructor'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}