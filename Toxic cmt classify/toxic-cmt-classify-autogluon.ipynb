{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":19018,"databundleVersionId":2703900,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:29:51.639745Z","iopub.execute_input":"2025-07-02T03:29:51.639941Z","iopub.status.idle":"2025-07-02T03:29:52.017017Z","shell.execute_reply.started":"2025-07-02T03:29:51.639922Z","shell.execute_reply":"2025-07-02T03:29:52.015865Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#---------------------------------------------------\n# PHẦN 1: CÀI ĐẶT VÀ CHUẨN BỊ DỮ LIỆU\n#---------------------------------------------------\n\n# 1.1. Cài đặt thư viện AutoGluon\n# Lệnh này cần thiết vì AutoGluon không có sẵn trên Kaggle\n!pip install autogluon --quiet\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularPredictor\n\nprint(\"=\"*50)\nprint(\"PHẦN 1: ĐANG TẢI VÀ CHUẨN BỊ DỮ LIỆU\")\nprint(\"=\"*50)\n\n# 1.2. Khai báo đường dẫn và tải dữ liệu\npath_toxic_comment = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv'\npath_unintended_bias = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv'\n\ntry:\n    df_toxic = pd.read_csv(path_toxic_comment)\n    df_bias = pd.read_csv(path_unintended_bias)\n\n    # Ghép nối và chuẩn hóa dữ liệu như các bước trước\n    df_toxic_subset = df_toxic[['comment_text', 'toxic']]\n    df_bias_subset = df_bias[['comment_text', 'toxic']]\n    full_train_df = pd.concat([df_toxic_subset, df_bias_subset], ignore_index=True)\n    full_train_df['toxic'] = full_train_df['toxic'].apply(lambda x: 1 if x >= 0.5 else 0)\n    \n    # Để chạy nhanh hơn cho ví dụ này, chúng ta sẽ lấy một mẫu nhỏ\n    # BỎ CHÚ THÍCH DÒNG DƯỚI ĐÂY NẾU BẠN MUỐN CHẠY TRÊN TOÀN BỘ DỮ LIỆU (sẽ mất rất nhiều thời gian)\n    full_train_df = full_train_df.sample(n=100000, random_state=42)\n    \n    print(f\"Đã tạo DataFrame training tổng hợp với {len(full_train_df)} mẫu.\")\n\nexcept FileNotFoundError as e:\n    print(f\"\\nLỖI: Không tìm thấy file training. Quy trình dừng lại.\")\n    print(f\"Chi tiết lỗi: {e}\")\n    exit()\n\n#---------------------------------------------------\n# PHẦN 2: CHIA DỮ LIỆU (80% TRAIN, 20% TEST)\n#---------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"PHẦN 2: CHIA DỮ LIỆU THÀNH TẬP TRAIN VÀ TEST\")\nprint(\"=\"*50)\n\n# Chia dữ liệu thành 80% train và 20% test (để đánh giá cuối cùng)\n# stratify=full_train_df['toxic'] rất quan trọng để đảm bảo tỉ lệ nhãn 'toxic'\n# là như nhau trong cả hai tập train và test.\ntrain_data, test_data = train_test_split(\n    full_train_df,\n    test_size=0.2,\n    random_state=42,\n    stratify=full_train_df['toxic']\n)\n\nprint(f\"Kích thước tập Train: {train_data.shape}\")\nprint(f\"Kích thước tập Test: {test_data.shape}\")\nprint(f\"Phân phối nhãn trong tập Train:\\n{train_data['toxic'].value_counts(normalize=True)}\")\nprint(f\"Phân phối nhãn trong tập Test:\\n{test_data['toxic'].value_counts(normalize=True)}\")\n\n\n#---------------------------------------------------\n# PHẦN 3: HUẤN LUYỆN VỚI AUTOGLUON\n#---------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"PHẦN 3: BẮT ĐẦU HUẤN LUYỆN VỚI AUTOGLUON\")\nprint(\"=\"*50)\n\n# Khởi tạo TabularPredictor\n# AutoGluon sẽ tự động xử lý cột 'comment_text' như một đặc trưng văn bản\npredictor = TabularPredictor(\n    label='toxic',                # Cột mục tiêu cần dự đoán\n    problem_type='binary',        # Loại bài toán: phân loại nhị phân\n    eval_metric='roc_auc',        # Thước đo để tối ưu, phù hợp với cuộc thi\n    path='./ag_models_toxic'      # Thư mục để lưu các mô hình đã huấn luyện\n)\n\n# Huấn luyện mô hình\n# AutoGluon sẽ thử nhiều mô hình khác nhau và kết hợp chúng lại\n# time_limit là giới hạn thời gian huấn luyện (tính bằng giây)\n# presets='best_quality' để có kết quả tốt nhất, bạn có thể dùng 'high_quality' hoặc 'medium_quality' để nhanh hơn\npredictor.fit(\n    train_data,\n    time_limit=1800, # Giới hạn thời gian 30 phút. Tăng lên để có kết quả tốt hơn.\n    presets='high_quality'\n)\n\n#---------------------------------------------------\n# PHẦN 4: ĐÁNH GIÁ MÔ HÌNH TRÊN TẬP TEST\n#---------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"PHẦN 4: ĐÁNH GIÁ HIỆU SUẤT MÔ HÌNH\")\nprint(\"=\"*50)\n\n# Xem bảng xếp hạng các mô hình đã được huấn luyện\nprint(\"Bảng xếp hạng các mô hình (đánh giá trên tập validation nội bộ của AutoGluon):\")\nleaderboard = predictor.leaderboard(silent=True)\nprint(leaderboard)\n\n# Đánh giá hiệu suất trên tập test 20% mà chúng ta đã tách ra\nprint(\"\\nĐánh giá trên tập test (20% dữ liệu giữ lại):\")\nperformance = predictor.evaluate(test_data)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:31:21.225794Z","iopub.execute_input":"2025-07-02T03:31:21.226079Z","iopub.status.idle":"2025-07-02T04:12:43.675269Z","shell.execute_reply.started":"2025-07-02T03:31:21.226057Z","shell.execute_reply":"2025-07-02T04:12:43.674188Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.5/454.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.4/382.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.8/275.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\ntextblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m==================================================\nPHẦN 1: ĐANG TẢI VÀ CHUẨN BỊ DỮ LIỆU\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.3.1\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       27.54 GB / 31.35 GB (87.8%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['high_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n\tYou can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n","output_type":"stream"},{"name":"stdout","text":"Đã tạo DataFrame training tổng hợp với 100000 mẫu.\n\n==================================================\nPHẦN 2: CHIA DỮ LIỆU THÀNH TẬP TRAIN VÀ TEST\n==================================================\nKích thước tập Train: (80000, 2)\nKích thước tập Test: (20000, 2)\nPhân phối nhãn trong tập Train:\ntoxic\n0    0.9191\n1    0.0809\nName: proportion, dtype: float64\nPhân phối nhãn trong tập Test:\ntoxic\n0    0.9191\n1    0.0809\nName: proportion, dtype: float64\n\n==================================================\nPHẦN 3: BẮT ĐẦU HUẤN LUYỆN VỚI AUTOGLUON\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2025-07-02 03:34:52,830\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n\t\tContext path: \"/kaggle/working/ag_models_toxic/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=311)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=311)\u001b[0m Beginning AutoGluon training ... Time limit = 443s\n\u001b[36m(_dystack pid=311)\u001b[0m AutoGluon will save models to \"/kaggle/working/ag_models_toxic/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=311)\u001b[0m Train Data Rows:    71111\n\u001b[36m(_dystack pid=311)\u001b[0m Train Data Columns: 1\n\u001b[36m(_dystack pid=311)\u001b[0m Label Column:       toxic\n\u001b[36m(_dystack pid=311)\u001b[0m Problem Type:       binary\n\u001b[36m(_dystack pid=311)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=311)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n\u001b[36m(_dystack pid=311)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \tAvailable Memory:                    27625.52 MB\n\u001b[36m(_dystack pid=311)\u001b[0m \tTrain Data (Original)  Memory Usage: 27.39 MB (0.1% of available memory)\n\u001b[36m(_dystack pid=311)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=311)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting TextSpecialFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t\tFitting BinnedFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting TextNgramFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t\tFitting CountVectorizer for text features: ['comment_text']\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t\tCountVectorizer fit with vocabulary size = 10000\n\u001b[36m(_dystack pid=311)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=311)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t('object', ['text']) : 1 | ['comment_text']\n\u001b[36m(_dystack pid=311)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t('category', ['text_as_category'])  :    1 | ['comment_text']\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t('int', ['binned', 'text_special']) :   30 | ['comment_text.char_count', 'comment_text.word_count', 'comment_text.capital_ratio', 'comment_text.lower_ratio', 'comment_text.digit_ratio', ...]\n\u001b[36m(_dystack pid=311)\u001b[0m \t\t('int', ['text_ngram'])             : 9602 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 000', '__nlp__.01', '__nlp__.02', ...]\n\u001b[36m(_dystack pid=311)\u001b[0m \t137.3s = Fit runtime\n\u001b[36m(_dystack pid=311)\u001b[0m \t1 features in original data used to generate 9633 features in processed data.\n\u001b[36m(_dystack pid=311)\u001b[0m \tTrain Data (Processed) Memory Usage: 1304.46 MB (4.8% of available memory)\n\u001b[36m(_dystack pid=311)\u001b[0m Data preprocessing and feature engineering runtime = 149.24s ...\n\u001b[36m(_dystack pid=311)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n\u001b[36m(_dystack pid=311)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n\u001b[36m(_dystack pid=311)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=311)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=311)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=311)\u001b[0m {\n\u001b[36m(_dystack pid=311)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=311)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=311)\u001b[0m }\n\u001b[36m(_dystack pid=311)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 196.09s of the 294.20s of remaining time.\n\u001b[36m(_dystack pid=311)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 9.864 GB out of 27.187 GB available memory (36.283%)... (20.000% of avail memory is the max safe size)\n\u001b[36m(_dystack pid=311)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.86 to avoid the error)\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\u001b[36m(_dystack pid=311)\u001b[0m \tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 165.66s of the 263.77s of remaining time.\n\u001b[36m(_dystack pid=311)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 9.864 GB out of 27.144 GB available memory (36.340%)... (20.000% of avail memory is the max safe size)\n\u001b[36m(_dystack pid=311)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.87 to avoid the error)\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\u001b[36m(_dystack pid=311)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\u001b[36m(_dystack pid=311)\u001b[0m \tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 140.83s of the 238.94s of remaining time.\n\u001b[36m(_dystack pid=311)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.52% memory usage per fold, 79.04%/80.00% total).\n\u001b[36m(_dystack pid=311)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=39.52%)\n\u001b[36m(_ray_fit pid=585)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n\u001b[36m(_ray_fit pid=585)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.273692\n\u001b[36m(_ray_fit pid=679)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n\u001b[36m(_ray_fit pid=679)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.270422\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=768)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=768)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.273469\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=857)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=857)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.271571\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.6939\t = Validation score   (roc_auc)\n\u001b[36m(_dystack pid=311)\u001b[0m \t319.92s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m \t13.04s\t = Validation runtime\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 294.21s of the -110.34s of remaining time.\n\u001b[36m(_dystack pid=311)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.6939\t = Validation score   (roc_auc)\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.01s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.07s\t = Validation runtime\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 294.21s of the -111.49s of remaining time.\n\u001b[36m(_dystack pid=311)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.6939\t = Validation score   (roc_auc)\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.01s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.01s\t = Validation runtime\n\u001b[36m(_dystack pid=311)\u001b[0m AutoGluon training complete, total runtime = 557.19s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 681.4 rows/s (8889 batch size)\n\u001b[36m(_dystack pid=311)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n\u001b[36m(_dystack pid=311)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n\u001b[36m(_dystack pid=311)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n\u001b[36m(_dystack pid=311)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n\u001b[36m(_dystack pid=311)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n\u001b[36m(_ray_fit pid=858)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n\u001b[36m(_ray_fit pid=858)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.272532\n\u001b[36m(_dystack pid=311)\u001b[0m \t43.88s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n\u001b[36m(_dystack pid=311)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.01s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n\u001b[36m(_dystack pid=311)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=311)\u001b[0m \t0.01s\t = Training   runtime\n\u001b[36m(_dystack pid=311)\u001b[0m Updated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\n\u001b[36m(_dystack pid=311)\u001b[0m Refit complete, total runtime = 62.95s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\n\u001b[36m(_dystack pid=311)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_models_toxic/ds_sub_fit/sub_fit_ho\")\n\u001b[36m(_dystack pid=311)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                      model  score_holdout  score_val eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0    LightGBMXT_BAG_L1_FULL       0.688227   0.693944     roc_auc        1.905623          None  43.881682                 1.905623                   None          43.881682            1       True          1\n1  WeightedEnsemble_L3_FULL       0.688227   0.693944     roc_auc        1.907188          None  43.891681                 0.001565                   None           0.009999            3       True          3\n2  WeightedEnsemble_L2_FULL       0.688227   0.693944     roc_auc        1.908150          None  43.894891                 0.002527                   None           0.013209            2       True          2\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t634s\t = DyStack   runtime |\t1166s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 1166s\nAutoGluon will save models to \"/kaggle/working/ag_models_toxic\"\nTrain Data Rows:    80000\nTrain Data Columns: 1\nLabel Column:       toxic\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    24476.53 MB\n\tTrain Data (Original)  Memory Usage: 32.39 MB (0.1% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\t\tFitting TextSpecialFeatureGenerator...\n\t\t\tFitting BinnedFeatureGenerator...\n\t\t\tFitting DropDuplicatesFeatureGenerator...\n\t\tFitting TextNgramFeatureGenerator...\n\t\t\tFitting CountVectorizer for text features: ['comment_text']\n\t\t\tCountVectorizer fit with vocabulary size = 10000\n\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n\t\tReducing Vectorizer vocab size from 10000 to 6514 to avoid OOM error\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('object', ['text']) : 1 | ['comment_text']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', ['text_as_category'])  :    1 | ['comment_text']\n\t\t('int', ['binned', 'text_special']) :   30 | ['comment_text.char_count', 'comment_text.word_count', 'comment_text.capital_ratio', 'comment_text.lower_ratio', 'comment_text.digit_ratio', ...]\n\t\t('int', ['text_ngram'])             : 6431 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 000', '__nlp__.08', '__nlp__.10', ...]\n\t142.1s = Fit runtime\n\t1 features in original data used to generate 6462 features in processed data.\n\tTrain Data (Processed) Memory Usage: 983.73 MB (3.7% of available memory)\nData preprocessing and feature engineering runtime = 152.14s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 110 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 675.70s of the 1013.79s of remaining time.\n\tWarning: Not enough memory to safely train model. Estimated to require 7.444 GB out of 26.653 GB available memory (27.930%)... (20.000% of avail memory is the max safe size)\n\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.45 to avoid the error)\n\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 657.17s of the 995.27s of remaining time.\n\tWarning: Not enough memory to safely train model. Estimated to require 7.444 GB out of 26.616 GB available memory (27.969%)... (20.000% of avail memory is the max safe size)\n\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.45 to avoid the error)\n\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 642.13s of the 980.23s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.78% memory usage per fold, 59.55%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=29.78%)\n\t0.8752\t = Validation score   (roc_auc)\n\t339.48s\t = Training   runtime\n\t15.08s\t = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 280.22s of the 618.31s of remaining time.\n\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.42% memory usage per fold, 58.84%/80.00% total).\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=2, gpus=0, memory=29.42%)\n\t0.8698\t = Validation score   (roc_auc)\n\t244.36s\t = Training   runtime\n\t11.58s\t = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 16.92s of the 355.01s of remaining time.\n\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 28.31s compared to 10s of available time.\n\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -346.72s of remaining time.\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.571, 'LightGBM_BAG_L1': 0.429}\n\t0.8784\t = Validation score   (roc_auc)\n\t0.73s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting 108 L2 models, fit_strategy=\"sequential\" ...\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -348.78s of remaining time.\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.571, 'LightGBM_BAG_L1': 0.429}\n\t0.8784\t = Validation score   (roc_auc)\n\t0.73s\t = Training   runtime\n\t0.01s\t = Validation runtime\nAutoGluon training complete, total runtime = 1517.24s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 375.1 rows/s (10000 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n\t47.04s\t = Training   runtime\nFitting 1 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBM_BAG_L1_FULL ...\n\t39.1s\t = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.571, 'LightGBM_BAG_L1': 0.429}\n\t0.73s\t = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.571, 'LightGBM_BAG_L1': 0.429}\n\t0.73s\t = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 109.27s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_models_toxic\")\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nPHẦN 4: ĐÁNH GIÁ HIỆU SUẤT MÔ HÌNH\n==================================================\nBảng xếp hạng các mô hình (đánh giá trên tập validation nội bộ của AutoGluon):\n                      model  score_val eval_metric  pred_time_val    fit_time  \\\n0       WeightedEnsemble_L2   0.878422     roc_auc      26.672686  584.572414   \n1       WeightedEnsemble_L3   0.878422     roc_auc      26.673401  584.574960   \n2         LightGBMXT_BAG_L1   0.875186     roc_auc      15.078219  339.478667   \n3           LightGBM_BAG_L1   0.869766     roc_auc      11.580819  244.361953   \n4  WeightedEnsemble_L3_FULL        NaN     roc_auc            NaN   86.874781   \n5  WeightedEnsemble_L2_FULL        NaN     roc_auc            NaN   86.872235   \n6      LightGBM_BAG_L1_FULL        NaN     roc_auc            NaN   39.095640   \n7    LightGBMXT_BAG_L1_FULL        NaN     roc_auc            NaN   47.044801   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.013647           0.731794            2      False   \n1                0.014363           0.734340            3      False   \n2               15.078219         339.478667            1      False   \n3               11.580819         244.361953            1      False   \n4                     NaN           0.734340            3       True   \n5                     NaN           0.731794            2       True   \n6                     NaN          39.095640            1       True   \n7                     NaN          47.044801            1       True   \n\n   fit_order  \n0          3  \n1          4  \n2          1  \n3          2  \n4          8  \n5          7  \n6          6  \n7          5  \n\nĐánh giá trên tập test (20% dữ liệu giữ lại):\n{'roc_auc': 0.8808101862156494, 'accuracy': 0.9394, 'balanced_accuracy': 0.6637914582694228, 'mcc': 0.4935895982152019, 'f1': 0.4721254355400697, 'precision': 0.799410029498525, 'recall': 0.33498145859085293}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"predictor.leaderboard(test_data, extra_metrics = ['accuracy', 'log_loss', 'f1_macro', 'f1_weighted', 'roc_auc'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T04:15:50.266683Z","iopub.execute_input":"2025-07-02T04:15:50.267030Z","iopub.status.idle":"2025-07-02T04:16:05.045321Z","shell.execute_reply.started":"2025-07-02T04:15:50.267004Z","shell.execute_reply":"2025-07-02T04:16:05.044368Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                      model  score_test  accuracy  log_loss  f1_macro  \\\n0  WeightedEnsemble_L3_FULL    0.880810   0.93940 -0.183294  0.719990   \n1  WeightedEnsemble_L2_FULL    0.880810   0.93940 -0.183294  0.719990   \n2    LightGBMXT_BAG_L1_FULL    0.879606   0.93950 -0.181599  0.727571   \n3      LightGBM_BAG_L1_FULL    0.868408   0.93715 -0.190716  0.699108   \n4       WeightedEnsemble_L2         NaN       NaN       NaN       NaN   \n5       WeightedEnsemble_L3         NaN       NaN       NaN       NaN   \n6         LightGBMXT_BAG_L1         NaN       NaN       NaN       NaN   \n7           LightGBM_BAG_L1         NaN       NaN       NaN       NaN   \n\n   f1_weighted   roc_auc  score_val eval_metric  pred_time_test  \\\n0     0.927750  0.880810        NaN     roc_auc        9.669240   \n1     0.927750  0.880810        NaN     roc_auc        9.670047   \n2     0.928976  0.879606        NaN     roc_auc        5.081077   \n3     0.923434  0.868408        NaN     roc_auc        4.586161   \n4          NaN       NaN   0.878422     roc_auc             NaN   \n5          NaN       NaN   0.878422     roc_auc             NaN   \n6          NaN       NaN   0.875186     roc_auc             NaN   \n7          NaN       NaN   0.869766     roc_auc             NaN   \n\n   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0            NaN   86.874781                 0.002003                     NaN   \n1            NaN   86.872235                 0.002809                     NaN   \n2            NaN   47.044801                 5.081077                     NaN   \n3            NaN   39.095640                 4.586161                     NaN   \n4      26.672686  584.572414                      NaN                0.013647   \n5      26.673401  584.574960                      NaN                0.014363   \n6      15.078219  339.478667                      NaN               15.078219   \n7      11.580819  244.361953                      NaN               11.580819   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           0.734340            3       True          8  \n1           0.731794            2       True          7  \n2          47.044801            1       True          5  \n3          39.095640            1       True          6  \n4           0.731794            2      False          3  \n5           0.734340            3      False          4  \n6         339.478667            1      False          1  \n7         244.361953            1      False          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>accuracy</th>\n      <th>log_loss</th>\n      <th>f1_macro</th>\n      <th>f1_weighted</th>\n      <th>roc_auc</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WeightedEnsemble_L3_FULL</td>\n      <td>0.880810</td>\n      <td>0.93940</td>\n      <td>-0.183294</td>\n      <td>0.719990</td>\n      <td>0.927750</td>\n      <td>0.880810</td>\n      <td>NaN</td>\n      <td>roc_auc</td>\n      <td>9.669240</td>\n      <td>NaN</td>\n      <td>86.874781</td>\n      <td>0.002003</td>\n      <td>NaN</td>\n      <td>0.734340</td>\n      <td>3</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2_FULL</td>\n      <td>0.880810</td>\n      <td>0.93940</td>\n      <td>-0.183294</td>\n      <td>0.719990</td>\n      <td>0.927750</td>\n      <td>0.880810</td>\n      <td>NaN</td>\n      <td>roc_auc</td>\n      <td>9.670047</td>\n      <td>NaN</td>\n      <td>86.872235</td>\n      <td>0.002809</td>\n      <td>NaN</td>\n      <td>0.731794</td>\n      <td>2</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBMXT_BAG_L1_FULL</td>\n      <td>0.879606</td>\n      <td>0.93950</td>\n      <td>-0.181599</td>\n      <td>0.727571</td>\n      <td>0.928976</td>\n      <td>0.879606</td>\n      <td>NaN</td>\n      <td>roc_auc</td>\n      <td>5.081077</td>\n      <td>NaN</td>\n      <td>47.044801</td>\n      <td>5.081077</td>\n      <td>NaN</td>\n      <td>47.044801</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LightGBM_BAG_L1_FULL</td>\n      <td>0.868408</td>\n      <td>0.93715</td>\n      <td>-0.190716</td>\n      <td>0.699108</td>\n      <td>0.923434</td>\n      <td>0.868408</td>\n      <td>NaN</td>\n      <td>roc_auc</td>\n      <td>4.586161</td>\n      <td>NaN</td>\n      <td>39.095640</td>\n      <td>4.586161</td>\n      <td>NaN</td>\n      <td>39.095640</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.878422</td>\n      <td>roc_auc</td>\n      <td>NaN</td>\n      <td>26.672686</td>\n      <td>584.572414</td>\n      <td>NaN</td>\n      <td>0.013647</td>\n      <td>0.731794</td>\n      <td>2</td>\n      <td>False</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>WeightedEnsemble_L3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.878422</td>\n      <td>roc_auc</td>\n      <td>NaN</td>\n      <td>26.673401</td>\n      <td>584.574960</td>\n      <td>NaN</td>\n      <td>0.014363</td>\n      <td>0.734340</td>\n      <td>3</td>\n      <td>False</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>LightGBMXT_BAG_L1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.875186</td>\n      <td>roc_auc</td>\n      <td>NaN</td>\n      <td>15.078219</td>\n      <td>339.478667</td>\n      <td>NaN</td>\n      <td>15.078219</td>\n      <td>339.478667</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.869766</td>\n      <td>roc_auc</td>\n      <td>NaN</td>\n      <td>11.580819</td>\n      <td>244.361953</td>\n      <td>NaN</td>\n      <td>11.580819</td>\n      <td>244.361953</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}