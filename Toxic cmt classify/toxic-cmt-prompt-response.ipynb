{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceType":"competition","sourceId":19018,"databundleVersionId":2703900},{"sourceType":"modelInstanceVersion","sourceId":206209,"databundleVersionId":10565160,"modelInstanceId":4721}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:09:04.085725Z","iopub.execute_input":"2025-07-02T14:09:04.086063Z","iopub.status.idle":"2025-07-02T14:09:07.296012Z","shell.execute_reply.started":"2025-07-02T14:09:04.086037Z","shell.execute_reply":"2025-07-02T14:09:07.289486Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nKaggle Solution for Jigsaw Multilingual Toxic Comment Classification.\n\nThis program implements an end-to-end pipeline for a cross-lingual text classification task.\nIt uses a pre-trained multilingual transformer model (XLM-Roberta) to classify online\ncomments as toxic or non-toxic. The model is fine-tuned on English-only data and\nevaluated on non-English data to test its generalization capabilities.\n\nThe solution is optimized to run on a Google Cloud TPU (v3-8), leveraging TensorFlow's\ndistribution strategies for efficient, large-scale training.\n\"\"\"\n\nimport os\nimport sys\nimport gc\n\n# Suppress verbose logging and warnings for a cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras_nlp\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Configuration ---\n# All hyperparameters and settings are centralized here for easy tuning.\nclass Config:\n    \"\"\"\n    Configuration class for model and training hyperparameters.\n    \"\"\"\n    # File Paths\n    BASE_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_TOXIC_PATH = os.path.join(BASE_PATH, \"jigsaw-toxic-comment-train.csv\")\n    TRAIN_BIAS_PATH = os.path.join(BASE_PATH, \"jigsaw-unintended-bias-train.csv\")\n    VALIDATION_PATH = os.path.join(BASE_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test.csv\")\n    SUBMISSION_PATH = \"submission.csv\"\n\n    # Model Configuration\n    # XLM-Roberta is a strong choice for multilingual tasks.\n    PRESET = \"xlm_roberta_base_multi\"\n    SEQUENCE_LENGTH = 192  # Max length of text sequences. Balances context and memory.\n\n    # Training Configuration\n    EPOCHS = 2  # Fine-tuning transformers requires only a few epochs.\n    BATCH_SIZE_PER_REPLICA = 16 # Batch size for each TPU core.\n    LEARNING_RATE = 2e-5  # A standard learning rate for fine-tuning transformers.\n    \n    # Set a random seed for reproducibility\n    SEED = 42\n\n# --- 2. TPU Initialization ---\n# This section detects and initializes the TPU for distributed training.\ndef initialize_tpu():\n    \"\"\"\n    Detects and initializes the TPU strategy.\n    Returns the distribution strategy and the global batch size.\n    \"\"\"\n    print(\"--- Initializing TPU ---\")\n    try:\n        tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu_resolver)\n        print(f\"TPU detected. Found {strategy.num_replicas_in_sync} replicas.\")\n    except ValueError:\n        print(\"TPU not detected. Falling back to CPU/GPU strategy.\")\n        strategy = tf.distribute.MirroredStrategy()\n\n    # Calculate the global batch size based on the number of replicas.\n    global_batch_size = Config.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    print(f\"Global batch size set to: {global_batch_size}\")\n    \n    # Enable mixed precision for performance boost on TPUs.\n    keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n    \n    return strategy, global_batch_size\n\n# --- 3. Data Loading and Preparation ---\ndef load_and_prepare_data():\n    \"\"\"\n    Loads, preprocesses, and combines the training, validation, and test datasets.\n    \"\"\"\n    print(\"\\n--- Loading and Preparing Data ---\")\n    \n    # Load primary training data\n    train_toxic_df = pd.read_csv(Config.TRAIN_TOXIC_PATH, usecols=['comment_text', 'toxic'])\n    \n    # Load secondary training data (unintended bias)\n    # The 'toxic' column is a float score; we convert it to a binary label.\n    train_bias_df = pd.read_csv(Config.TRAIN_BIAS_PATH, usecols=['comment_text', 'toxic'])\n    train_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n    \n    # Combine the two training datasets for a larger, more robust training set\n    train_df = pd.concat([train_toxic_df, train_bias_df], ignore_index=True)\n    # Remove duplicates that might arise from concatenating datasets\n    train_df.drop_duplicates(subset=['comment_text'], keep='first', inplace=True)\n    print(f\"Combined training data shape: {train_df.shape}\")\n\n    # Load multilingual validation data\n    valid_df = pd.read_csv(Config.VALIDATION_PATH)\n    print(f\"Validation data shape: {valid_df.shape}\")\n\n    # Load test data\n    test_df = pd.read_csv(Config.TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Extract text and labels for model input\n    X_train = train_df['comment_text'].values\n    y_train = train_df['toxic'].values\n    \n    X_valid = valid_df['comment_text'].values\n    y_valid = valid_df['toxic'].values\n    \n    X_test = test_df['content'].values\n    \n    return X_train, y_train, X_valid, y_valid, X_test, test_df['id']\n\n# --- 4. Dataset Pipeline ---\n# This function creates a tf.data.Dataset for efficient feeding to the model.\ndef build_dataset(texts, labels=None, batch_size=32, is_training=True):\n    \"\"\"\n    Creates a tf.data.Dataset from text and label arrays.\n    \n    Args:\n        texts (np.array): Array of text strings.\n        labels (np.array, optional): Array of labels. Defaults to None.\n        batch_size (int): The batch size for the dataset.\n        is_training (bool): If True, shuffles the dataset.\n\n    Returns:\n        tf.data.Dataset: A configured dataset object.\n    \"\"\"\n    # Use AUTOTUNE to automatically tune prefetch buffer sizes\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(texts)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n    \n    if is_training:\n        dataset = dataset.shuffle(buffer_size=1024, seed=Config.SEED).repeat()\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE) # Prefetch data for faster consumption\n    return dataset\n\n# --- 5. Model Building ---\ndef build_model(strategy):\n    \"\"\"\n    Builds and compiles the Keras NLP classifier model within the TPU strategy scope.\n    \"\"\"\n    print(\"\\n--- Building Model ---\")\n    with strategy.scope():\n        # KerasNLP's Classifier handles tokenization and model architecture in one step.\n        # It's a high-level API that simplifies building transformer models.\n        classifier = keras_nlp.models.Classifier.from_preset(\n            Config.PRESET,\n            num_classes=1, # Binary classification (toxic/not-toxic)\n            preprocessor=keras_nlp.models.XlmRobertaPreprocessor.from_preset(\n                Config.PRESET,\n                sequence_length=Config.SEQUENCE_LENGTH\n            )\n        )\n        \n        # Define optimizer with a learning rate schedule for better convergence\n        optimizer = keras.optimizers.AdamW(learning_rate=Config.LEARNING_RATE)\n        \n        # Compile the model\n        # We use from_logits=True because the model outputs raw logits, which is\n        # numerically more stable than outputting probabilities.\n        classifier.compile(\n            optimizer=optimizer,\n            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[\n                keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n                keras.metrics.AUC(name=\"roc_auc\", from_logits=True)\n            ],\n            jit_compile=True # Enable XLA compilation for another speed boost\n        )\n        \n    classifier.summary()\n    return classifier\n\n# --- 6. Training, Prediction, and Evaluation ---\ndef run_pipeline():\n    \"\"\"\n    Executes the full ML pipeline: setup, data loading, training, and prediction.\n    \"\"\"\n    # Initialize TPU\n    strategy, global_batch_size = initialize_tpu()\n    \n    # Load and prepare data\n    X_train, y_train, X_valid, y_valid, X_test, test_ids = load_and_prepare_data()\n    \n    # Create tf.data.Dataset objects\n    train_dataset = build_dataset(X_train, y_train, batch_size=global_batch_size, is_training=True)\n    valid_dataset = build_dataset(X_valid, y_valid, batch_size=global_batch_size, is_training=False)\n    test_dataset = build_dataset(X_test, batch_size=global_batch_size, is_training=False)\n    \n    # Build the model within the strategy scope\n    model = build_model(strategy)\n    \n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    # NOTE on evaluation metrics: The user requested a split of the training data for evaluation.\n    # However, for this specific cross-lingual problem, it is far more informative to evaluate\n    # on the provided multilingual `validation.csv` set. This directly tests the model's ability\n    # to generalize across languages, which is the core objective. The metrics below will be\n    # calculated on this validation set at the end of each epoch.\n    \n    # We calculate steps_per_epoch because the training dataset is set to repeat()\n    steps_per_epoch = len(X_train) // global_batch_size\n\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=Config.EPOCHS,\n        steps_per_epoch=steps_per_epoch\n    )\n    \n    print(\"\\n--- Training Complete ---\")\n    \n    # --- Prediction and Evaluation on Validation Set ---\n    print(\"\\n--- Evaluating on Validation Set ---\")\n    valid_preds_logits = model.predict(valid_dataset, verbose=1)\n    # Convert logits to probabilities using the sigmoid function\n    valid_preds_probs = tf.nn.sigmoid(valid_preds_logits).numpy().flatten()\n    \n    # Calculate requested evaluation metrics\n    accuracy = accuracy_score(y_valid, valid_preds_probs > 0.5)\n    loss = log_loss(y_valid, valid_preds_probs)\n    roc_auc = roc_auc_score(y_valid, valid_preds_probs)\n    f1_macro = f1_score(y_valid, valid_preds_probs > 0.5, average='macro')\n    f1_weighted = f1_score(y_valid, valid_preds_probs > 0.5, average='weighted')\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation Log Loss: {loss:.4f}\")\n    print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n    print(f\"Validation F1 Macro: {f1_macro:.4f}\")\n    print(f\"Validation F1 Weighted: {f1_weighted:.4f}\")\n    \n    # --- Prediction for Submission ---\n    print(\"\\n--- Generating Test Predictions ---\")\n    test_preds_logits = model.predict(test_dataset, verbose=1)\n    test_preds_probs = tf.nn.sigmoid(test_preds_logits).numpy().flatten()\n    \n    # Create submission file\n    submission_df = pd.DataFrame({'id': test_ids, 'toxic': test_preds_probs})\n    submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n    print(f\"\\nSubmission file created at: {Config.SUBMISSION_PATH}\")\n    print(submission_df.head())\n    \n    # Clean up memory\n    del model, X_train, y_train, X_valid, y_valid, X_test, test_ids\n    gc.collect()\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:10:42.133123Z","iopub.execute_input":"2025-07-02T14:10:42.133516Z","iopub.status.idle":"2025-07-02T14:11:11.268184Z","shell.execute_reply.started":"2025-07-02T14:10:42.133485Z","shell.execute_reply":"2025-07-02T14:11:11.264324Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1751465449.261477      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"--- Initializing TPU ---\nTPU not detected. Falling back to CPU/GPU strategy.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751465470.537716      10 service.cc:148] XLA service 0x5b765b86b0c0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751465470.537764      10 service.cc:156]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1751465470.537768      10 service.cc:156]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1751465470.537771      10 service.cc:156]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1751465470.537774      10 service.cc:156]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1751465470.537776      10 service.cc:156]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1751465470.537778      10 service.cc:156]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1751465470.537781      10 service.cc:156]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1751465470.537784      10 service.cc:156]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"Global batch size set to: 16\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 263\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 194\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mExecutes the full ML pipeline: setup, data loading, training, and prediction.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Initialize TPU\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m strategy, global_batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_tpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[1;32m    197\u001b[0m X_train, y_train, X_valid, y_valid, X_test, test_ids \u001b[38;5;241m=\u001b[39m load_and_prepare_data()\n","Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36minitialize_tpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal batch size set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Enable mixed precision for performance boost on TPUs.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmixed_precision\u001b[38;5;241m.\u001b[39mset_global_policy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_bfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m strategy, global_batch_size\n","\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"],"ename":"NameError","evalue":"name 'keras' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nKaggle Solution for Jigsaw Multilingual Toxic Comment Classification.\n\nThis program implements an end-to-end pipeline for a cross-lingual text classification task.\nIt uses a pre-trained multilingual transformer model (XLM-Roberta) to classify online\ncomments as toxic or non-toxic. The model is fine-tuned on English-only data and\nevaluated on non-English data to test its generalization capabilities.\n\nThe solution is optimized to run on a Google Cloud TPU (v3-8), leveraging TensorFlow's\ndistribution strategies for efficient, large-scale training.\n\"\"\"\n\nimport os\nimport sys\nimport gc\n\n# Suppress verbose logging and warnings for a cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport keras_nlp\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Configuration ---\n# All hyperparameters and settings are centralized here for easy tuning.\nclass Config:\n    \"\"\"\n    Configuration class for model and training hyperparameters.\n    \"\"\"\n    # File Paths\n    BASE_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_TOXIC_PATH = os.path.join(BASE_PATH, \"jigsaw-toxic-comment-train.csv\")\n    TRAIN_BIAS_PATH = os.path.join(BASE_PATH, \"jigsaw-unintended-bias-train.csv\")\n    VALIDATION_PATH = os.path.join(BASE_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test.csv\")\n    SUBMISSION_PATH = \"submission.csv\"\n\n    # Model Configuration\n    # XLM-Roberta is a strong choice for multilingual tasks.\n    PRESET = \"xlm_roberta_base_multi\"\n    SEQUENCE_LENGTH = 192  # Max length of text sequences. Balances context and memory.\n\n    # Training Configuration\n    EPOCHS = 2  # Fine-tuning transformers requires only a few epochs.\n    BATCH_SIZE_PER_REPLICA = 16 # Batch size for each TPU core.\n    LEARNING_RATE = 2e-5  # A standard learning rate for fine-tuning transformers.\n    \n    # Set a random seed for reproducibility\n    SEED = 42\n\n# --- 2. TPU Initialization ---\n# This section detects and initializes the TPU for distributed training.\ndef initialize_tpu():\n    \"\"\"\n    Detects and initializes the TPU strategy.\n    Returns the distribution strategy and the global batch size.\n    \"\"\"\n    print(\"--- Initializing TPU ---\")\n    try:\n        tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu_resolver)\n        print(f\"TPU detected. Found {strategy.num_replicas_in_sync} replicas.\")\n    except ValueError:\n        print(\"TPU not detected. Falling back to CPU/GPU strategy.\")\n        strategy = tf.distribute.MirroredStrategy()\n\n    # Calculate the global batch size based on the number of replicas.\n    global_batch_size = Config.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    print(f\"Global batch size set to: {global_batch_size}\")\n    \n    # Enable mixed precision for performance boost on TPUs.\n    keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n    \n    return strategy, global_batch_size\n\n# --- 3. Data Loading and Preparation ---\ndef load_and_prepare_data():\n    \"\"\"\n    Loads, preprocesses, and combines the training, validation, and test datasets.\n    \"\"\"\n    print(\"\\n--- Loading and Preparing Data ---\")\n    \n    # Load primary training data\n    train_toxic_df = pd.read_csv(Config.TRAIN_TOXIC_PATH, usecols=['comment_text', 'toxic'])\n    \n    # Load secondary training data (unintended bias)\n    # The 'toxic' column is a float score; we convert it to a binary label.\n    train_bias_df = pd.read_csv(Config.TRAIN_BIAS_PATH, usecols=['comment_text', 'toxic'])\n    train_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n    \n    # Combine the two training datasets for a larger, more robust training set\n    train_df = pd.concat([train_toxic_df, train_bias_df], ignore_index=True)\n    # Remove duplicates that might arise from concatenating datasets\n    train_df.drop_duplicates(subset=['comment_text'], keep='first', inplace=True)\n    print(f\"Combined training data shape: {train_df.shape}\")\n\n    # Load multilingual validation data\n    valid_df = pd.read_csv(Config.VALIDATION_PATH)\n    print(f\"Validation data shape: {valid_df.shape}\")\n\n    # Load test data\n    test_df = pd.read_csv(Config.TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Extract text and labels for model input\n    X_train = train_df['comment_text'].values\n    y_train = train_df['toxic'].values\n    \n    X_valid = valid_df['comment_text'].values\n    y_valid = valid_df['toxic'].values\n    \n    X_test = test_df['content'].values\n    \n    return X_train, y_train, X_valid, y_valid, X_test, test_df['id']\n\n# --- 4. Dataset Pipeline ---\n# This function creates a tf.data.Dataset for efficient feeding to the model.\ndef build_dataset(texts, labels=None, batch_size=32, is_training=True):\n    \"\"\"\n    Creates a tf.data.Dataset from text and label arrays.\n    \n    Args:\n        texts (np.array): Array of text strings.\n        labels (np.array, optional): Array of labels. Defaults to None.\n        batch_size (int): The batch size for the dataset.\n        is_training (bool): If True, shuffles the dataset.\n\n    Returns:\n        tf.data.Dataset: A configured dataset object.\n    \"\"\"\n    # Use AUTOTUNE to automatically tune prefetch buffer sizes\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(texts)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n    \n    if is_training:\n        dataset = dataset.shuffle(buffer_size=1024, seed=Config.SEED).repeat()\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE) # Prefetch data for faster consumption\n    return dataset\n\n# --- 5. Model Building ---\ndef build_model(strategy):\n    \"\"\"\n    Builds and compiles the Keras NLP classifier model within the TPU strategy scope.\n    \"\"\"\n    print(\"\\n--- Building Model ---\")\n    with strategy.scope():\n        # KerasNLP's Classifier handles tokenization and model architecture in one step.\n        # It's a high-level API that simplifies building transformer models.\n        classifier = keras_nlp.models.Classifier.from_preset(\n            Config.PRESET,\n            num_classes=1, # Binary classification (toxic/not-toxic)\n            preprocessor=keras_nlp.models.XlmRobertaPreprocessor.from_preset(\n                Config.PRESET,\n                sequence_length=Config.SEQUENCE_LENGTH\n            )\n        )\n        \n        # Define optimizer with a learning rate schedule for better convergence\n        optimizer = keras.optimizers.AdamW(learning_rate=Config.LEARNING_RATE)\n        \n        # Compile the model\n        # We use from_logits=True because the model outputs raw logits, which is\n        # numerically more stable than outputting probabilities.\n        classifier.compile(\n            optimizer=optimizer,\n            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[\n                keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n                keras.metrics.AUC(name=\"roc_auc\", from_logits=True)\n            ],\n            jit_compile=True # Enable XLA compilation for another speed boost\n        )\n        \n    classifier.summary()\n    return classifier\n\n# --- 6. Training, Prediction, and Evaluation ---\ndef run_pipeline():\n    \"\"\"\n    Executes the full ML pipeline: setup, data loading, training, and prediction.\n    \"\"\"\n    # Initialize TPU\n    strategy, global_batch_size = initialize_tpu()\n    \n    # Load and prepare data\n    X_train, y_train, X_valid, y_valid, X_test, test_ids = load_and_prepare_data()\n    \n    # Create tf.data.Dataset objects\n    train_dataset = build_dataset(X_train, y_train, batch_size=global_batch_size, is_training=True)\n    valid_dataset = build_dataset(X_valid, y_valid, batch_size=global_batch_size, is_training=False)\n    test_dataset = build_dataset(X_test, batch_size=global_batch_size, is_training=False)\n    \n    # Build the model within the strategy scope\n    model = build_model(strategy)\n    \n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    # NOTE on evaluation metrics: The user requested a split of the training data for evaluation.\n    # However, for this specific cross-lingual problem, it is far more informative to evaluate\n    # on the provided multilingual `validation.csv` set. This directly tests the model's ability\n    # to generalize across languages, which is the core objective. The metrics below will be\n    # calculated on this validation set after training is complete.\n    \n    # We calculate steps_per_epoch because the training dataset is set to repeat()\n    steps_per_epoch = len(X_train) // global_batch_size\n\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=Config.EPOCHS,\n        steps_per_epoch=steps_per_epoch\n    )\n    \n    print(\"\\n--- Training Complete ---\")\n    \n    # --- Prediction and Evaluation on Validation Set ---\n    print(\"\\n--- Evaluating on Validation Set ---\")\n    valid_preds_logits = model.predict(valid_dataset, verbose=1)\n    # Convert logits to probabilities using the sigmoid function\n    valid_preds_probs = tf.nn.sigmoid(valid_preds_logits).numpy().flatten()\n    \n    # Calculate requested evaluation metrics\n    # For AUC, the 'ovr' setting is for multi-class; for binary, it's not needed.\n    # We'll calculate both macro and weighted F1 for completeness.\n    y_pred_binary = valid_preds_probs > 0.5\n    accuracy = accuracy_score(y_valid, y_pred_binary)\n    loss = log_loss(y_valid, valid_preds_probs)\n    roc_auc = roc_auc_score(y_valid, valid_preds_probs)\n    f1_macro = f1_score(y_valid, y_pred_binary, average='macro')\n    f1_weighted = f1_score(y_valid, y_pred_binary, average='weighted')\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation Log Loss: {loss:.4f}\")\n    print(f\"Validation ROC AUC (AUC): {roc_auc:.4f}\")\n    print(f\"Validation F1 Macro: {f1_macro:.4f}\")\n    print(f\"Validation F1 Weighted: {f1_weighted:.4f}\")\n    \n    # --- Prediction for Submission ---\n    print(\"\\n--- Generating Test Predictions ---\")\n    test_preds_logits = model.predict(test_dataset, verbose=1)\n    test_preds_probs = tf.nn.sigmoid(test_preds_logits).numpy().flatten()\n    \n    # Create submission file\n    submission_df = pd.DataFrame({'id': test_ids, 'toxic': test_preds_probs})\n    submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n    print(f\"\\nSubmission file created at: {Config.SUBMISSION_PATH}\")\n    print(submission_df.head())\n    \n    # Clean up memory\n    del model, X_train, y_train, X_valid, y_valid, X_test, test_ids\n    gc.collect()\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:13:22.623903Z","iopub.execute_input":"2025-07-02T14:13:22.624347Z","iopub.status.idle":"2025-07-02T14:13:46.246748Z","shell.execute_reply.started":"2025-07-02T14:13:22.624298Z","shell.execute_reply":"2025-07-02T14:13:46.241314Z"}},"outputs":[{"name":"stdout","text":"--- Initializing TPU ---\nTPU not detected. Falling back to CPU/GPU strategy.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nGlobal batch size set to: 16\n\n--- Loading and Preparing Data ---\nCombined training data shape: (2099954, 2)\nValidation data shape: (8000, 4)\nTest data shape: (63812, 3)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 267\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[3], line 201\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m X_train, y_train, X_valid, y_valid, X_test, test_ids \u001b[38;5;241m=\u001b[39m load_and_prepare_data()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Create tf.data.Dataset objects\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m build_dataset(X_valid, y_valid, batch_size\u001b[38;5;241m=\u001b[39mglobal_batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    203\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m build_dataset(X_test, batch_size\u001b[38;5;241m=\u001b[39mglobal_batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","Cell \u001b[0;32mIn[3], line 143\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(texts, labels, batch_size, is_training)\u001b[0m\n\u001b[1;32m    141\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(texts)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    146\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mSEED)\u001b[38;5;241m.\u001b[39mrepeat()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:827\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:732\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    731\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."],"ename":"ValueError","evalue":"Failed to convert a NumPy array to a Tensor (Unsupported object type float).","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nKaggle Solution for Jigsaw Multilingual Toxic Comment Classification.\n\nThis program implements an end-to-end pipeline for a cross-lingual text classification task.\nIt uses a pre-trained multilingual transformer model (XLM-Roberta) to classify online\ncomments as toxic or non-toxic. The model is fine-tuned on English-only data and\nevaluated on non-English data to test its generalization capabilities.\n\nThe solution is optimized to run on a Google Cloud TPU (v3-8), leveraging TensorFlow's\ndistribution strategies for efficient, large-scale training.\n\"\"\"\n\nimport os\nimport sys\nimport gc\n\n# Suppress verbose logging and warnings for a cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport keras_nlp\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Configuration ---\n# All hyperparameters and settings are centralized here for easy tuning.\nclass Config:\n    \"\"\"\n    Configuration class for model and training hyperparameters.\n    \"\"\"\n    # File Paths\n    BASE_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_TOXIC_PATH = os.path.join(BASE_PATH, \"jigsaw-toxic-comment-train.csv\")\n    TRAIN_BIAS_PATH = os.path.join(BASE_PATH, \"jigsaw-unintended-bias-train.csv\")\n    VALIDATION_PATH = os.path.join(BASE_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test.csv\")\n    SUBMISSION_PATH = \"submission.csv\"\n\n    # Model Configuration\n    # XLM-Roberta is a strong choice for multilingual tasks.\n    PRESET = \"xlm_roberta_base_multi\"\n    SEQUENCE_LENGTH = 192  # Max length of text sequences. Balances context and memory.\n\n    # Training Configuration\n    EPOCHS = 2  # Fine-tuning transformers requires only a few epochs.\n    BATCH_SIZE_PER_REPLICA = 16 # Batch size for each TPU core.\n    LEARNING_RATE = 2e-5  # A standard learning rate for fine-tuning transformers.\n    \n    # Set a random seed for reproducibility\n    SEED = 42\n\n# --- 2. TPU Initialization ---\n# This section detects and initializes the TPU for distributed training.\ndef initialize_tpu():\n    \"\"\"\n    Detects and initializes the TPU strategy.\n    Returns the distribution strategy and the global batch size.\n    \"\"\"\n    print(\"--- Initializing TPU ---\")\n    try:\n        tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu_resolver)\n        print(f\"TPU detected. Found {strategy.num_replicas_in_sync} replicas.\")\n    except ValueError:\n        print(\"TPU not detected. Falling back to CPU/GPU strategy.\")\n        strategy = tf.distribute.MirroredStrategy()\n\n    # Calculate the global batch size based on the number of replicas.\n    global_batch_size = Config.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    print(f\"Global batch size set to: {global_batch_size}\")\n    \n    # Enable mixed precision for performance boost on TPUs.\n    keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n    \n    return strategy, global_batch_size\n\n# --- 3. Data Loading and Preparation ---\ndef load_and_prepare_data():\n    \"\"\"\n    Loads, preprocesses, and combines the training, validation, and test datasets.\n    \"\"\"\n    print(\"\\n--- Loading and Preparing Data ---\")\n    \n    # Load primary training data\n    train_toxic_df = pd.read_csv(Config.TRAIN_TOXIC_PATH, usecols=['comment_text', 'toxic'])\n    \n    # Load secondary training data (unintended bias)\n    train_bias_df = pd.read_csv(Config.TRAIN_BIAS_PATH, usecols=['comment_text', 'toxic'])\n    train_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n    \n    # Combine the two training datasets\n    train_df = pd.concat([train_toxic_df, train_bias_df], ignore_index=True)\n    train_df.drop_duplicates(subset=['comment_text'], keep='first', inplace=True)\n    print(f\"Combined training data shape: {train_df.shape}\")\n\n    # Load multilingual validation data\n    valid_df = pd.read_csv(Config.VALIDATION_PATH)\n    print(f\"Validation data shape: {valid_df.shape}\")\n\n    # Load test data\n    test_df = pd.read_csv(Config.TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # --- FIX: Handle potential NaN values in text data ---\n    # NaN values in a text column cause the numpy array to have dtype=object,\n    # which TensorFlow cannot convert directly. We fill them with an empty string.\n    print(\"Cleaning missing values from text columns...\")\n    train_df['comment_text'].fillna('', inplace=True)\n    valid_df['comment_text'].fillna('', inplace=True)\n    test_df['content'].fillna('', inplace=True)\n\n    # To be robust, also ensure label columns have no NaNs and are integers.\n    train_df['toxic'].fillna(0, inplace=True)\n    valid_df['toxic'].fillna(0, inplace=True)\n    \n    # Extract text and labels for model input, explicitly casting types\n    X_train = train_df['comment_text'].astype(str).values\n    y_train = train_df['toxic'].astype(int).values\n    \n    X_valid = valid_df['comment_text'].astype(str).values\n    y_valid = valid_df['toxic'].astype(int).values\n    \n    X_test = test_df['content'].astype(str).values\n    \n    return X_train, y_train, X_valid, y_valid, X_test, test_df['id']\n\n# --- 4. Dataset Pipeline ---\n# This function creates a tf.data.Dataset for efficient feeding to the model.\ndef build_dataset(texts, labels=None, batch_size=32, is_training=True):\n    \"\"\"\n    Creates a tf.data.Dataset from text and label arrays.\n    \n    Args:\n        texts (np.array): Array of text strings.\n        labels (np.array, optional): Array of labels. Defaults to None.\n        batch_size (int): The batch size for the dataset.\n        is_training (bool): If True, shuffles the dataset.\n\n    Returns:\n        tf.data.Dataset: A configured dataset object.\n    \"\"\"\n    # Use AUTOTUNE to automatically tune prefetch buffer sizes\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(texts)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n    \n    if is_training:\n        dataset = dataset.shuffle(buffer_size=1024, seed=Config.SEED).repeat()\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE) # Prefetch data for faster consumption\n    return dataset\n\n# --- 5. Model Building ---\ndef build_model(strategy):\n    \"\"\"\n    Builds and compiles the Keras NLP classifier model within the TPU strategy scope.\n    \"\"\"\n    print(\"\\n--- Building Model ---\")\n    with strategy.scope():\n        # KerasNLP's Classifier handles tokenization and model architecture in one step.\n        classifier = keras_nlp.models.Classifier.from_preset(\n            Config.PRESET,\n            num_classes=1, # Binary classification (toxic/not-toxic)\n            preprocessor=keras_nlp.models.XlmRobertaPreprocessor.from_preset(\n                Config.PRESET,\n                sequence_length=Config.SEQUENCE_LENGTH\n            )\n        )\n        \n        # Define optimizer with a learning rate schedule for better convergence\n        optimizer = keras.optimizers.AdamW(learning_rate=Config.LEARNING_RATE)\n        \n        # Compile the model\n        classifier.compile(\n            optimizer=optimizer,\n            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[\n                keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n                keras.metrics.AUC(name=\"roc_auc\", from_logits=True)\n            ],\n            jit_compile=True # Enable XLA compilation for another speed boost\n        )\n        \n    classifier.summary()\n    return classifier\n\n# --- 6. Training, Prediction, and Evaluation ---\ndef run_pipeline():\n    \"\"\"\n    Executes the full ML pipeline: setup, data loading, training, and prediction.\n    \"\"\"\n    # Initialize TPU\n    strategy, global_batch_size = initialize_tpu()\n    \n    # Load and prepare data\n    X_train, y_train, X_valid, y_valid, X_test, test_ids = load_and_prepare_data()\n    \n    # Create tf.data.Dataset objects\n    train_dataset = build_dataset(X_train, y_train, batch_size=global_batch_size, is_training=True)\n    valid_dataset = build_dataset(X_valid, y_valid, batch_size=global_batch_size, is_training=False)\n    test_dataset = build_dataset(X_test, batch_size=global_batch_size, is_training=False)\n    \n    # Build the model within the strategy scope\n    model = build_model(strategy)\n    \n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    steps_per_epoch = len(X_train) // global_batch_size\n\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=Config.EPOCHS,\n        steps_per_epoch=steps_per_epoch\n    )\n    \n    print(\"\\n--- Training Complete ---\")\n    \n    # --- Prediction and Evaluation on Validation Set ---\n    print(\"\\n--- Evaluating on Validation Set ---\")\n    valid_preds_logits = model.predict(valid_dataset, verbose=1)\n    # Convert logits to probabilities using the sigmoid function\n    valid_preds_probs = tf.nn.sigmoid(valid_preds_logits).numpy().flatten()\n    \n    # Calculate requested evaluation metrics\n    y_pred_binary = valid_preds_probs > 0.5\n    accuracy = accuracy_score(y_valid, y_pred_binary)\n    loss = log_loss(y_valid, valid_preds_probs)\n    roc_auc = roc_auc_score(y_valid, valid_preds_probs)\n    f1_macro = f1_score(y_valid, y_pred_binary, average='macro')\n    f1_weighted = f1_score(y_valid, y_pred_binary, average='weighted')\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation Log Loss: {loss:.4f}\")\n    print(f\"Validation ROC AUC (AUC): {roc_auc:.4f}\")\n    print(f\"Validation F1 Macro: {f1_macro:.4f}\")\n    print(f\"Validation F1 Weighted: {f1_weighted:.4f}\")\n    \n    # --- Prediction for Submission ---\n    print(\"\\n--- Generating Test Predictions ---\")\n    test_preds_logits = model.predict(test_dataset, verbose=1)\n    test_preds_probs = tf.nn.sigmoid(test_preds_logits).numpy().flatten()\n    \n    # Create submission file\n    submission_df = pd.DataFrame({'id': test_ids, 'toxic': test_preds_probs})\n    submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n    print(f\"\\nSubmission file created at: {Config.SUBMISSION_PATH}\")\n    print(submission_df.head())\n    \n    # Clean up memory\n    del model, X_train, y_train, X_valid, y_valid, X_test, test_ids\n    gc.collect()\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:16:40.651581Z","iopub.execute_input":"2025-07-02T14:16:40.651986Z","iopub.status.idle":"2025-07-02T14:16:55.476017Z","shell.execute_reply.started":"2025-07-02T14:16:40.651958Z","shell.execute_reply":"2025-07-02T14:16:55.470750Z"}},"outputs":[{"name":"stdout","text":"--- Initializing TPU ---\nTPU not detected. Falling back to CPU/GPU strategy.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nGlobal batch size set to: 16\n\n--- Loading and Preparing Data ---\nCombined training data shape: (2099954, 2)\nValidation data shape: (8000, 4)\nTest data shape: (63812, 3)\nCleaning missing values from text columns...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/2115946294.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/2115946294.py:114: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/2115946294.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df['content'].fillna('', inplace=True)\n/tmp/ipykernel_10/2115946294.py:118: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['toxic'].fillna(0, inplace=True)\n/tmp/ipykernel_10/2115946294.py:119: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['toxic'].fillna(0, inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Building Model ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 265\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 265\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 213\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m build_dataset(X_test, batch_size\u001b[38;5;241m=\u001b[39mglobal_batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Build the model within the strategy scope\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Model Training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 173\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(strategy)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Building Model ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# KerasNLP's Classifier handles tokenization and model architecture in one step.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(\n\u001b[1;32m    171\u001b[0m         Config\u001b[38;5;241m.\u001b[39mPRESET,\n\u001b[1;32m    172\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# Binary classification (toxic/not-toxic)\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m         preprocessor\u001b[38;5;241m=\u001b[39m\u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXlmRobertaPreprocessor\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_preset(\n\u001b[1;32m    174\u001b[0m             Config\u001b[38;5;241m.\u001b[39mPRESET,\n\u001b[1;32m    175\u001b[0m             sequence_length\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mSEQUENCE_LENGTH\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Define optimizer with a learning rate schedule for better convergence\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(learning_rate\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mLEARNING_RATE)\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras_hub.api.models' has no attribute 'XlmRobertaPreprocessor'"],"ename":"AttributeError","evalue":"module 'keras_hub.api.models' has no attribute 'XlmRobertaPreprocessor'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nKaggle Solution for Jigsaw Multilingual Toxic Comment Classification.\n\nThis program implements an end-to-end pipeline for a cross-lingual text classification task.\nIt uses a pre-trained multilingual transformer model (XLM-Roberta) to classify online\ncomments as toxic or non-toxic. The model is fine-tuned on English-only data and\nevaluated on non-English data to test its generalization capabilities.\n\nThe solution is optimized to run on a Google Cloud TPU (v3-8), leveraging TensorFlow's\ndistribution strategies for efficient, large-scale training.\n\"\"\"\n\nimport os\nimport sys\nimport gc\n\n# Suppress verbose logging and warnings for a cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport keras_nlp\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Configuration ---\n# All hyperparameters and settings are centralized here for easy tuning.\nclass Config:\n    \"\"\"\n    Configuration class for model and training hyperparameters.\n    \"\"\"\n    # File Paths\n    BASE_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_TOXIC_PATH = os.path.join(BASE_PATH, \"jigsaw-toxic-comment-train.csv\")\n    TRAIN_BIAS_PATH = os.path.join(BASE_PATH, \"jigsaw-unintended-bias-train.csv\")\n    VALIDATION_PATH = os.path.join(BASE_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test.csv\")\n    SUBMISSION_PATH = \"submission.csv\"\n\n    # Model Configuration\n    # XLM-Roberta is a strong choice for multilingual tasks.\n    PRESET = \"xlm_roberta_base_multi\"\n    SEQUENCE_LENGTH = 192  # Max length of text sequences. Balances context and memory.\n\n    # Training Configuration\n    EPOCHS = 2  # Fine-tuning transformers requires only a few epochs.\n    BATCH_SIZE_PER_REPLICA = 16 # Batch size for each TPU core.\n    LEARNING_RATE = 2e-5  # A standard learning rate for fine-tuning transformers.\n    \n    # Set a random seed for reproducibility\n    SEED = 42\n\n# --- 2. TPU Initialization ---\n# This section detects and initializes the TPU for distributed training.\ndef initialize_tpu():\n    \"\"\"\n    Detects and initializes the TPU strategy.\n    Returns the distribution strategy and the global batch size.\n    \"\"\"\n    print(\"--- Initializing TPU ---\")\n    try:\n        tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu_resolver)\n        print(f\"TPU detected. Found {strategy.num_replicas_in_sync} replicas.\")\n    except ValueError:\n        print(\"TPU not detected. Falling back to CPU/GPU strategy.\")\n        strategy = tf.distribute.MirroredStrategy()\n\n    # Calculate the global batch size based on the number of replicas.\n    global_batch_size = Config.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    print(f\"Global batch size set to: {global_batch_size}\")\n    \n    # Enable mixed precision for performance boost on TPUs.\n    keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n    \n    return strategy, global_batch_size\n\n# --- 3. Data Loading and Preparation ---\ndef load_and_prepare_data():\n    \"\"\"\n    Loads, preprocesses, and combines the training, validation, and test datasets.\n    \"\"\"\n    print(\"\\n--- Loading and Preparing Data ---\")\n    \n    # Load primary training data\n    train_toxic_df = pd.read_csv(Config.TRAIN_TOXIC_PATH, usecols=['comment_text', 'toxic'])\n    \n    # Load secondary training data (unintended bias)\n    train_bias_df = pd.read_csv(Config.TRAIN_BIAS_PATH, usecols=['comment_text', 'toxic'])\n    train_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n    \n    # Combine the two training datasets\n    train_df = pd.concat([train_toxic_df, train_bias_df], ignore_index=True)\n    train_df.drop_duplicates(subset=['comment_text'], keep='first', inplace=True)\n    print(f\"Combined training data shape: {train_df.shape}\")\n\n    # Load multilingual validation data\n    valid_df = pd.read_csv(Config.VALIDATION_PATH)\n    print(f\"Validation data shape: {valid_df.shape}\")\n\n    # Load test data\n    test_df = pd.read_csv(Config.TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Handle potential NaN values in text data\n    print(\"Cleaning missing values from text columns...\")\n    train_df['comment_text'].fillna('', inplace=True)\n    valid_df['comment_text'].fillna('', inplace=True)\n    test_df['content'].fillna('', inplace=True)\n\n    # To be robust, also ensure label columns have no NaNs and are integers.\n    train_df['toxic'].fillna(0, inplace=True)\n    valid_df['toxic'].fillna(0, inplace=True)\n    \n    # Extract text and labels for model input, explicitly casting types\n    X_train = train_df['comment_text'].astype(str).values\n    y_train = train_df['toxic'].astype(int).values\n    \n    X_valid = valid_df['comment_text'].astype(str).values\n    y_valid = valid_df['toxic'].astype(int).values\n    \n    X_test = test_df['content'].astype(str).values\n    \n    return X_train, y_train, X_valid, y_valid, X_test, test_df['id']\n\n# --- 4. Dataset Pipeline ---\ndef build_dataset(texts, labels=None, batch_size=32, is_training=True):\n    \"\"\"\n    Creates a tf.data.Dataset from text and label arrays.\n    \"\"\"\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(texts)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n    \n    if is_training:\n        dataset = dataset.shuffle(buffer_size=1024, seed=Config.SEED).repeat()\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n# --- 5. Model Building ---\ndef build_model(strategy):\n    \"\"\"\n    Builds and compiles the Keras NLP classifier model within the TPU strategy scope.\n    \"\"\"\n    print(\"\\n--- Building Model ---\")\n    with strategy.scope():\n        # --- FIX APPLIED HERE ---\n        # The Classifier.from_preset method is smart. We can pass preprocessor\n        # arguments like `sequence_length` directly to it. This avoids the\n        # need to manually instantiate the preprocessor, fixing the error.\n        classifier = keras_nlp.models.Classifier.from_preset(\n            Config.PRESET,\n            num_classes=1, # Binary classification (toxic/not-toxic)\n            sequence_length=Config.SEQUENCE_LENGTH # This is passed to the preprocessor\n        )\n        \n        # Define optimizer\n        optimizer = keras.optimizers.AdamW(learning_rate=Config.LEARNING_RATE)\n        \n        # Compile the model\n        classifier.compile(\n            optimizer=optimizer,\n            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[\n                keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n                keras.metrics.AUC(name=\"roc_auc\", from_logits=True)\n            ],\n            jit_compile=True\n        )\n        \n    classifier.summary()\n    return classifier\n\n# --- 6. Training, Prediction, and Evaluation ---\ndef run_pipeline():\n    \"\"\"\n    Executes the full ML pipeline: setup, data loading, training, and prediction.\n    \"\"\"\n    # Initialize TPU\n    strategy, global_batch_size = initialize_tpu()\n    \n    # Load and prepare data\n    X_train, y_train, X_valid, y_valid, X_test, test_ids = load_and_prepare_data()\n    \n    # Create tf.data.Dataset objects\n    train_dataset = build_dataset(X_train, y_train, batch_size=global_batch_size, is_training=True)\n    valid_dataset = build_dataset(X_valid, y_valid, batch_size=global_batch_size, is_training=False)\n    test_dataset = build_dataset(X_test, batch_size=global_batch_size, is_training=False)\n    \n    # Build the model within the strategy scope\n    model = build_model(strategy)\n    \n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    steps_per_epoch = len(X_train) // global_batch_size\n\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=Config.EPOCHS,\n        steps_per_epoch=steps_per_epoch\n    )\n    \n    print(\"\\n--- Training Complete ---\")\n    \n    # --- Prediction and Evaluation on Validation Set ---\n    print(\"\\n--- Evaluating on Validation Set ---\")\n    valid_preds_logits = model.predict(valid_dataset, verbose=1)\n    valid_preds_probs = tf.nn.sigmoid(valid_preds_logits).numpy().flatten()\n    \n    # Calculate requested evaluation metrics\n    y_pred_binary = valid_preds_probs > 0.5\n    accuracy = accuracy_score(y_valid, y_pred_binary)\n    loss = log_loss(y_valid, valid_preds_probs)\n    roc_auc = roc_auc_score(y_valid, valid_preds_probs)\n    f1_macro = f1_score(y_valid, y_pred_binary, average='macro')\n    f1_weighted = f1_score(y_valid, y_pred_binary, average='weighted')\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation Log Loss: {loss:.4f}\")\n    print(f\"Validation ROC AUC (AUC): {roc_auc:.4f}\")\n    print(f\"Validation F1 Macro: {f1_macro:.4f}\")\n    print(f\"Validation F1 Weighted: {f1_weighted:.4f}\")\n    \n    # --- Prediction for Submission ---\n    print(\"\\n--- Generating Test Predictions ---\")\n    test_preds_logits = model.predict(test_dataset, verbose=1)\n    test_preds_probs = tf.nn.sigmoid(test_preds_logits).numpy().flatten()\n    \n    # Create submission file\n    submission_df = pd.DataFrame({'id': test_ids, 'toxic': test_preds_probs})\n    submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n    print(f\"\\nSubmission file created at: {Config.SUBMISSION_PATH}\")\n    print(submission_df.head())\n    \n    # Clean up memory\n    del model, X_train, y_train, X_valid, y_valid, X_test, test_ids\n    gc.collect()\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:22:43.888491Z","iopub.execute_input":"2025-07-02T14:22:43.888922Z","iopub.status.idle":"2025-07-02T14:23:53.055709Z","shell.execute_reply.started":"2025-07-02T14:22:43.888894Z","shell.execute_reply":"2025-07-02T14:23:53.048170Z"}},"outputs":[{"name":"stdout","text":"--- Initializing TPU ---\nTPU not detected. Falling back to CPU/GPU strategy.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nGlobal batch size set to: 16\n\n--- Loading and Preparing Data ---\nCombined training data shape: (2099954, 2)\nValidation data shape: (8000, 4)\nTest data shape: (63812, 3)\nCleaning missing values from text columns...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/1005185967.py:111: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/1005185967.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/1005185967.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df['content'].fillna('', inplace=True)\n/tmp/ipykernel_10/1005185967.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['toxic'].fillna(0, inplace=True)\n/tmp/ipykernel_10/1005185967.py:117: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['toxic'].fillna(0, inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Building Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"xlm_roberta_text_classifier_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"xlm_roberta_text_classifier_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n\n xlm_roberta_tokenizer (\u001b[38;5;33mXLMRobertaTokenizer\u001b[0m)                                         Vocab size: \u001b[38;5;34m250,002\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n\n xlm_roberta_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">XLMRobertaTokenizer</span>)                                         Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">250,002</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"xlm_roberta_text_classifier\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"xlm_roberta_text_classifier\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n\n padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n xlm_roberta_backbone           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)              \u001b[38;5;34m277,450,752\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n (\u001b[38;5;33mXLMRobertaBackbone\u001b[0m)                                                       token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n get_item (\u001b[38;5;33mGetItem\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  xlm_roberta_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n pooled_dropout (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n\n pooled_dense (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                        \u001b[38;5;34m590,592\u001b[0m  pooled_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n output_dropout (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  pooled_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n logits (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                              \u001b[38;5;34m769\u001b[0m  output_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n\n padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n xlm_roberta_backbone           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">277,450,752</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">XLMRobertaBackbone</span>)                                                       token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  xlm_roberta_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n pooled_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n\n pooled_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span>  pooled_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n output_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  pooled_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">769</span>  output_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m278,042,113\u001b[0m (1.04 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,042,113</span> (1.04 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m278,042,113\u001b[0m (1.04 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,042,113</span> (1.04 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Model Training ---\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 251\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 206\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Model Training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m global_batch_size\n\u001b[0;32m--> 206\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# --- Prediction and Evaluation on Validation Set ---\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py:177\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    171\u001b[0m             validation_data\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    174\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 765, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_eventloop.py\", line 74, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2310, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 217, in run\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 435, in process_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 512, in process_shell_message\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 361, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 457, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 606, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_10/1005185967.py\", line 251, in <module>\n\n  File \"/tmp/ipykernel_10/1005185967.py\", line 206, in run_pipeline\n\n  File \"/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_27370]"],"ename":"NotFoundError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 765, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_eventloop.py\", line 74, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2310, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 217, in run\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 435, in process_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 512, in process_shell_message\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 361, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 457, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 606, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_10/1005185967.py\", line 251, in <module>\n\n  File \"/tmp/ipykernel_10/1005185967.py\", line 206, in run_pipeline\n\n  File \"/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_27370]","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nKaggle Solution for Jigsaw Multilingual Toxic Comment Classification.\n\nThis program implements an end-to-end pipeline for a cross-lingual text classification task.\nIt uses a pre-trained multilingual transformer model (XLM-Roberta) to classify online\ncomments as toxic or non-toxic. The model is fine-tuned on English-only data and\nevaluated on non-English data to test its generalization capabilities.\n\nThe solution is optimized to run on a Google Cloud TPU (v3-8), leveraging TensorFlow's\ndistribution strategies for efficient, large-scale training.\n\"\"\"\n\nimport os\nimport sys\nimport gc\n\n# Suppress verbose logging and warnings for a cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport keras_nlp\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Configuration ---\n# All hyperparameters and settings are centralized here for easy tuning.\nclass Config:\n    \"\"\"\n    Configuration class for model and training hyperparameters.\n    \"\"\"\n    # File Paths\n    BASE_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\n    TRAIN_TOXIC_PATH = os.path.join(BASE_PATH, \"jigsaw-toxic-comment-train.csv\")\n    TRAIN_BIAS_PATH = os.path.join(BASE_PATH, \"jigsaw-unintended-bias-train.csv\")\n    VALIDATION_PATH = os.path.join(BASE_PATH, \"validation.csv\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test.csv\")\n    SUBMISSION_PATH = \"submission.csv\"\n\n    # Model Configuration\n    # XLM-Roberta is a strong choice for multilingual tasks.\n    PRESET = \"xlm_roberta_base_multi\"\n    SEQUENCE_LENGTH = 192  # Max length of text sequences. Balances context and memory.\n\n    # Training Configuration\n    EPOCHS = 2  # Fine-tuning transformers requires only a few epochs.\n    BATCH_SIZE_PER_REPLICA = 16 # Batch size for each TPU core.\n    LEARNING_RATE = 2e-5  # A standard learning rate for fine-tuning transformers.\n    \n    # Set a random seed for reproducibility\n    SEED = 42\n\n# --- 2. TPU Initialization ---\n# This section detects and initializes the TPU for distributed training.\ndef initialize_tpu():\n    \"\"\"\n    Detects and initializes the TPU strategy.\n    Returns the distribution strategy and the global batch size.\n    \"\"\"\n    print(\"--- Initializing TPU ---\")\n    try:\n        tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu_resolver)\n        print(f\"TPU detected. Found {strategy.num_replicas_in_sync} replicas.\")\n    except ValueError:\n        print(\"TPU not detected. Falling back to CPU/GPU strategy.\")\n        strategy = tf.distribute.MirroredStrategy()\n\n    # Calculate the global batch size based on the number of replicas.\n    global_batch_size = Config.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    print(f\"Global batch size set to: {global_batch_size}\")\n    \n    # Enable mixed precision for performance boost on TPUs.\n    keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n    \n    return strategy, global_batch_size\n\n# --- 3. Data Loading and Preparation ---\ndef load_and_prepare_data():\n    \"\"\"\n    Loads, preprocesses, and combines the training, validation, and test datasets.\n    \"\"\"\n    print(\"\\n--- Loading and Preparing Data ---\")\n    \n    # Load primary training data\n    train_toxic_df = pd.read_csv(Config.TRAIN_TOXIC_PATH, usecols=['comment_text', 'toxic'])\n    \n    # Load secondary training data (unintended bias)\n    train_bias_df = pd.read_csv(Config.TRAIN_BIAS_PATH, usecols=['comment_text', 'toxic'])\n    train_bias_df['toxic'] = (train_bias_df['toxic'] >= 0.5).astype(int)\n    \n    # Combine the two training datasets\n    train_df = pd.concat([train_toxic_df, train_bias_df], ignore_index=True)\n    train_df.drop_duplicates(subset=['comment_text'], keep='first', inplace=True)\n    print(f\"Combined training data shape: {train_df.shape}\")\n\n    # Load multilingual validation data\n    valid_df = pd.read_csv(Config.VALIDATION_PATH)\n    print(f\"Validation data shape: {valid_df.shape}\")\n\n    # Load test data\n    test_df = pd.read_csv(Config.TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Handle potential NaN values in text data\n    print(\"Cleaning missing values from text columns...\")\n    train_df['comment_text'].fillna('', inplace=True)\n    valid_df['comment_text'].fillna('', inplace=True)\n    test_df['content'].fillna('', inplace=True)\n\n    # To be robust, also ensure label columns have no NaNs and are integers.\n    train_df['toxic'].fillna(0, inplace=True)\n    valid_df['toxic'].fillna(0, inplace=True)\n    \n    # Extract text and labels for model input, explicitly casting types\n    X_train = train_df['comment_text'].astype(str).values\n    y_train = train_df['toxic'].astype(int).values\n    \n    X_valid = valid_df['comment_text'].astype(str).values\n    y_valid = valid_df['toxic'].astype(int).values\n    \n    X_test = test_df['content'].astype(str).values\n    \n    return X_train, y_train, X_valid, y_valid, X_test, test_df['id']\n\n# --- 4. Dataset Pipeline ---\ndef build_dataset(texts, labels=None, batch_size=32, is_training=True):\n    \"\"\"\n    Creates a tf.data.Dataset from text and label arrays.\n    \"\"\"\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(texts)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n    \n    if is_training:\n        dataset = dataset.shuffle(buffer_size=1024, seed=Config.SEED).repeat()\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n# --- 5. Model Building ---\ndef build_model(strategy):\n    \"\"\"\n    Builds and compiles the Keras NLP classifier model within the TPU strategy scope.\n    \"\"\"\n    print(\"\\n--- Building Model ---\")\n    with strategy.scope():\n        # The Classifier.from_preset method is smart. We can pass preprocessor\n        # arguments like `sequence_length` directly to it.\n        classifier = keras_nlp.models.Classifier.from_preset(\n            Config.PRESET,\n            num_classes=1, # Binary classification (toxic/not-toxic)\n            sequence_length=Config.SEQUENCE_LENGTH # This is passed to the preprocessor\n        )\n        \n        # Define optimizer\n        optimizer = keras.optimizers.AdamW(learning_rate=Config.LEARNING_RATE)\n        \n        # Compile the model\n        classifier.compile(\n            optimizer=optimizer,\n            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[\n                keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n                keras.metrics.AUC(name=\"roc_auc\", from_logits=True)\n            ],\n            jit_compile=True\n        )\n        \n    classifier.summary()\n    return classifier\n\n# --- 6. Training, Prediction, and Evaluation ---\ndef run_pipeline():\n    \"\"\"\n    Executes the full ML pipeline: setup, data loading, training, and prediction.\n    \"\"\"\n    # Initialize TPU\n    strategy, global_batch_size = initialize_tpu()\n    \n    # Load and prepare data\n    X_train, y_train, X_valid, y_valid, X_test, test_ids = load_and_prepare_data()\n    \n    # Create tf.data.Dataset objects\n    train_dataset = build_dataset(X_train, y_train, batch_size=global_batch_size, is_training=True)\n    valid_dataset = build_dataset(X_valid, y_valid, batch_size=global_batch_size, is_training=False)\n    test_dataset = build_dataset(X_test, batch_size=global_batch_size, is_training=False)\n    \n    # Build the model within the strategy scope\n    model = build_model(strategy)\n    \n    # Train the model\n    print(\"\\n--- Starting Model Training ---\")\n    steps_per_epoch = len(X_train) // global_batch_size\n\n    history = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=Config.EPOCHS,\n        steps_per_epoch=steps_per_epoch\n    )\n    \n    print(\"\\n--- Training Complete ---\")\n    \n    # --- Prediction and Evaluation on Validation Set ---\n    print(\"\\n--- Evaluating on Validation Set ---\")\n    valid_preds_logits = model.predict(valid_dataset, verbose=1)\n    valid_preds_probs = tf.nn.sigmoid(valid_preds_logits).numpy().flatten()\n    \n    # Calculate requested evaluation metrics\n    y_pred_binary = valid_preds_probs > 0.5\n    accuracy = accuracy_score(y_valid, y_pred_binary)\n    loss = log_loss(y_valid, valid_preds_probs)\n    roc_auc = roc_auc_score(y_valid, valid_preds_probs)\n    f1_macro = f1_score(y_valid, y_pred_binary, average='macro')\n    f1_weighted = f1_score(y_valid, y_pred_binary, average='weighted')\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation Log Loss: {loss:.4f}\")\n    print(f\"Validation ROC AUC (AUC): {roc_auc:.4f}\")\n    print(f\"Validation F1 Macro: {f1_macro:.4f}\")\n    print(f\"Validation F1 Weighted: {f1_weighted:.4f}\")\n    \n    # --- Prediction for Submission ---\n    print(\"\\n--- Generating Test Predictions ---\")\n    test_preds_logits = model.predict(test_dataset, verbose=1)\n    test_preds_probs = tf.nn.sigmoid(test_preds_logits).numpy().flatten()\n    \n    # Create submission file\n    submission_df = pd.DataFrame({'id': test_ids, 'toxic': test_preds_probs})\n    submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n    print(f\"\\nSubmission file created at: {Config.SUBMISSION_PATH}\")\n    print(submission_df.head())\n    \n    # Clean up memory\n    del model, X_train, y_train, X_valid, y_valid, X_test, test_ids\n    gc.collect()\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:59.117028Z","iopub.execute_input":"2025-07-02T14:26:59.117393Z","iopub.status.idle":"2025-07-02T14:27:47.122043Z","shell.execute_reply.started":"2025-07-02T14:26:59.117366Z","shell.execute_reply":"2025-07-02T14:27:47.115528Z"}},"outputs":[{"name":"stdout","text":"--- Initializing TPU ---\nTPU not detected. Falling back to CPU/GPU strategy.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nGlobal batch size set to: 16\n\n--- Loading and Preparing Data ---\nCombined training data shape: (2099954, 2)\nValidation data shape: (8000, 4)\nTest data shape: (63812, 3)\nCleaning missing values from text columns...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/4072198260.py:111: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/4072198260.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['comment_text'].fillna('', inplace=True)\n/tmp/ipykernel_10/4072198260.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df['content'].fillna('', inplace=True)\n/tmp/ipykernel_10/4072198260.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['toxic'].fillna(0, inplace=True)\n/tmp/ipykernel_10/4072198260.py:117: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  valid_df['toxic'].fillna(0, inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Building Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"xlm_roberta_text_classifier_preprocessor_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"xlm_roberta_text_classifier_preprocessor_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n\n xlm_roberta_tokenizer (\u001b[38;5;33mXLMRobertaTokenizer\u001b[0m)                                         Vocab size: \u001b[38;5;34m250,002\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n\n xlm_roberta_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">XLMRobertaTokenizer</span>)                                         Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">250,002</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"xlm_roberta_text_classifier_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"xlm_roberta_text_classifier_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n\n padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n xlm_roberta_backbone           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)              \u001b[38;5;34m277,450,752\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n (\u001b[38;5;33mXLMRobertaBackbone\u001b[0m)                                                       token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n get_item_1 (\u001b[38;5;33mGetItem\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  xlm_roberta_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n pooled_dropout (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n pooled_dense (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                        \u001b[38;5;34m590,592\u001b[0m  pooled_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n output_dropout (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  pooled_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n logits (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                              \u001b[38;5;34m769\u001b[0m  output_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n\n padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n xlm_roberta_backbone           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">277,450,752</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">XLMRobertaBackbone</span>)                                                       token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n get_item_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  xlm_roberta_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n pooled_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n pooled_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span>  pooled_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n output_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  pooled_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">769</span>  output_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m278,042,113\u001b[0m (1.04 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,042,113</span> (1.04 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m278,042,113\u001b[0m (1.04 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,042,113</span> (1.04 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting Model Training ---\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 249\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 204\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Model Training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m global_batch_size\n\u001b[0;32m--> 204\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# --- Prediction and Evaluation on Validation Set ---\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py:177\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    171\u001b[0m             validation_data\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    174\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 765, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_eventloop.py\", line 74, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2310, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 217, in run\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 435, in process_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 512, in process_shell_message\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 361, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 457, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 606, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_10/4072198260.py\", line 249, in <module>\n\n  File \"/tmp/ipykernel_10/4072198260.py\", line 204, in run_pipeline\n\n  File \"/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_54693]"],"ename":"NotFoundError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 765, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_eventloop.py\", line 74, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2310, in run\n\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 217, in run\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 435, in process_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 512, in process_shell_message\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 361, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 457, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 606, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_10/4072198260.py\", line 249, in <module>\n\n  File \"/tmp/ipykernel_10/4072198260.py\", line 204, in run_pipeline\n\n  File \"/usr/local/lib/python3.10/site-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_54693]","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}