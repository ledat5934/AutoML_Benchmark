# Stage 1 – Data Loading & Pre-processing

Dataset: {{ project.name }}

{% if accelerator %}
Hardware accelerator requested: {{ accelerator }}
{% endif %}

You are an expert Kaggle competitor. Using the task description in the accompanying **description.txt** file and the EDA report below, write a self-contained **Python** module that performs data loading and preprocessing.

Contest / Task Description (context):
{{ description_text }}

Requirements:
1. Declare **file-path constants** at the top of the script so they can be overridden easily when running outside Kaggle. Defaults should mirror the standard Kaggle notebook directory layout _but allow automatic fallback_:  
   • `TRAIN_CSV_PATH = "/kaggle/input/{{ project.name }}/train.csv"`  
   • `TEST_CSV_PATH  = "/kaggle/input/{{ project.name }}/test.csv"`  
   • `PROCESSED_DIR  = "./processed"`  
   If the default files are **not found**, programmatically search `os.walk("/kaggle/input/{{ project.name }}")` (or `project_root`) for the first `*.csv`/**.parquet** file whose path or filename contains "train" / "test" (case-insensitive, including folders like `train_images/`, `train_data.csv`, etc.) and use those instead.  
   Use these constants (possibly reassigned after auto-detection) everywhere else—do **not** hard-code paths.
2. **Target awareness** – the **train** dataset contains the target/label column, whereas the **test** dataset (used only for inference) **usually does *NOT***. Therefore:  
   • Identify the target column **from `train_df` only** (either via description.txt, a column named like `label`, `target`, `y`, or the last column as fallback).  
   • Exclude this target column from all feature preprocessing steps (imputation, scaling, encoding, etc.).  
   • Fit all preprocessing transformers **only on training features** and then apply them to the test features.  
   • When writing outputs, `train_processed.csv` **must include** the untouched target column, while `test_processed.csv` **must NOT contain** any target column.
3. Perform robust preprocessing:
   • Identify numerical vs. categorical vs. text vs. image columns automatically.
   • Impute missing numerical values with the median, categorical with the mode (or constant "missing").
   • One-hot-encode categorical columns (drop_first=False).
   • For free-text columns, create TF-IDF (or a tokenizer compatible with transformers if Stage 2 plans to use them).
   • For image paths, implement a lightweight loader – either a PyTorch `Dataset` **or** a TensorFlow `tf.data` pipeline – that loads, resizes (e.g. 224×224), and normalizes images; optionally extract embeddings via a pre-trained backbone (e.g. EfficientNet-B0) so they can be merged later.
   • Scale numerical features with `StandardScaler` (store the scaler for later reuse).
   • Handle any other preprocessing steps as needed (feature selection, feature engineering, etc.).
4. Return two processed `pandas.DataFrame`s named `train_df_processed` and `test_df_processed`.
   • `train_df_processed` = processed features **+** original target column.  
   • `test_df_processed`  = processed features **only**.
5. Persist the processed CSVs to a folder `processed/` inside the project directory as `train_processed.csv` and `test_processed.csv`.
6. Keep a deterministic workflow: set `RANDOM_STATE = 42` and use it wherever randomness is involved.
7. Wrap the script in a `def main()` so it can be executed directly **or** imported.

Exploratory Data Analysis (context):
{{ analysis_report }}

Response format:
```python
# === STAGE 1 START ===
<your code here>
# === STAGE 1 END ===
```
Return **only** the code snippet – no commentary. 